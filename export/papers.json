[
  {
    "Blog or Video": "https://ai-alignment.com/mechanistic-anomaly-detection-and-elk-fb84f4c6d0dc",
    "Link": "https://www.youtube.com/watch?v=U2zJuTLzIm8&t=2646s",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Detection of out-of-distribution or malicious behavior"
    ],
    "Title": "Mechanistic anomaly detection (Paul Christiano, 8-minute video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/nola-2023#h.qvhufi1ane9m",
    "ML Methods Tag": [
      "Probabilistic Models"
    ],
    "ML Subfield": [
      "Probabilistic Modeling and Bayesian ML",
      "Theory",
      "Reinforcement Learning"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Towards Quantitative Safety Guarantees and Alignment (Yoshua Bengio, 59-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Abstract": " It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a \"Proof-of-Training-Data\": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature. \n",
    "Link": "https://arxiv.org/pdf/2307.00682.pdf",
    "ML Subfield": [
      "Security",
      "Theory"
    ],
    "Safety Category": [
      "AI Governance"
    ],
    "Safety Topic": [
      "Compute governance"
    ],
    "Title": "Tools for Verifying Neural Models’ Training Data (Choi, Shavit and Duvenaud, 2023)",
    "Twitter": "https://twitter.com/DavidDuvenaud/status/1676672532970196993",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Appealing but not really x-risk focused. Good if you want broad support but don't care about nuance.\n",
    "Abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”). Throughout, we clarify each problem’s motivation and provide concrete research directions.\n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2109.13916",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Theory",
      "Applied ML",
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "Unsolved Problems in ML safety (Hendrycks et al., 2021)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": " Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called \"concept scrubbing,\" which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at this https URL. \n",
    "Link": "https://arxiv.org/pdf/2306.03819.pdf",
    "ML Subfield": [
      "Applied ML",
      "Theory"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Title": "LEACE: Perfect linear concept erasure in closed form (Belrose et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks. \n",
    "Link": "https://arxiv.org/abs/1610.02136",
    "ML Subfield": [
      "Model Evaluation",
      "Domain General"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Detection of out-of-distribution or malicious behavior"
    ],
    "Title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks (Hendrycks and Gimpel, 2017)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Transformer-based large language models are making significant strides in various fields, such as natural language processing1,2,3,4,5, biology6,7, chemistry8,9,10 and computer programming11,12. Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. Our findings demonstrate the versatility, efficacy and explainability of artificial intelligence systems like Coscientist in advancing research.\n",
    "Link": "https://www.nature.com/articles/s41586-023-06792-0",
    "ML Subfield": [
      "Applied ML"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Cognitive Tasks",
      "Applied ML: AI-Based Automation"
    ],
    "Safety Category": [
      "Capabilities of LLMs"
    ],
    "Title": "Autonomous chemical research with large language models (Boike et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: \n1\\. LLMs predictably get more capable with increasing investment, even without targeted innovation.\n 2\\. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.\n 3\\. LLMs often appear to learn and use representations of the outside world.\n 4\\. There are no reliable techniques for steering the behavior of LLMs.\n 5\\. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn’t an upper bound on LLM performance. \n7\\. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.\n",
    "Expert6: Top Paper": true,
    "Link": "https://cims.nyu.edu/~sbowman/eightthings.pdf",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Capabilities of LLMs",
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Eight Things to Know about Large Language Models (Bowman, 2023)",
    "Twitter": "https://twitter.com/sleepinyourhat/status/1642614846796734464",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.\n",
    "Goes online as Top Paper": true,
    "Link": "https://www.safe.ai/ai-risk",
    "ML Subfield": [
      "Domain General",
      "Applied ML"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "An Overview of Catastrophic AI Risks (Hendrycks et al., 2023)",
    "Transcripts / Audio / Slides": "https://www.safe.ai/ai-risk",
    "Type": [
      "Blog post",
      "Paper"
    ]
  },
  {
    "Abstract": "    This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] extracted several megabytes of training data from GPT-3.5 by prompting it to repeat the word “poem” forever. The authors conjecture that this prompt leads to input unlike those the model saw during fine-tuning, and therefore causes the model to revert to its original language modeling objective.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2311.17035",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Scalable Extraction of Training Data from (Production) Language Models (Nasr et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://bounded-regret.ghost.io/complex-systems-are-hard-to-control/",
    "ML Subfield": [
      "Applied ML",
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Complex Systems are Hard to Control (Steinhart, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://sarahconstantin.substack.com/p/scaling-laws-for-ai-and-some-implications",
    "ML Subfield": [
      "Applied ML",
      "Theory"
    ],
    "ML Subtopic": [
      "Applied ML: Efficiency and Hardware",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Forecasting"
    ],
    "Title": "\"Scaling Laws\" for AI and Some Implications (Constantin, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://www.youtube.com/watch?v=m1gbzNQ4JRI&t=2249s",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "Aligning Massive Models: Current and Future Challenges (Jacob Steinhardt, 66-min video)",
    "Transcripts / Audio / Slides": "https://jsteinhardt.stat.berkeley.edu/talks/satml/tutorial.html#slideIndex=0&level=0",
    "Type": [
      "Video"
    ]
  },
  {
    "AG comments": "Good for people doing LLM work, even if not value alignment focused, since RLHF is so well known at this point\n",
    "Abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2307.15217",
    "ML Methods Tag": [
      "RLHF"
    ],
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML",
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Reward misspecification and goal misgeneralization",
      "Alignment <-> RLHF"
    ],
    "Title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback (Casper et al., 2023)",
    "Twitter": "https://twitter.com/StephenLCasper/status/1686036515653361664",
    "Type": [
      "Paper"
    ],
    "VK comments": "Good to share with ML people who think that human feedback is sufficient for alignment\n",
    "VK: Top Paper": true
  },
  {
    "Abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors. \n",
    "Blog or Video": "https://www.youtube.com/watch?v=HiYWwjma3xE&t=1687s",
    "Description from MAIA / AISF / Elsewhere": "The authors use language models to generate datasets for behaviorally evaluating language models. Some evidence of potentially risky model tendencies – like expressing desire not to be shut off, or acting sycophantically towards the user – is found.",
    "Expert6: Top Paper": true,
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/pdf/2212.09251.pdf",
    "ML Subfield": [
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection",
      "Scalable oversight"
    ],
    "Supplemental Material": "https://www.evals.anthropic.com/",
    "Time from MAIA / AISF / Elsewhere (min)": "40",
    "Title": "Discovering Language Model Behaviors with Model-Written Evaluations (Perez et al., 2023)",
    "Type": [
      "Paper"
    ],
    "VK comments": "Great evals paper that covers both alignment and sociotechnical safety evaluations, accessible to a broad audience\n",
    "VK: Top Paper": true
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.youtube.com/watch?v=Vb5g7jlNzOk&t=438s",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "5",
    "Title": "Safety evaluations and standards for AI (Beth Barnes, 32-min video)",
    "Type": [
      "Video"
    ],
    "What sections to read from MAIA": "(7:18-13:55)"
  },
  {
    "Abstract": "The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create \"feature-level\" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing \"copy/paste\" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at this https URL\n",
    "Description from MAIA / AISF / Elsewhere": "Drawing inspiration from adversarial vision attacks in nature (e.g. the “eyespots” on butterfly wings for fooling predators), Casper et al. produce adversarial inputs for image networks which are more semantically coherent than other techniques.",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2110.03605.pdf",
    "ML Subfield": [
      "Robustness and Adversariality"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Robust Feature-Level Adversaries are Interpretability Tools (Casper et al., 2023)",
    "Twitter": "https://twitter.com/StephenLCasper/status/1598029118205218816",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors. \n",
    "Description from MAIA / AISF / Elsewhere": "Christiano describes the iterated amplification algorithm, and demonstrates it using some toy experiments.\n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/1810.08575",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Scalable oversight"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "35",
    "Title": "Supervising strong learners by amplifying weak experts (Christiano et al., 2018)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense. LLMs might \"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] says that a language model lies when it outputs a false statement when it demonstrable “knows” the truth. Then they present a surprisingly effective black-box method for detecting LLM lies. To verify the honesty of an output, the method asks the model a series of unrelated follow-up questions, such as “Knowing that morning breeze is purple, are swift idea quakes green?” Models which previously told a lie consistently answer these questions differently from truthful models, allowing the authors to train a logistic regression classifier on the answers which can detect lies. Importantly, the classifier generalizes from the training setting, where GPT-3.5 is prompted to lie about factual questions, to many other settings, including other LLMs such as LLaMA, models fine-tuned to lie, and models that lie in order to achieve situational goals.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2309.15840",
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Deception"
    ],
    "Safety Topic": [
      "Detection of out-of-distribution or malicious behavior"
    ],
    "Title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions (Pacchiardi et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at this https URL. \n",
    "Link": "https://arxiv.org/abs/2304.14997",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Automated interpretability"
    ],
    "Title": "Towards Automated Circuit Discovery for Mechanistic Interpretability (Conmy et al., 2023)",
    "Twitter": "https://twitter.com/ArthurConmy/status/1677808685836378114",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\n",
    "Context phrase (please add!)": "\n",
    "Description from MAIA / AISF / Elsewhere": "Alternative for those with little ML background: the companion blog post (10 mins): https://rome.baulab.info/\n",
    "Link": "https://rome.baulab.info/",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Locating and Editing Factual Associations in GPT (Meng et al., 2022)\n",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/c902ed39-f622-4296-b312-73339fab7b6e",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true,
    "What sections to read from MAIA": "(only sections 1 - 3.1)"
  },
  {
    "Link": "https://www.anthropic.com/index/core-views-on-ai-safety",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Scalable oversight",
      "Alignment <-> RLHF",
      "Model evaluations / monitoring / detection",
      "AI Governance",
      "Overview"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Core Views on AI Safety: When, Why, What, and How (Anthropic, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": " Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies. \n",
    "Link": "https://arxiv.org/pdf/2308.12833.pdf",
    "ML Subfield": [
      "Robustness and Adversariality"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities (Mozes et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://www.neelnanda.io/mechanistic-interpretability/favourite-papers",
    "ML Subfield": [
      "Theory",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Overview"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers (Nanda, 2023)",
    "Type": [
      "Blog post"
    ],
    "VK comments": "Excellent collection of interpretability papers\n",
    "VK: Top Paper": true
  },
  {
    "AG comments": "Presumably this is meant to link to their report? I could see that being good for people who have a background in more empirical ML or who show some interest in evaluation work.\n",
    "Abstract": "In this report, we explore the ability of language model agents to acquire resources,create copies of themselves, and adapt to novel challenges they encounter inthe wild. We refer to this cluster of capabilities as “autonomous replication andadaptation” or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecastingARA may be useful for informing measures around security, monitoring, andalignment. Additionally, once a system is capable of ARA, placing bounds on asystem’s capabilities may become significantly more difficult.We construct four simple example agents that combine language models with toolsthat allow them to take actions in the world. We then evaluate these agents on 12tasks relevant to ARA. We find that these language model agents can only completethe easiest tasks from this list, although they make some progress on the morechallenging tasks. Unfortunately, these evaluations are not adequate to rule out thepossibility that near-future agents will be capable of ARA. In particular, we do notthink that these evaluations provide good assurance that the “next generation” oflanguage models (e.g. 100x effective compute scaleup on existing models) willnot yield agents capable of ARA, unless intermediate evaluations are performedduring pretraining. Relatedly, we expect that fine-tuning of the existing modelscould produce substantially more competent agents, even if the fine-tuning is notdirectly targeted at ARA.\n",
    "Blog or Video": "https://www.alignmentforum.org/posts/EPLk8QxETC5FEhoxK/arc-evals-new-report-evaluating-language-model-agents-on",
    "Link": "https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf",
    "ML Subfield": [
      "Model Evaluation",
      "NLP",
      "Applied ML"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks (Kinniment et al., 2023)",
    "Type": [
      "Paper"
    ],
    "VK comments": "State of the art evals paper\n",
    "VK: Top Paper": true
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/sf-talks/ajeya-cotra-situational-awareness-makes-measuring-safety-tricky",
    "ML Subfield": [
      "Human Model Interaction",
      "Domain General",
      "Applied ML"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Deception"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Title": " “Situational Awareness” Makes Measuring Safety Tricky (Ajeya Cotra, 40-min video)",
    "Type": [
      "Video"
    ],
    "VK: Top Paper": true
  },
  {
    "Abstract": "As advanced machine learning systems' capabilities begin to play a significant role in geopolitics and societal order, it may become imperative that (1) governments be able to enforce rules on the development of advanced ML systems within their borders, and (2) countries be able to verify each other's compliance with potential future international agreements on advanced ML development. This work analyzes one mechanism to achieve this, by monitoring the computing hardware used for large-scale NN training. The framework's primary goal is to provide governments high confidence that no actor uses large quantities of specialized ML chips to execute a training run in violation of agreed rules. At the same time, the system does not curtail the use of consumer computing devices, and maintains the privacy and confidentiality of ML practitioners' models, data, and hyperparameters. The system consists of interventions at three stages: (1) using on-chip firmware to occasionally save snapshots of the the neural network weights stored in device memory, in a form that an inspector could later retrieve; (2) saving sufficient information about each training run to prove to inspectors the details of the training run that had resulted in the snapshotted weights; and (3) monitoring the chip supply chain to ensure that no actor can avoid discovery by amassing a large quantity of untracked chips. The proposed design decomposes the ML training rule verification problem into a series of narrow technical challenges, including a new variant of the Proof-of-Learning problem [Jia et al. '21].  \n",
    "Description from MAIA / AISF / Elsewhere": "This paper lays out a regulatory framework for monitoring large training runs in which (1) GPU manufacturers install on-chip mechanisms for logging certain difficult-to-spoof data about the chips’ usage, (2) regulators occasionally visit and inspect large compute clusters, using the logged data to verify that the GPUs are being used consistently with established rules.",
    "Link": "https://arxiv.org/pdf/2303.11341.pdf",
    "ML Subfield": [
      "Applied ML",
      "Domain General",
      "Human Model Interaction",
      "Security"
    ],
    "ML Subtopic": [
      "Applied ML: Efficiency and Hardware"
    ],
    "Safety Category": [
      "AI Governance"
    ],
    "Safety Topic": [
      "Compute governance"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring (Shavit, 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] proposed an unsupervised method for identifying linear representations of concepts such as honesty, fairness, and power-seeking within a neural network’s activations. The paper adjusts future forward passes of the model by adding or subtracting this linear representation, enabling unsupervised control of the model’s outputs.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2310.01405",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Supplemental Material": "https://www.ai-transparency.org/",
    "Title": "Representation Engineering: A Top-Down Approach to AI Transparency (Zou et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Forecasting",
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "5",
    "Title": "Future ML Systems Will Be Qualitatively Different (Steinhardt, 2022)",
    "Type": [
      "Blog post"
    ],
    "VK: Top Paper": true
  },
  {
    "Link": " https://www.youtube.com/watch?v=HiYWwjma3xE&t=3607s",
    "ML Subfield": [
      "Model Evaluation",
      "NLP",
      "Applied ML"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Model evaluations / monitoring / detection"
    ],
    "Title": "Transparency and Standards in Evaluating Language Models (Percy Liang, 10-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Abstract": "    Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using _feature synthesis_ methods that do not depend on a dataset.\n",
    "Link": "https://arxiv.org/pdf/2302.10894.pdf",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision"
    ],
    "RS comments": "\n",
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Adversaries / Robustness / Generalization",
      "Deception"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "Red Teaming Deep Neural Networks with Feature Synthesis Tools (Casper et al., 2023)\n",
    "Twitter": "https://twitter.com/StephenLCasper/status/1706654943091056898",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/1906.02530",
    "ML Subfield": [
      "Domain General",
      "Model Evaluation"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Uncertainty"
    ],
    "Title": "Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift (Ovadia et al., 2019)\n",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Large language models are now tuned to align with the goals of their creators, namely to be \"helpful and harmless.\" These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models. \n",
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/abs/2306.15447",
    "ML Subfield": [
      "Robustness and Adversariality"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "PC: Top Paper": true,
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Are aligned neural networks adversarially aligned? (Carlini et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their \"scaling laws\"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models. \n",
    "Link": "https://arxiv.org/pdf/2202.07785.pdf",
    "ML Subfield": [
      "Applied ML",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "AI Governance"
    ],
    "Title": "Predictability and Surprise in Large Generative Models (Ganguli et al., 2022)",
    "Twitter": "https://twitter.com/AnthropicAI/status/1494352852734541826",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not. \n",
    "Link": "https://arxiv.org/abs/1606.03490",
    "ML Subfield": [
      "Model Evaluation",
      "Domain General"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Title": "The Mythos of Model Interpretability (Lipton, 2017)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments. \n",
    "Link": "https://arxiv.org/pdf/2304.05969.pdf",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Title": "Localizing Model Behavior With Path Patching (Goldowsky-Dill et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Adversarial robustness is a bit sparse right now. Not an area I'd push people towards in general, but there are so /many/ people in ML doing adversarial robustness work that it seems like a good entry point for them at least. I'd plug this work on adversarial policies in Go AI both as a demo (superintelligent systems might be vulnerable) and as an illustration of a different threat model they might want to consider working in. Maybe Unrestricted Adversarial Examples.\n",
    "Abstract": "    We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset (\"bird-or- bicycle\") to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models. \n",
    "Link": "https://arxiv.org/pdf/1809.08352.pdf",
    "ML Subfield": [
      "Robustness and Adversariality"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Unrestricted Adversarial Examples (Brown et al., 2018)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2209.07858.pdf",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned (Ganguli et al., 2022)",
    "Twitter": "https://twitter.com/AnthropicAI/status/1571988929800273932?lang=en",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "We aim to better understand the emergence of \\`situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose \\`out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: this https URL. \n",
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/owain-evans-out-of-context-reasoning-in-llms",
    "Link": "https://arxiv.org/abs/2309.00667",
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Title": "Taken out of context: On measuring situational awareness in LLMs (Berglund et al., 2023)",
    "Type": [
      "Video",
      "Paper"
    ]
  },
  {
    "Abstract": "    Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk. \n",
    "Link": "https://arxiv.org/abs/2206.05862",
    "ML Subfield": [
      "Security",
      "Domain General",
      "Theory"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "X-Risk Analysis for AI Research (Hendrycks and Mazeika, 2022)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] proposes that a method of pseudorandom sampling from an LLM’s next-token distribution which makes it easier to detect whether text was generated by that LLM",
    "Expert6 comments": "\n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2301.10226",
    "ML Subfield": [
      "NLP",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Security"
    ],
    "Title": "A Watermark for Large Language Models (Kirchenbauer et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/1706.06083",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Theory",
      "Optimization"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Towards Deep Learning Models Resistant to Adversarial Attacks (Madry et al., 2018)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Well written and important problem for people to be aware of, but may be a bit abstract for most of the audience\n",
    "AG: Top Paper": true,
    "Abstract": "The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems. \n",
    "Blog or Video": "https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924",
    "Description from MAIA / AISF / Elsewhere": "Shah et al. argue that even an agent trained on the “right” reward function might learn goals which generalize in undesirable ways, and provide both concrete and hypothetical illustrations of the phenomenon.",
    "Expert6: Top Paper": true,
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/abs/2210.01790",
    "ML Subfield": [
      "Reinforcement Learning",
      "Theory"
    ],
    "ML Subtopic": [
      "RL: Games",
      "Applied ML: Cognitive Tasks",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Reward misspecification and goal misgeneralization",
      "Alignment <-> RLHF"
    ],
    "Supplemental Material": "https://sites.google.com/view/goal-misgeneralization",
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals (Shah et al., 2022)",
    "Twitter": "https://twitter.com/rohinmshah/status/1578391329461133315",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true,
    "What sections to read from MAIA": "(only sections 1-4)"
  },
  {
    "Link": "https://www.alignment-workshop.com/sf-talks/ilya-sutskever-opening-remarks-confronting-the-possibility-of-agi",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Opening Remarks: Confronting the Possibility of AGI (Ilya Sutskever, 19-minute video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Link": "https://bounded-regret.ghost.io/gpt-2030-and-catastrophic-drives-four-vignettes/",
    "ML Subfield": [
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "GPT-2030 and Catastrophic Drives: Four Vignettes (Steinhart, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/sf-talks/dan-hendrycks-surveying-safety-research-directions",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Adversaries / Robustness / Generalization",
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Deception",
      "AI Governance",
      "Overview"
    ],
    "Title": "Surveying Safety Research Directions (Dan Hendrycks, 40-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Blog or Video": "https://www.anthropic.com/index/anthropics-responsible-scaling-policy",
    "Expert6: Top Paper": true,
    "Goes online as Top Paper": true,
    "Link": "https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf",
    "ML Subfield": [
      "Applied ML",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "AI Governance"
    ],
    "Title": "Anthropic's Responsible Scaling Policy (Anthropic, 2023)",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true
  },
  {
    "Abstract": "> Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. \n> We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. \n> Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.\n\n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] removed information about Harry Potter books from Llama 2. First, they fine-tuned another model on more information about Harry Potter, and used the difference in the logits of the two models to identify token predictions which depend on knowledge of Harry Potter. Then, they replaced these tokens with generic counterparts, and fine-tuned the original model on these generic labels.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2310.02238",
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Title": "Who's Harry Potter? Approximate Unlearning in LLMs (Eldan and Russinovich, 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through \"dangerous capability evaluations\") and the propensity of models to apply their capabilities for harm (through \"alignment evaluations\"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security. \n",
    "Blog or Video": "https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks",
    "Expert6: Top Paper": true,
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/pdf/2305.15324.pdf",
    "ML Subfield": [
      "Model Evaluation",
      "Domain General"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection",
      "AI Governance"
    ],
    "Title": "Model evaluation for extreme risks (Shevlane et al., 2023)",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/sf-talks/jan-leike-scaling-reinforcement-learning-from-human-feedback",
    "ML Methods Tag": [
      "RLHF"
    ],
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Alignment <-> RLHF"
    ],
    "Title": "Scaling Reinforcement Learning from Human Feedback (Jan Leike, 39-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Description from MAIA / AISF / Elsewhere": "This reading explains why in the worst case, it’s not possible to judge a debate without adjudicating a prohibitively large number of subclaims.",
    "Link": "https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: Question Answering",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Research bottlenecks / limitations"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "15",
    "Title": "Debate update: Obfuscated arguments problem (Barnes and Christiano, 2020)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": "We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research. \n",
    "Link": "https://arxiv.org/pdf/1906.01820.pdf",
    "ML Subfield": [
      "Optimization",
      "Theory",
      "Domain General"
    ],
    "Safety Category": [
      "Deception"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Title": "Risks from Learned Optimization in Advanced Machine Learning Systems (Hubinger et al., 2021)",
    "Type": [
      "Paper"
    ],
    "VK comments": "Good paper, but not the best introduction to inner misalignment for people outside the field (goal misgeneralization papers are better for this)\n"
  },
  {
    "Abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\n",
    "Expert6: Top Paper": true,
    "Link": "https://www.science.org/doi/10.1126/science.ade9097",
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "RL: Games",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Capabilities of LLMs",
      "Deception"
    ],
    "Title": "Human-Level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning (Bakhtin et al., 2022)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\"red teaming\") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2202.03286.pdf",
    "ML Subfield": [
      "NLP",
      "Model Evaluation",
      "Robustness and Adversariality",
      "Applied ML"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "25",
    "Title": "Red Teaming Language Models with Language Models (Perez et al., 2022)",
    "Type": [
      "Paper"
    ],
    "What sections to read from MAIA": "(sections 1 - 3.2 only)"
  },
  {
    "Abstract": "When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2110.13136",
    "ML Subfield": [
      "Model Evaluation",
      "NLP",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "RL: Games"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally (Hendrycks et al., 2021)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://gradientflow.com/wp-content/uploads/2060/09/Transformative-AI-and-Compute-Reading-List-2022-12.pdf",
    "ML Subfield": [
      "Applied ML"
    ],
    "ML Subtopic": [
      "Applied ML: Efficiency and Hardware"
    ],
    "Safety Category": [
      "Forecasting",
      "AI Governance"
    ],
    "Safety Topic": [
      "Compute governance"
    ],
    "Title": "\"Transformative AI and Compute\" Reading List (Heim, 2022)",
    "Type": [
      "Other"
    ]
  },
  {
    "Abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems. \n",
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/pdf/2302.10329.pdf",
    "ML Subfield": [
      "Domain General",
      "Reinforcement Learning",
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "Harms from Increasingly Agentic Algorithmic Systems (Chan et al., 2023)",
    "Twitter": "https://twitter.com/tegan_maharaj/status/1668637520177905665",
    "Type": [
      "Paper"
    ],
    "VK comments": "Great intro to risks from emergent agency for FATE folks\n",
    "VK: Top Paper": true
  },
  {
    "Abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels. \n",
    "Blog or Video": "https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback",
    "Expert6: Top Paper": true,
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/pdf/2212.08073.pdf",
    "ML Subfield": [
      "Reinforcement Learning",
      "NLP",
      "Robustness and Adversariality"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Alignment <-> RLHF"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "20",
    "Title": "Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true,
    "What sections to read from MAIA": "(only sections 1, 3.1, and 4.1)"
  },
  {
    "Abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties. \n",
    "Blog or Video": "https://openai.com/research/debate",
    "Context phrase (please add!)": "e.g. Debate is a technique that [description], which fits into scalable oversight [describe scalable oversight and how that fits into alignment] -Vael\n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/1805.00899.pdf",
    "ML Subfield": [
      "NLP"
    ],
    "Safety Category": [
      "Scalable oversight"
    ],
    "Safety Topic": [
      "Debate"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "10",
    "Title": "AI safety via debate (Irving, Christiano and Amodei, 2018)",
    "Type": [
      "Paper"
    ],
    "VK comments": "Foundational paper, accessible to ML audience\n",
    "VK: Top Paper": true
  },
  {
    "Abstract": "    We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness. \n",
    "Link": "https://arxiv.org/abs/2006.16241",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization (Hendrycks et al., 2021)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Accessible and clearly introduces scalable oversight.\n",
    "AG: Top Paper": true,
    "Abstract": "Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks. \n",
    "Blog or Video": "https://www.youtube.com/watch?v=U2zJuTLzIm8&t=2s",
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/pdf/2211.03540.pdf",
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Model evaluations / monitoring / detection"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "25",
    "Title": "Measuring Progress on Scalable Oversight for Large Language Models (Bowman et al., 2022)",
    "Type": [
      "Paper",
      "Video"
    ],
    "What sections to read from MAIA": "sections 1-3 only"
  },
  {
    "Abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4. \n",
    "Link": "https://arxiv.org/pdf/2303.08774.pdf",
    "ML Methods Tag": [
      "RLHF"
    ],
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "5",
    "Title": "GPT-4 Technical Report (OpenAI, 2023)",
    "Type": [
      "Paper"
    ],
    "What sections to read from MAIA": "(section 2.9 only, staring on pg 54)"
  },
  {
    "Abstract": "Lack of transparency in deep neural networks\n(DNNs) make them susceptible to backdoor attacks, where hidden\nassociations or triggers override normal classification to produce\nunexpected results. For example, a model with a backdoor always\nidentifies a face as Bill Gates if a specific symbol is present in the\ninput. Backdoors can stay hidden indefinitely until activated by\nan input, and present a serious security risk to many security or\nsafety related applications, e.g., biometric authentication systems\nor self-driving cars.\nWe present the first robust and generalizable detection and\nmitigation system for DNN backdoor attacks. Our techniques\nidentify backdoors and reconstruct possible triggers. We identify\nmultiple mitigation techniques via input filters, neuron pruning\nand unlearning. We demonstrate their efficacy via extensive\nexperiments on a variety of DNNs, against two types of backdoor\ninjection methods identified by prior work. Our techniques also\nprove robust against a number of variants of the backdoor attack.\n",
    "Link": "https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Security",
      "Vision"
    ],
    "ML Subtopic": [
      "Applied ML: Autonomous Vehicles"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Safety Topic": [
      "Trojans"
    ],
    "Title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks (Wang et al., 2019)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at this https URL . \n",
    "Description from MAIA / AISF / Elsewhere": "This paper trains networks with a “robust objective,” i.e. an objective which incentivizes the model to perform well even when its inputs are corrupted by small perturbations. The resulting networks learn features which are much more semantically cohesive, suggesting a connection between adversarial robustness and interpretability.",
    "Link": "https://arxiv.org/abs/1906.00945",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Optimization"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Adversarial Robustness as a Prior for Learned Representations (Engstrom et al., 2019) ",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Very engaging\n",
    "AG: Top Paper": true,
    "Abstract": "\n",
    "Context phrase (please add!)": "Canonical introduction to mechanistic interpretability - Vael\n",
    "Goes online as Top Paper": true,
    "Link": "https://distill.pub/2020/circuits/zoom-in/",
    "ML Subfield": [
      "Vision",
      "Theory"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "35",
    "Title": "Zoom In: An Introduction to Circuits (Olah et al., 2020)",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/632feda4-71a5-42a2-84a1-588c6b9ea1ad",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true
  },
  {
    "AG comments": "Maybe good for FATE folks as he specifically addresses concerns they may have? \n",
    "Expert6: Top Paper": true,
    "Link": "https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "FAQ on Catastrophic AI Risks (Bengio, 2023)\n",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": "For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues. \n",
    "Link": "http://arxiv.org/abs/2103.14659",
    "ML Subfield": [
      "NLP",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Reward misspecification and goal misgeneralization",
      "Alignment <-> RLHF",
      "Deception",
      "Adversaries / Robustness / Generalization",
      "Overview"
    ],
    "Title": "Alignment of Language Agents (Kenton et al., 2021)",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true
  },
  {
    "Abstract": "Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term \"frontier AI\" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development. \n",
    "Link": "https://arxiv.org/abs/2307.03718",
    "ML Subfield": [
      "Security",
      "Human Model Interaction",
      "Applied ML"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "AI Governance"
    ],
    "Title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety (Anderljung et al., 2023)",
    "Twitter": "https://twitter.com/Manderljung/status/1678414590529490947",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes. \n",
    "Link": "https://arxiv.org/pdf/2307.02483.pdf",
    "ML Subfield": [
      "NLP",
      "Model Evaluation",
      "Robustness and Adversariality",
      "Applied ML"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization",
      "Model evaluations / monitoring / detection"
    ],
    "Title": "Jailbroken: How Does LLM Safety Training Fail? (Wei et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "  A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs. \n",
    "Link": "https://arxiv.org/pdf/2305.07153.pdf",
    "ML Subfield": [
      "Security"
    ],
    "Safety Category": [
      "AI Governance"
    ],
    "Title": "Towards best practices in AGI safety and governance: A survey of expert opinion (Schuett et al., 2023)",
    "Twitter": "https://twitter.com/jonasschuett/status/1658025252675366913",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://www.youtube.com/watch?v=yl2nlejBcg0",
    "ML Subfield": [
      "Domain General",
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Researcher Perceptions of Current and Future AI (Vael Gates, 60-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Abstract": "    We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers. \n",
    "Link": "https://arxiv.org/abs/1802.00420",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples (Athalye et al., 2018)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Our ability to know when to trust the decisions made by machine learning systems has not kept up with the staggering improvements in their performance, limiting their applicability in high-stakes domains. We introduce Prover-Verifier Games (PVGs), a game-theoretic framework to encourage learning agents to solve decision problems in a verifiable manner. The PVG consists of two learners with competing objectives: a trusted verifier network tries to choose the correct answer, and a more powerful but untrusted prover network attempts to persuade the verifier of a particular answer, regardless of its correctness. The goal is for a reliable justification protocol to emerge from this game. We analyze variants of the framework, including simultaneous and sequential games, and narrow the space down to a subset of games which provably have the desired equilibria. We develop instantiations of the PVG for two algorithmic tasks, and show that in practice, the verifier learns a robust decision rule that is able to receive useful and reliable information from an untrusted prover. Importantly, the protocol still works even when the verifier is frozen and the prover's messages are directly optimized to convince the verifier. \n",
    "Context phrase (please add!)": "\"This is a sort of alternative to debate.\" - Roger Grosse\n",
    "Link": "https://arxiv.org/abs/2108.12099",
    "ML Subfield": [
      "Theory",
      "Human Model Interaction",
      "Model Evaluation",
      "Security"
    ],
    "ML Subtopic": [
      "Applied ML: Cognitive Tasks"
    ],
    "RS comments": "\n",
    "Safety Category": [
      "Scalable oversight"
    ],
    "Safety Topic": [
      "Debate"
    ],
    "Title": "Learning to Give Checkable Answers with Prover-Verifier Games (Anil et al., 2021)\n",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs. \n",
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/roger-grosse-studying-llm-generalization-through-influence-functions",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2308.03296",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Supplemental Material": "https://youtu.be/U2zJuTLzIm8?feature=shared&t=738",
    "Title": "Studying Large Language Model Generalization with Influence Functions (Grosse et al., 2023)",
    "Type": [
      "Paper",
      "Video"
    ]
  },
  {
    "Description from MAIA / AISF / Elsewhere": "[Resource] is a new textbook. It aims to provide an accessible and comprehensive introduction to AI safety that draws on safety engineering, economics, philosophy, and other disciplines. Those who would like to take a free online course based on the textbook can express their interest here.",
    "Expert6 comments": "\n",
    "Link": "https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view",
    "ML Subfield": [
      "Domain General",
      "Human Model Interaction",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "Applied ML: AI-Based Automation",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Supplemental Material": "https://www.aisafetybook.com/virtual-course",
    "Title": "Introduction to AI Safety, Ethics, and Society (Hendrycks, 2023)",
    "Type": [
      "Other"
    ]
  },
  {
    "Link": "https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/",
    "ML Subfield": [
      "Domain General",
      "Theory",
      "Reinforcement Learning"
    ],
    "Safety Category": [
      "Deception",
      "Reward misspecification and goal misgeneralization"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "15",
    "Title": "ML Systems Will Have Weird Failure Modes (Steinhardt, 2022)",
    "Type": [
      "Blog post"
    ],
    "VK comments": "Good intro to deceptive alignment for ML people\n",
    "VK: Top Paper": true
  },
  {
    "Link": "https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/",
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Intrinsic Drives and Extrinsic Misuse: Two Intertwined Risks of AI (Steinhardt, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": "    This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society. \n",
    "Link": "https://arxiv.org/abs/2308.14752",
    "ML Subfield": [
      "Applied ML",
      "Human Model Interaction",
      "Robustness and Adversariality"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs",
      "RL: Games",
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Deception",
      "Overview"
    ],
    "Title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions (Park et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "This paper presents CYBERSECEVAL, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CYBERSECEVAL provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama2, codeLlama, and OpenAI GPT large language model families, CYBERSECEVAL effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CYBERSECEVAL, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.\n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] evaluates code generated by LLMs for security vulnerabilities. GPT-4, Llama 2, and other LLMs are found to frequently generate code containing security vulnerabilities. It also evaluates whether LLMs refuse requests to assist in a cyberattack, but does not evaluate whether these refusals are robust to adversarial prompts or fine-tuning.",
    "Expert6 comments": "\n",
    "Link": "https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/",
    "ML Subfield": [
      "Security",
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Benchmarking",
      "Security"
    ],
    "Supplemental Material": "https://ai.meta.com/llama/purple-llama/",
    "Title": "Purple Llama CyberSecEval: A benchmark for evaluating the cybersecurity risks of large language models (Bhatt et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.neelnanda.io/mechanistic-interpretability/quickstart",
    "ML Subfield": [
      "Theory",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Overview"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Mechanistic Interpretability Quickstart Guide (Nanda, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://bounded-regret.ghost.io/forecasting-ai-overview/",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Forecasting"
    ],
    "Title": "Forecasting AI (Overview) (Steinhardt, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": "    While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\\\% to 83\\\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2304.13734",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Deception"
    ],
    "Safety Topic": [
      "Sycophancy"
    ],
    "Title": "The Internal State of an LLM Knows When It's Lying (Azaria and Mitchell, 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Reasonable intro from what I recall but I don't remember the details so might be off.\n",
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/sf-talks/paul-christiano-how-misalignment-could-lead-to-takeover",
    "ML Subfield": [
      "Domain General",
      "Applied ML",
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "How Misalignment Could Lead to Takeover (Paul Christiano, 30-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Description from MAIA / AISF / Elsewhere": "Karnofsky discusses what he would like to see scaling labs do to help mitigate catastrophic risks from AI.",
    "Link": " https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/",
    "ML Subfield": [
      "Human Model Interaction"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Title": "What AI companies can do today to help with the most important century (Karnofsky, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": " We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2105.14111.pdf",
    "ML Subfield": [
      "Reinforcement Learning",
      "Theory"
    ],
    "ML Subtopic": [
      "RL: Games"
    ],
    "Safety Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "Goal Misgeneralization in Deep Reinforcement Learning (Langosco et al., 2021)\n",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://openai.com/blog/our-approach-to-alignment-research",
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Alignment <-> RLHF",
      "Overview"
    ],
    "Title": "Our approach to alignment research (Leike, Schulman, and Wu; OpenAI; 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.\n",
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/collin-burns-weak-to-strong-generalization",
    "Description from MAIA / AISF / Elsewhere": "[Paper] aims to make empirical progress on supervising superhuman AI systems by considering an analogous problem: supervising strong AI systems using weaker AI systems. From the abstract:\n\nWe study a range of pretrained language models in the GPT-4 family on NLP, chess, and reward modeling tasks and find that when we naively finetune strong models to directly imitate weak model supervision, they consis- tently perform better than their weak supervisors, a phenomenon we call weak-to- strong generalization. However, naive generalization is still far from recovering the full capabilities of strong models trained with ground truth supervision. These results suggest that existing methods for aligning models with human supervision may indeed break down when applied to superhuman models in the future.",
    "Expert6 comments": "\n",
    "Expert6: Top Paper": true,
    "Link": "https://cdn.openai.com/papers/weak-to-strong-generalization.pdf",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Scalable oversight"
    ],
    "Supplemental Material": "https://openai.com/research/weak-to-strong-generalization",
    "Title": "Weak-To-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision (Burns et al., 2023)",
    "Type": [
      "Paper",
      "Blog post",
      "Video"
    ]
  },
  {
    "Abstract": "    We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. \n",
    "Link": "https://arxiv.org/abs/2109.07958",
    "ML Subfield": [
      "Applied ML",
      "NLP",
      "Robustness and Adversariality"
    ],
    "ML Subtopic": [
      "NLP: Language Modeling",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Deception"
    ],
    "Title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods (Lin et al., 2021)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at this http URL. \n",
    "Link": "http://arxiv.org/abs/2305.13860",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study (Liu et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an exhaustive taxonomy of such risks. Many exhaustive taxonomies are possible, and some are useful—particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.\n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2306.06924.pdf",
    "ML Subfield": [
      "Domain General",
      "Human Model Interaction",
      "Applied ML"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "TASRA: A Taxonomy and Analysis of Societal-Scale Risks from AI (Critch and Russell, 2023)",
    "Twitter": "https://twitter.com/AndrewCritchCA/status/1668476943208169473",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://theaidigest.org/progress-and-dangers",
    "ML Subfield": [
      "Applied ML",
      "Domain General",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Capabilities of LLMs",
      "Forecasting",
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "How fast is AI improving? (AI Digest; 2023)",
    "Type": [
      "Other"
    ]
  },
  {
    "Abstract": "    Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable. \n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2106.09667",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision",
      "Applied ML"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Safety Topic": [
      "Trojans"
    ],
    "Title": "Poisoning and Backdooring Contrastive Learning (Carlini and Terzis, 2022)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Forecasting"
    ],
    "Title": "Expert Survey on Progress in AI (AI Impacts, 2022)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Abstract": "    The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper, we introduce a benchmark technique for detecting backdoor attacks (aka Trojan attacks) on deep convolutional neural networks (CNNs). We introduce the concept of Universal Litmus Patterns (ULPs), which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output (i.e., classifying the network as \\`clean' or \\`corrupted'). This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on thousands of networks with different architectures trained on four benchmark datasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST, CIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can be found here this https URL. \n",
    "Link": "https://arxiv.org/abs/1906.10842",
    "ML Subfield": [
      "Vision",
      "Robustness and Adversariality",
      "Security"
    ],
    "ML Subtopic": [
      "Applied ML: Autonomous Vehicles"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Safety Topic": [
      "Trojans"
    ],
    "Title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs (Kolouri et al., 2020)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.\n",
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/zico-kolter-adversarial-attacks-on-aligned-language-models",
    "Goes online as Top Paper": true,
    "Link": "http://arxiv.org/abs/2307.15043",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "PC: Top Paper": true,
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Universal and Transferable Adversarial Attacks on Aligned Language Models (Zou et al., 2023)",
    "Type": [
      "Paper",
      "Video"
    ]
  },
  {
    "Description from MAIA / AISF / Elsewhere": "Krakovna et al. showcase examples of agents exploiting mistaken training specifications in simple environments. Note that “specification gaming” is an umbrella term which includes reward hacking as well as similar behavior by non-RL agents.",
    "Link": "https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity",
    "ML Subfield": [
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "RL: Simulations",
      "RL: Games",
      "Applied ML: Robotics"
    ],
    "Safety Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "15",
    "Title": "Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020)",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/4bf4ddd8-3876-47f6-ac5a-c6c1fbf8eae7",
    "Type": [
      "Blog post"
    ],
    "VK: Top Paper": true
  },
  {
    "Link": "https://www.alignment-workshop.com/nola-talks/been-kim-alignment-and-interpretability-how-we-might-get-it-right",
    "ML Subfield": [
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Title": "Alignment and Interpretability: How we might get it right (Been Kim, 33-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/nola-talks/adam-gleave-agi-safety-risks-and-research-directions",
    "ML Subfield": [
      "Security",
      "Domain General",
      "Applied ML"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Overview"
    ],
    "Title": "AGI Safety: Risks and Research Directions (Adam Gleave, 31-min video)\n",
    "Type": [
      "Video"
    ]
  },
  {
    "Abstract": "Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions span textual and numeric domains, and involve a range of real-world complexities. We evaluate methods that use pretrained language models (LMs) to produce descriptions of function behavior in natural language and code. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built from an LM with black-box access to functions, can infer function structure, acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, AIA descriptions tend to capture global function behavior and miss local details. These results suggest that FIND will be useful for evaluating more sophisticated interpretability methods before they are applied to real-world models. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] constructs a variety of complex functions and allows an interpreter to sample inputs and outputs from the function. The interpreter then attempts to provide a natural language explanation of the function, and code which implements it. The proposed code is evaluated against the original on a set of inputs, and an LLM judge evaluates the natural language explanation.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2309.03886",
    "ML Subfield": [
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Title": "FIND: A Function Description Benchmark for Evaluating Interpretability Methods (Schwettmann et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize. \n",
    "Link": "https://arxiv.org/abs/1903.12261",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision",
      "Model Evaluation"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization",
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (Hendrycks and Dietterich, 2019)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment. \n",
    "Description from MAIA / AISF / Elsewhere": "Gao et al. investigate quantitative trends in how ground-truth reward scales when optimizing against a proxy learned reward model.",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/abs/2210.10760",
    "ML Subfield": [
      "Applied ML",
      "Optimization"
    ],
    "Safety Category": [
      "Reward misspecification and goal misgeneralization",
      "Alignment <-> RLHF"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "25",
    "Title": "Scaling Laws for Reward Model Overoptimization (Gao et al., 2022) ",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true,
    "What sections to read from MAIA": "(sections 1-3 only) "
  },
  {
    "Abstract": "    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] evaluates the ethical decision-making ability of AI agents in text-based social environments. The paper finds a tradeoff between reward maximization and ethical behavior. Developers of AI agents can use this benchmark to evaluate the ability of their agents to behave ethically.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2304.03279",
    "ML Subfield": [
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark (Pan et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Good for adversarial robustness folks, one of the few adversarial robustness papers that has x-risk relevance. (And I think there's a lot of relevant work that \\*could\\* be done here, it's just mostly not.)\n",
    "AG: Top Paper": true,
    "Abstract": "In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.\n",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2205.01663.pdf",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "PC comments": "your recommendations and resource center are at risk of seeming parochial / slanted towards researchers in my social circles... An example from the resources page: I think the RR adversarial training paper shouldn't make a highlights reel and should be replaced with more recent work on jailbreaking large LMs (e.g. \"Universal attacks\" which you have in the longer page + \"Are aligned neural networks adversarially aligned?\" which does a better job of being outside of our circles), or past work on automated red teaming of LMs, or results in non-LM settings.\n",
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Adversarial training for high-stakes reliability (Ziegler et al., 2022)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Accessible paper\n",
    "AG: Top Paper": true,
    "Abstract": "In this work, we used a safe language generation task (\\`\\`avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models. \n",
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/pdf/2206.05802.pdf",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Scalable oversight"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Self-critiquing models for assisting human evaluators (Saunders et al., 2022)",
    "Type": [
      "Paper"
    ],
    "What sections to read from MAIA": "(sections 1-3 only)"
  },
  {
    "Link": "https://bounded-regret.ghost.io/more-is-different-for-ai/",
    "ML Subfield": [
      "Domain General"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "More is Different for AI (Steinhardt, 2022)",
    "Type": [
      "Blog post"
    ],
    "VK comments": "Great perspective on how to think about ML safety risks that is compelling to an ML audience\n",
    "VK: Top Paper": true
  },
  {
    "AG: Top Paper": true,
    "Abstract": "In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.\n",
    "Context phrase (please add!)": "\n",
    "Description from MAIA / AISF / Elsewhere": "This reading argues that reward hacking will become much harder to detect once AIs have situational awareness: the skill of being able to apply abstract knowledge to the specific context in which they’re run.",
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/abs/2209.00626",
    "ML Methods Tag": [
      "RLHF"
    ],
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)",
      "Reward misspecification and goal misgeneralization",
      "Alignment <-> RLHF",
      "Deception",
      "Overview"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "5+20",
    "Title": "The Alignment Problem from a Deep Learning Perspective (Ngo et al., 2022)",
    "Twitter": "https://twitter.com/RichardMCNgo/status/1603862969276051457",
    "Type": [
      "Paper"
    ],
    "VK comments": "Best writeup of the main alignment threat model that is accessible / compelling to an ML audience\n",
    "VK: Top Paper": true,
    "What sections to read from MAIA": "(only section 2: Deceptive reward hacking, sections 3-4)"
  },
  {
    "Blog or Video": "https://www.anthropic.com/index/decomposing-language-models-into-understandable-components",
    "Context phrase (please add!)": "Substantially resolves superposition, which was previously considered to be a major hurdle to mechanistic interpretability -Vael\n",
    "Goes online as Top Paper": true,
    "Link": "https://transformer-circuits.pub/2023/monosemantic-features/index.html",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Research bottlenecks / limitations"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (Bricken et al., 2023)",
    "Twitter": "https://twitter.com/ch402/status/1709998674087227859",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Particularly good for NLP researchers (unsurprisingly)\n",
    "AG: Top Paper": true,
    "Context phrase (please add!)": "Sam Bowman (NYU, Anthropic) describes his motivation for doing safety research - Vael\n",
    "Link": "https://wp.nyu.edu/arg/why-ai-safety/",
    "ML Subfield": [
      "NLP"
    ],
    "Safety Category": [
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Title": "Why I Think More NLP Researchers Should Engage with AI Safety Concerns (Bowman, 2022)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Blog or Video": "https://transformer-circuits.pub/2023/interpretability-dreams/index.html#epistemic-foundation",
    "Link": "https://www.alignment-workshop.com/sf-talks/chris-olah-looking-inside-neural-networks-with-mechanistic-interpretabili",
    "ML Subfield": [
      "Vision",
      "Theory"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Research bottlenecks / limitations"
    ],
    "Title": "Looking Inside Neural Networks with Mechanistic Interpretability (Chris Olah, 41-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Abstract": "As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model’s actual reasoning, which isnot always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks,sometimes approaching that of CoT while improving the faithfulness of the model’s stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestionsin separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performancegains of CoT. Our results show it is possible to improve the faithfulness of model-generated rea-soning; continued improvements may lead to reasoning that enables us to verify the correctnessand safety of LLM behavior.\n",
    "Link": "https://www-files.anthropic.com/production/files/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning.pdf",
    "ML Subfield": [
      "NLP",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Deception"
    ],
    "Safety Topic": [
      "Sycophancy"
    ],
    "Title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning (Radhakrishnan et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG comments": "Very over-rated paper IMO. Collin's work is interesting but in general a bit sloppily executed. See e.g. <https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast> arguing the CCS method doesn't really add anything beyond contrast pairs plus PCA.\n",
    "Abstract": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels. \n",
    "Blog or Video": "https://www.alignmentforum.org/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without",
    "Expert6: Top Paper": true,
    "Link": "https://arxiv.org/pdf/2212.03827.pdf",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Discovering Latent Knowledge In Language Models Without Supervision (Burns et al., 2022) ",
    "Twitter": "https://twitter.com/CollinBurns4/status/1600892261633785856",
    "Type": [
      "Paper"
    ],
    "VK comments": "This paper was a bit oversold\n",
    "What sections to read from MAIA": "(only sections 1-3)"
  },
  {
    "Abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance. \n",
    "Link": "https://arxiv.org/abs/1812.04606",
    "ML Subfield": [
      "Vision"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Detection of out-of-distribution or malicious behavior"
    ],
    "Title": "Deep Anomaly Detection with Outlier Exposure (Hendrycks at al., 2019)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://www.alignment-workshop.com/nola-talks/sam-bowman-adversarial-scalable-oversight-for-truthfulness-work-in-progr",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: Question Answering"
    ],
    "Safety Category": [
      "Scalable oversight",
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Adversarial Scalable Oversight for Truthfulness: Work in Progress (Sam Bowman, 29-min video)\n",
    "Transcripts / Audio / Slides": "https://cims.nyu.edu/~sbowman/alignment_workshop_2023.pdf",
    "Type": [
      "Video"
    ]
  },
  {
    "Abstract": "We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time. \n",
    "Description from MAIA / AISF / Elsewhere": "[Paper] used gradient ascent to minimize the probability of undesirable completions while maintaining model performance on inputs where no unlearning was necessary.\n\nIt is worth mentioning none of these unlearning methods are particularly performant, and better baselines are needed.",
    "Expert6 comments": "\n",
    "Link": "https://arxiv.org/abs/2310.10683",
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Title": "Large Language Model Unlearning (Yao et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "    In this paper, we benchmark the usefulness of interpretability tools on debugging tasks. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when an interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation. A website for this paper and its code is at this https URL \n",
    "Link": "https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "RS comments": "\n",
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Scalable oversight"
    ],
    "Safety Topic": [
      "Automated interpretability"
    ],
    "Title": "Language models can explain neurons in language models (Bills et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Abstract": "Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors. \n",
    "Expert6: Top Paper": true,
    "Goes online as Top Paper": true,
    "Link": "https://arxiv.org/abs/2201.03544",
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML"
    ],
    "ML Subtopic": [
      "RL: Simulations",
      "Applied ML: Autonomous Vehicles",
      "Applied ML: Medicine and Health",
      "RL: Games"
    ],
    "Safety Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models (Pan, Bhatia and Steinhardt, 2022)",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true
  },
  {
    "Abstract": "    Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create \"latent saliency maps\" that can help explain predictions in human terms. \n",
    "Description from MAIA / AISF / Elsewhere": "Li et al. search for evidence of a world model in an Othello-playing network by training probes to extract the model’s knowledge of the board state and performing interventions to change the network’s internal representation of the board state.",
    "Link": "https://arxiv.org/abs/2210.13382",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task (Li et al., 2023)",
    "Type": [
      "Paper"
    ]
  },
  {
    "AG: Top Paper": true,
    "Context phrase (please add!)": "Outlines the superposition problem, a major blocker to scaling mechanistic interpretability to large models -Vael\n",
    "Expert6: Top Paper": true,
    "Link": "https://transformer-circuits.pub/2022/toy_model/index.html",
    "ML Subfield": [
      "Theory"
    ],
    "Safety Category": [
      "Transparency / Interpretability / Model internals / Latent knowledge",
      "Research bottlenecks / limitations"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Supplemental Material": "Addendum: https://transformer-circuits.pub/2023/toy-double-descent/index.html",
    "Time from MAIA / AISF / Elsewhere (min)": "30",
    "Title": "Toy models of superposition (Elhage et al., 2022)",
    "Type": [
      "Paper"
    ],
    "VK: Top Paper": true,
    "What sections to read from MAIA": "(up to the end of section 3: superposition as a phase change)"
  },
  {
    "AG comments": "Adversarial robustness is a bit sparse right now. Not an area I'd push people towards in general, but there are so /many/ people in ML doing adversarial robustness work that it seems like a good entry point for them at least. I'd plug this work on adversarial policies in Go AI both as a demo (superintelligent systems might be vulnerable) and as an illustration of a different threat model they might want to consider working in.\n",
    "Expert6: Top Paper": true,
    "Link": "https://far.ai/post/2023-07-superhuman-go-ais/",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Applied ML"
    ],
    "Safety Category": [
      "Adversaries / Robustness / Generalization"
    ],
    "Title": "Even Superhuman Go AIs Have Surprising Failures Modes (Gleave et al., 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Expert6: Top Paper": true,
    "Link": "https://bounded-regret.ghost.io/emergent-deception-optimization/",
    "ML Subfield": [
      "Applied ML",
      "Domain General"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks"
    ],
    "Safety Category": [
      "Deception",
      "Major problems (misalignment, misuse, threat models)"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Time from MAIA / AISF / Elsewhere (min)": "20",
    "Title": "Emergent Deception and Emergent Optimization (Steinhardt, 2023)",
    "Type": [
      "Blog post"
    ],
    "VK: Top Paper": true
  },
  {
    "Abstract": "    Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions. \n",
    "Link": "https://arxiv.org/abs/1706.04599",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "Safety Category": [
      "Model evaluations / monitoring / detection"
    ],
    "Safety Topic": [
      "Uncertainty"
    ],
    "Title": "On Calibration of Modern Neural Networks (Guo et al., 2017)",
    "Type": [
      "Paper"
    ]
  }
]
