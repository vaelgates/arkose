[
  {
    "Blog or Video": "https://www.anthropic.com/index/anthropics-responsible-scaling-policy",
    "Link": "https://www.anthropic.com/news/anthropics-responsible-scaling-policy",
    "ML Subfield": [
      "Applied ML",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "AI Governance"
    ],
    "Title": "Anthropic's Responsible Scaling Policy (Anthropic, 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1792598295388279124"
  },
  {
    "Link": "https://www.science.org/doi/10.1126/science.ade9097",
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "RL: Games",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks",
      "NLP: LLMs"
    ],
    "Category": [
      "Deception",
      "Reinforcement Learning"
    ],
    "Title": "Human-Level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning (Bakhtin et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2302.10329",
    "ML Subfield": [
      "Domain General",
      "Reinforcement Learning",
      "Human Model Interaction"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "Harms from Increasingly Agentic Algorithmic Systems (Chan et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/_achan96_/status/1656690639264792579",
    "Abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems. \n"
  },
  {
    "Link": "https://metr.github.io/autonomy-evals-guide/",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "METR's Autonomy Evaluation Resources",
    "Type": [
      "Other"
    ],
    "Twitter": "https://x.com/METR_Evals/status/1768684026792190410"
  },
  {
    "Blog or Video": "https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback",
    "Link": "https://arxiv.org/abs/2212.08073",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1603791161419698181",
    "Abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels. \n"
  },
  {
    "Blog or Video": "https://www.youtube.com/watch?v=U2zJuTLzIm8&t=2s",
    "Link": "https://arxiv.org/abs/2211.03540",
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Measuring Progress on Scalable Oversight for Large Language Models (Bowman et al., 2022)",
    "Type": [
      "Paper",
      "Video"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1590019597109202946",
    "Abstract": "Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks. \n"
  },
  {
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/zico-kolter-adversarial-attacks-on-aligned-language-models",
    "Link": "http://arxiv.org/abs/2307.15043",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Universal and Transferable Adversarial Attacks on Aligned Language Models (Zou et al., 2023)",
    "Type": [
      "Paper",
      "Video"
    ],
    "Twitter": "https://x.com/andyzou_jiaming/status/1684766170766004224",
    "Abstract": "    Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2403.13793",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Title": "Evaluating Frontier Models for Dangerous Capabilities (Phuong et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/tshevl/status/1770744344669990981",
    "Abstract": "To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new \"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2312.06942",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality"
    ],
    "Category": [
      "Deception",
      "Scalable oversight"
    ],
    "Title": "AI Control: Improving Safety Despite Intentional Subversion (Greenblatt et al., 2024)",
    "Type": [
      "Paper"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2205.01663",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Title": "Adversarial training for high-stakes reliability (Ziegler et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2209.00626",
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML",
      "Domain General"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "The Alignment Problem from a Deep Learning Perspective (Ngo et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/RichardMCNgo/status/1603862969276051457",
    "Abstract": "In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.\n"
  },
  {
    "Link": "https://github.com/UKGovernmentBEIS/inspect_ai",
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Title": "UK AISI Inspect",
    "Type": [
      "Other"
    ],
    "Twitter": "https://x.com/soundboy/status/1788910977003504010"
  },
  {
    "Link": "https://llm-safety-challenges.github.io/",
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models (Anwar et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/DavidSKrueger/status/1779900511627452467",
    "Abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose 200+, concrete research questions.\n"
  },
  {
    "Link": "https://rome.baulab.info/",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Unlearning and Knowledge Editing"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Title": "Locating and Editing Factual Associations in GPT (Meng et al., 2022)\n",
    "Type": [
      "Paper"
    ],
    "Abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\n",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/c902ed39-f622-4296-b312-73339fab7b6e"
  },
  {
    "Link": "https://arxiv.org/abs/2310.01405",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Title": "Representation Engineering: A Top-Down Approach to AI Transparency (Zou et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/andyzou_jiaming/status/1709365304789238201",
    "Abstract": "    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2306.15447",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Are aligned neural networks adversarially aligned? (Carlini et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Large language models are now tuned to align with the goals of their creators, namely to be \"helpful and harmless.\" These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2402.06782",
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "Type": [
      "Paper"
    ],
    "Abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.\n"
  },
  {
    "Link": "https://distill.pub/2020/circuits/zoom-in/",
    "ML Subfield": [
      "Vision",
      "Theory"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Zoom In: An Introduction to Circuits (Olah et al., 2020)",
    "Type": [
      "Paper"
    ],
    "Abstract": "\n",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/632feda4-71a5-42a2-84a1-588c6b9ea1ad"
  },
  {
    "Link": "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/?utm_source=substack&utm_medium=email",
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Title": "Frontier Safety Framework",
    "Abstract": "The Frontier Safety Framework is our rst version of a set of protocols that aims to address severe risks\nthat may arise from poweul capabilities of future foundation models. In focusing on these risks at the\nmodel level, it is intended to complement Google’s existing suite of AI responsibility and safety\npractices, and enable AI innovation and deployment consistent with our AI Principles.\n"
  },
  {
    "Blog or Video": "https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training",
    "Link": "https://arxiv.org/abs/2401.05566",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality"
    ],
    "Category": [
      "Deception",
      "Non-Adversarial Robustness"
    ],
    "Title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training (Hubinger et al., 2024)\n",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/AnthropicAI/status/1745854907968880970",
    "Abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.\n"
  },
  {
    "Link": "https://www.safe.ai/ai-risk",
    "ML Subfield": [
      "Domain General",
      "Applied ML"
    ],
    "Category": [
      "Overviews and agendas",
      "Why large-scale safety?"
    ],
    "Title": "An Overview of Catastrophic AI Risks (Hendrycks et al., 2023)",
    "Type": [
      "Blog post",
      "Paper"
    ],
    "Twitter": "https://x.com/DanHendrycks/status/1671894767331061763",
    "Abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.\n",
    "Transcripts / Audio / Slides": "https://www.safe.ai/ai-risk"
  },
  {
    "Blog or Video": "https://www.youtube.com/watch?v=HiYWwjma3xE&t=1687s",
    "Link": "https://arxiv.org/abs/2212.09251",
    "ML Subfield": [
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "Scalable oversight"
    ],
    "Title": "Discovering Language Model Behaviors with Model-Written Evaluations (Perez et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1604883576218341376?lang=en",
    "Abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2304.13734",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Interpretability / explainability",
      "Deception"
    ],
    "Safety Topic": [
      "Sycophancy"
    ],
    "Title": "The Internal State of an LLM Knows When It's Lying (Azaria and Mitchell, 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\\\% to 83\\\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios. \n"
  },
  {
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/collin-burns-weak-to-strong-generalization",
    "Link": "https://arxiv.org/abs/2312.09390",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Weak-To-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision (Burns et al., 2023)",
    "Type": [
      "Paper",
      "Blog post",
      "Video"
    ],
    "Twitter": "https://x.com/CollinBurns4/status/1735350133926314431",
    "Abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.\n"
  },
  {
    "Link": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html",
    "Category": [
      "Interpretability / explainability",
      "Unlearning and Knowledge Editing"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1792935506587656625"
  }
]
