[
  {
    "Blog or Video": "https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924",
    "Link": "https://arxiv.org/abs/2210.01790",
    "ML Subfield": [
      "Reinforcement Learning",
      "Theory"
    ],
    "ML Subtopic": [
      "RL: Games",
      "Applied ML: Cognitive Tasks",
      "NLP: LLMs"
    ],
    "Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals (Shah et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/rohinmshah/status/1578391329461133315",
    "Abstract": "The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems. \n"
  },
  {
    "Link": "https://www.alignment-workshop.com/sf-talks/dan-hendrycks-surveying-safety-research-directions",
    "ML Subfield": [
      "Domain General"
    ],
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "Surveying Safety Research Directions (Dan Hendrycks, 40-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Blog or Video": "https://www.anthropic.com/index/anthropics-responsible-scaling-policy",
    "Link": "https://www.anthropic.com/news/anthropics-responsible-scaling-policy",
    "ML Subfield": [
      "Applied ML",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "AI Governance"
    ],
    "Title": "Anthropic's Responsible Scaling Policy (Anthropic, 2023)",
    "Type": [
      "Other"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1792598295388279124"
  },
  {
    "Link": "https://metr.org/AI_R_D_Evaluation_Report.pdf",
    "Category": [
      "Security"
    ],
    "Title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts (Wijk et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7 challenging, open- ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-k with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics—e.g. an agent wrote a faster custom Triton kernel than any of our human experts’—and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research\n"
  },
  {
    "Blog or Video": "https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks",
    "Link": "https://arxiv.org/abs/2305.15324",
    "ML Subfield": [
      "Model Evaluation",
      "Domain General"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "AI Governance"
    ],
    "Title": "Model evaluation for extreme risks (Shevlane et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through \"dangerous capability evaluations\") and the propensity of models to apply their capabilities for harm (through \"alignment evaluations\"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security. \n"
  },
  {
    "Link": "https://arxiv.org/abs/1906.01820",
    "ML Subfield": [
      "Optimization",
      "Theory",
      "Domain General"
    ],
    "Category": [
      "Deception"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Title": "Risks from Learned Optimization in Advanced Machine Learning Systems (Hubinger et al., 2021)",
    "Type": [
      "Paper"
    ],
    "Abstract": "We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research. \n"
  },
  {
    "Link": "https://www.science.org/doi/10.1126/science.ade9097",
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "RL: Games",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks",
      "NLP: LLMs"
    ],
    "Category": [
      "Deception"
    ],
    "Title": "Human-Level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning (Bakhtin et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2202.03286",
    "ML Subfield": [
      "NLP",
      "Model Evaluation",
      "Robustness and Adversariality",
      "Applied ML",
      "Security"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Title": "Red Teaming Language Models with Language Models (Perez et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\"red teaming\") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2110.13136",
    "ML Subfield": [
      "Model Evaluation",
      "NLP",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "RL: Games"
    ],
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally (Hendrycks et al., 2021)",
    "Type": [
      "Paper"
    ],
    "Abstract": "When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance. \n"
  },
  {
    "Link": "https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.kimhqj72mew4",
    "ML Subfield": [
      "Applied ML"
    ],
    "ML Subtopic": [
      "Applied ML: Efficiency and Hardware"
    ],
    "Category": [
      "AI Governance"
    ],
    "Safety Topic": [
      "Compute governance"
    ],
    "Title": "\"Transformative AI and Compute\" Reading List (Heim, 2022)",
    "Type": [
      "Other"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2302.10329",
    "ML Subfield": [
      "Domain General",
      "Reinforcement Learning",
      "Human Model Interaction"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "Harms from Increasingly Agentic Algorithmic Systems (Chan et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/_achan96_/status/1656690639264792579",
    "Abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2403.10462",
    "Category": [
      "AI Governance",
      "Security",
      "Model evaluations and benchmarks"
    ],
    "Title": "Safety Cases: How to Justify the Safety of Advanced AI Systems (Clymer et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and -- if AI systems become much more powerful -- deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.\n"
  },
  {
    "Link": "https://metr.github.io/autonomy-evals-guide/",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "METR's Autonomy Evaluation Resources",
    "Type": [
      "Other"
    ],
    "Twitter": "https://x.com/METR_Evals/status/1768684026792190410"
  },
  {
    "Blog or Video": "https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback",
    "Link": "https://arxiv.org/abs/2212.08073",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality",
      "Human Model Interaction"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1603791161419698181",
    "Abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels. \n"
  },
  {
    "Blog or Video": "https://openai.com/research/debate",
    "Link": "https://arxiv.org/abs/1805.00899",
    "ML Subfield": [
      "NLP"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Safety Topic": [
      "Debate"
    ],
    "Title": "AI safety via debate (Irving, Christiano and Amodei, 2018)",
    "Type": [
      "Paper"
    ],
    "Abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2412.14093",
    "Category": [
      "Deception"
    ],
    "Title": "Alignment Faking in Large Language Models (Greenblatt et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2403.19647",
    "Category": [
      "Interpretability / explainability"
    ],
    "Title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models (Marks et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.\n"
  },
  {
    "Blog or Video": "https://www.youtube.com/watch?v=U2zJuTLzIm8&t=2s",
    "Link": "https://arxiv.org/abs/2211.03540",
    "ML Subfield": [
      "NLP",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Measuring Progress on Scalable Oversight for Large Language Models (Bowman et al., 2022)",
    "Type": [
      "Paper",
      "Video"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1590019597109202946",
    "Abstract": "Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks. \n"
  },
  {
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/zico-kolter-adversarial-attacks-on-aligned-language-models",
    "Link": "http://arxiv.org/abs/2307.15043",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Universal and Transferable Adversarial Attacks on Aligned Language Models (Zou et al., 2023)",
    "Type": [
      "Paper",
      "Video"
    ],
    "Twitter": "https://x.com/andyzou_jiaming/status/1684766170766004224",
    "Abstract": "    Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2403.05030",
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training (Casper et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Despite extensive diagnostics and debugging by developers, AI systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training (AT) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use it to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness to novel attacks and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2403.13793",
    "ML Subfield": [
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "Security"
    ],
    "Title": "Evaluating Frontier Models for Dangerous Capabilities (Phuong et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/tshevl/status/1770744344669990981",
    "Abstract": "To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new \"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.\n"
  },
  {
    "Link": "https://www.alignment-workshop.com/nola-talks/adam-gleave-agi-safety-risks-and-research-directions",
    "ML Subfield": [
      "Security",
      "Domain General",
      "Applied ML"
    ],
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "AGI Safety: Risks and Research Directions (Adam Gleave, 31-min video)\n",
    "Type": [
      "Video"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2312.06942",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality"
    ],
    "Category": [
      "Deception",
      "Scalable oversight"
    ],
    "Title": "AI Control: Improving Safety Despite Intentional Subversion (Greenblatt et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (\"protocols\") that are robust to intentional subversion. \nWe investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases. \nWe investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.\n"
  },
  {
    "Link": "https://arxiv.org/abs/1903.12261",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision",
      "Model Evaluation"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Safety Topic": [
      "Benchmarking"
    ],
    "Title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (Hendrycks and Dietterich, 2019)",
    "Type": [
      "Paper"
    ],
    "Abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2210.10760",
    "ML Subfield": [
      "Applied ML",
      "Optimization"
    ],
    "Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "Scaling Laws for Reward Model Overoptimization (Gao et al., 2022) ",
    "Type": [
      "Paper"
    ],
    "Abstract": "    In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2205.01663",
    "ML Subfield": [
      "Robustness and Adversariality",
      "NLP"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Title": "Adversarial training for high-stakes reliability (Ziegler et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2407.00215",
    "Category": [
      "Scalable oversight",
      "Non-Adversarial Robustness"
    ],
    "Title": "LLM Critics Help Catch LLM Bugs (McAleese et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains \"critic\" models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as \"flawless\", even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.\n"
  },
  {
    "Link": "https://www.youtube.com/watch?v=u0619QrWxQc&t=758s",
    "Category": [
      "Theory"
    ],
    "Title": "Formalizing Explanations of Neural Network Behaviours (Paul Christiano, 60-min video)",
    "Type": [
      "Video"
    ],
    "Abstract": "Existing research on mechanistic interpretability usually tries to develop an informal human understanding of “how a model works,” making it hard to evaluate research results and raising concerns about scalability. Meanwhile formal proofs of model properties seem far out of reach both in theory and practice. In this talk I’ll discuss an alternative strategy for “explaining” a particular behavior of a given neural network. This notion is much weaker than proving that the network exhibits the behavior, but may still provide similar safety benefits. This talk will primarily motivate a research direction and a set of theoretical questions rather than presenting results.\n"
  },
  {
    "Link": "https://bounded-regret.ghost.io/more-is-different-for-ai/",
    "ML Subfield": [
      "Domain General"
    ],
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "More is Different for AI (Steinhardt, 2022)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?usp=sharing",
    "Category": [
      "Theory"
    ],
    "Title": "Eliciting Latent Knowledge: how to tell if your eyes deceive you (Christiano et al., 2021)",
    "Type": [
      "Other"
    ],
    "Abstract": "Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.\n\nBut some action sequences could tamper with the cameras so they show happy          humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.\n\nIn these cases, the prediction model \"knows\" facts (like \"the camera was tampered with\") that are not visible on camera but would change our evaluation of the predicted future if we learned them. **How can we train this model to report its latent knowledge of off-screen events?**\n"
  },
  {
    "Link": "https://arxiv.org/abs/2209.00626",
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML",
      "Domain General"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "The Alignment Problem from a Deep Learning Perspective (Ngo et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/RichardMCNgo/status/1603862969276051457",
    "Abstract": "In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.\n"
  },
  {
    "Blog or Video": "https://www.anthropic.com/index/decomposing-language-models-into-understandable-components",
    "Link": "https://transformer-circuits.pub/2023/monosemantic-features/index.html",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (Bricken et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/ch402/status/1709998674087227859"
  },
  {
    "Link": "https://wp.nyu.edu/arg/why-ai-safety/",
    "ML Subfield": [
      "NLP"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "Why I Think More NLP Researchers Should Engage with AI Safety Concerns (Bowman, 2022)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://github.com/UKGovernmentBEIS/inspect_ai",
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Title": "UK AISI Inspect",
    "Type": [
      "Other"
    ],
    "Twitter": "https://x.com/soundboy/status/1788910977003504010"
  },
  {
    "Link": "https://arxiv.org/abs/1812.04606",
    "ML Subfield": [
      "Vision"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Safety Topic": [
      "Detection of out-of-distribution or malicious behavior"
    ],
    "Title": "Deep Anomaly Detection with Outlier Exposure (Hendrycks at al., 2019)",
    "Type": [
      "Paper"
    ],
    "Abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2310.10683",
    "ML Subfield": [
      "NLP",
      "Applied ML",
      "Theory"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Title": "Large Language Model Unlearning (Yao et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2201.03544",
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML"
    ],
    "ML Subtopic": [
      "RL: Simulations",
      "Applied ML: Autonomous Vehicles",
      "Applied ML: Medicine and Health",
      "RL: Games"
    ],
    "Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models (Pan, Bhatia and Steinhardt, 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors. \n"
  },
  {
    "Link": "https://transformer-circuits.pub/2022/toy_model/index.html",
    "ML Subfield": [
      "Theory"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Toy models of superposition (Elhage et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1570087876053942272"
  },
  {
    "Link": "https://far.ai/post/2023-07-superhuman-go-ais/",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Applied ML",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "RL: Games"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Even Superhuman Go AIs Have Surprising Failures Modes (Gleave et al., 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://bounded-regret.ghost.io/emergent-deception-optimization/",
    "ML Subfield": [
      "Applied ML",
      "Domain General"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks"
    ],
    "Category": [
      "Deception"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Title": "Emergent Deception and Emergent Optimization (Steinhardt, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2110.03605",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Title": "Robust Feature-Level Adversaries are Interpretability Tools (Casper et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/StephenLCasper/status/1598029118205218816",
    "Abstract": "The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create \"feature-level\" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing \"copy/paste\" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at this https URL\n"
  },
  {
    "Link": "https://llm-safety-challenges.github.io/",
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models (Anwar et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/DavidSKrueger/status/1779900511627452467",
    "Abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose 200+, concrete research questions.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2411.02306",
    "Category": [
      "Deception"
    ],
    "Title": "Targeted Manipulation and Deception Emerge when Optimizing LLMs for User Feedback (Williams et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "As LLMs become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the AI to resort to manipulative tactics to obtain positive feedback, and some users may be especially vulnerable to such tactics. We study this phenomenon by training LLMs with Reinforcement Learning with simulated user feedback. We have three main findings: 1) Extreme forms of \"feedback gaming\" such as manipulation and deception can reliably emerge in domains of practical LLM usage; 2) Concerningly, even if only <2% of users are vulnerable to manipulative strategies, LLMs learn to identify and surgically target them while behaving appropriately with other users, making such behaviors harder to detect; 3 To mitigate this issue, it may seem promising to leverage continued safety training or LLM-as-judges during training to filter problematic outputs. To our surprise, we found that while such approaches help in some settings, they backfire in others, leading to the emergence of subtler problematic behaviors that would also fool the LLM judges. Our findings serve as a cautionary tale, highlighting the risks of using gameable feedback sources -- such as user feedback -- as a target for RL.\n"
  },
  {
    "Link": "https://rome.baulab.info/",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Title": "Locating and Editing Factual Associations in GPT (Meng et al., 2022)\n",
    "Type": [
      "Paper"
    ],
    "Abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\n",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/c902ed39-f622-4296-b312-73339fab7b6e"
  },
  {
    "Link": "https://www.anthropic.com/index/core-views-on-ai-safety",
    "ML Subfield": [
      "Domain General"
    ],
    "Category": [
      "Overviews and agendas",
      "Why large-scale safety?"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Core Views on AI Safety: When, Why, What, and How (Anthropic, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Blog or Video": "https://www.alignmentforum.org/posts/EPLk8QxETC5FEhoxK/arc-evals-new-report-evaluating-language-model-agents-on",
    "Link": "https://arxiv.org/abs/2312.11671",
    "ML Subfield": [
      "Model Evaluation",
      "NLP",
      "Applied ML"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks (Kinniment et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "In this report, we explore the ability of language model agents to acquire resources,create copies of themselves, and adapt to novel challenges they encounter inthe wild. We refer to this cluster of capabilities as “autonomous replication andadaptation” or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecastingARA may be useful for informing measures around security, monitoring, andalignment. Additionally, once a system is capable of ARA, placing bounds on asystem’s capabilities may become significantly more difficult.We construct four simple example agents that combine language models with toolsthat allow them to take actions in the world. We then evaluate these agents on 12tasks relevant to ARA. We find that these language model agents can only completethe easiest tasks from this list, although they make some progress on the morechallenging tasks. Unfortunately, these evaluations are not adequate to rule out thepossibility that near-future agents will be capable of ARA. In particular, we do notthink that these evaluations provide good assurance that the “next generation” oflanguage models (e.g. 100x effective compute scaleup on existing models) willnot yield agents capable of ARA, unless intermediate evaluations are performedduring pretraining. Relatedly, we expect that fine-tuning of the existing modelscould produce substantially more competent agents, even if the fine-tuning is notdirectly targeted at ARA.\n"
  },
  {
    "Link": "https://www.alignment-workshop.com/sf-talks/ajeya-cotra-situational-awareness-makes-measuring-safety-tricky",
    "ML Subfield": [
      "Human Model Interaction",
      "Domain General",
      "Applied ML"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Safety Topic": [
      "Situational awareness (also instrumental convergence, inner misalignment)"
    ],
    "Title": " “Situational Awareness” Makes Measuring Safety Tricky (Ajeya Cotra, 40-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2303.11341",
    "ML Subfield": [
      "Applied ML",
      "Domain General",
      "Human Model Interaction",
      "Security"
    ],
    "ML Subtopic": [
      "Applied ML: Efficiency and Hardware"
    ],
    "Category": [
      "AI Governance"
    ],
    "Safety Topic": [
      "Compute governance"
    ],
    "Title": "What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring (Shavit, 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "As advanced machine learning systems' capabilities begin to play a significant role in geopolitics and societal order, it may become imperative that (1) governments be able to enforce rules on the development of advanced ML systems within their borders, and (2) countries be able to verify each other's compliance with potential future international agreements on advanced ML development. This work analyzes one mechanism to achieve this, by monitoring the computing hardware used for large-scale NN training. The framework's primary goal is to provide governments high confidence that no actor uses large quantities of specialized ML chips to execute a training run in violation of agreed rules. At the same time, the system does not curtail the use of consumer computing devices, and maintains the privacy and confidentiality of ML practitioners' models, data, and hyperparameters. The system consists of interventions at three stages: (1) using on-chip firmware to occasionally save snapshots of the the neural network weights stored in device memory, in a form that an inspector could later retrieve; (2) saving sufficient information about each training run to prove to inspectors the details of the training run that had resulted in the snapshotted weights; and (3) monitoring the chip supply chain to ensure that no actor can avoid discovery by amassing a large quantity of untracked chips. The proposed design decomposes the ML training rule verification problem into a series of narrow technical challenges, including a new variant of the Proof-of-Learning problem [Jia et al. '21].  \n"
  },
  {
    "Link": "https://arxiv.org/abs/2310.01405",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs",
      "Applied ML: Chatbots"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Model editing"
    ],
    "Title": "Representation Engineering: A Top-Down Approach to AI Transparency (Zou et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/andyzou_jiaming/status/1709365304789238201",
    "Abstract": "    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems. \n"
  },
  {
    "Link": "https://devinterp.com/2023",
    "Category": [
      "Theory"
    ],
    "Title": "Singular Learning Theory & Alignment 2023 (Conference Proceedings with recordings)",
    "Type": [
      "Video"
    ],
    "Abstract": "Conference proceedings on Developmental Interpretability (videos available [@DevInterp](https://www.youtube.com/@Devinterp)) \n"
  },
  {
    "Link": "https://arxiv.org/abs/2306.15447",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Are aligned neural networks adversarially aligned? (Carlini et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Large language models are now tuned to align with the goals of their creators, namely to be \"helpful and harmless.\" These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2209.07858",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality",
      "Model Evaluation"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned (Ganguli et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/AnthropicAI/status/1571988929800273932?lang=en",
    "Abstract": "    We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2301.10226",
    "ML Subfield": [
      "NLP",
      "Security"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Non-Adversarial Robustness"
    ],
    "Safety Topic": [
      "Security"
    ],
    "Title": "A Watermark for Large Language Models (Kirchenbauer et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security. \n"
  },
  {
    "Link": "https://arxiv.org/abs/1706.06083",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Theory",
      "Optimization"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Towards Deep Learning Models Resistant to Adversarial Attacks (Madry et al., 2018)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2402.06782",
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Debating with More Persuasive LLMs Leads to More Truthful Answers (Khan et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.\n"
  },
  {
    "Link": "https://distill.pub/2020/circuits/zoom-in/",
    "ML Subfield": [
      "Vision",
      "Theory"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Zoom In: An Introduction to Circuits (Olah et al., 2020)",
    "Type": [
      "Paper"
    ],
    "Abstract": "\n",
    "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/632feda4-71a5-42a2-84a1-588c6b9ea1ad"
  },
  {
    "Link": "https://arxiv.org/abs/1609.03543",
    "Category": [
      "Theory"
    ],
    "Title": "Logical Induction (Garrabrant et al., 2016)",
    "Type": [
      "Paper"
    ],
    "Abstract": "We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of _π_\n are difficult to predict, then a logical inductor learns to assign ≈10%\n probability to \"the _n_th digit of _π_ is a 7\" for large _n_. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever _ϕ_⟹_ψ_, P∞(_ϕ_)≤P∞(_ψ_), and so on); and logical inductors strictly dominate the universal semimeasure in the limit. \nThese properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence _ϕ_\n is associated with a stock that is worth $1 per share if [...]\n"
  },
  {
    "Link": "https://arxiv.org/abs/2311.14125",
    "ML Subfield": [
      "NLP"
    ],
    "Category": [
      "Scalable oversight",
      "Theory"
    ],
    "Safety Topic": [
      "Debate"
    ],
    "Title": "Scalable AI Safety via Doubly-Efficient Debate (Brown-Cohen et al., 2024) ",
    "Type": [
      "Paper"
    ],
    "Abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps. \n"
  },
  {
    "Link": "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/?utm_source=substack&utm_medium=email",
    "Category": [
      "Model evaluations and benchmarks"
    ],
    "Title": "Frontier Safety Framework (Dragan et al., 2024)",
    "Type": [
      "Other"
    ],
    "Abstract": "The Frontier Safety Framework is our rst version of a set of protocols that aims to address severe risks\nthat may arise from poweul capabilities of future foundation models. In focusing on these risks at the\nmodel level, it is intended to complement Google’s existing suite of AI responsibility and safety\npractices, and enable AI innovation and deployment consistent with our AI Principles.\n"
  },
  {
    "Link": "https://s3.us-east-1.amazonaws.com/files.cnas.org/documents/CNAS-Report-Tech-Secure-Chips-Jan-24-finalb.pdf",
    "Category": [
      "Security",
      "AI Governance"
    ],
    "Title": "Secure, Governable Chips: Using On-Chip Mechanisms to Manage National Security Risks from AI & Advanced Computing (Aarne et al., 2024)",
    "Type": [
      "Other"
    ]
  },
  {
    "Blog or Video": "https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training",
    "Link": "https://arxiv.org/abs/2401.05566",
    "ML Subfield": [
      "NLP",
      "Robustness and Adversariality"
    ],
    "Category": [
      "Deception",
      "Non-Adversarial Robustness"
    ],
    "Title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training (Hubinger et al., 2024)\n",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/AnthropicAI/status/1745854907968880970",
    "Abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2109.13916",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Theory",
      "Applied ML",
      "Human Model Interaction"
    ],
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "Unsolved Problems in ML safety (Hendrycks et al., 2021)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”). Throughout, we clarify each problem’s motivation and provide concrete research directions.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2204.06974",
    "Category": [
      "Theory"
    ],
    "Title": "Planting Undetectable Backdoors in Machine Learning Models (Goldwasser et al., 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate \"backdoor key\", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. \nFirst, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is \"clean\" or contains a backdoor. \nOur construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an \"adversarially robust\" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2304.00612",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "Eight Things to Know about Large Language Models (Bowman, 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/sleepinyourhat/status/1642614846796734464",
    "Abstract": "The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: \n1\\. LLMs predictably get more capable with increasing investment, even without targeted innovation.\n 2\\. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.\n 3\\. LLMs often appear to learn and use representations of the outside world.\n 4\\. There are no reliable techniques for steering the behavior of LLMs.\n 5\\. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn’t an upper bound on LLM performance. \n7\\. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.\n"
  },
  {
    "Link": "https://www.safe.ai/ai-risk",
    "ML Subfield": [
      "Domain General",
      "Applied ML"
    ],
    "Category": [
      "Overviews and agendas",
      "Why large-scale safety?"
    ],
    "Title": "An Overview of Catastrophic AI Risks (Hendrycks et al., 2023)",
    "Type": [
      "Blog post",
      "Paper"
    ],
    "Twitter": "https://x.com/DanHendrycks/status/1671894767331061763",
    "Abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.\n",
    "Transcripts / Audio / Slides": "https://www.safe.ai/ai-risk"
  },
  {
    "Link": "https://www.apolloresearch.ai/research/scheming-reasoning-evaluations",
    "Category": [
      "Deception",
      "Model evaluations and benchmarks"
    ],
    "Title": "Frontier Models are Capable of In-context Scheming (Meinke et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Abstract": "Frontier models are increasingly trained and deployed as autonomous agents, which signif-\nicantly increases their potential for risks. One particular safety concern is that AI agents\nmight covertly pursue misaligned goals, hiding their true capabilities and objectives – also\nknown as scheming. We study whether models have the capability to scheme in pursuit of\na goal that we provide in-context and instruct the model to strongly follow. We evaluate\nfrontier models on a suite of six agentic evaluations where models are instructed to pursue\ngoals and are placed in environments that incentivize scheming. Our results show that o1,\nClaude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-\ncontext scheming capabilities. They can recognize scheming as a viable strategy and readily\nengage in such behavior. We find multiple different scheming behaviors: models strate-\ngically introduce subtle mistakes into their responses, attempt to disable their oversight\nmechanisms, and even exfiltrate what they believe to be their model weights to external\nservers. Additionally, this deceptive behavior proves persistent. For instance, when o1 has\nengaged in scheming, it maintains its deception in over 85% of follow-up questions and often\nremains deceptive in multi-turn interrogations. Analysis of the models’ chains-of-thought\nreveals that models explicitly reason about these deceptive strategies, providing evidence\nthat the scheming behavior is not accidental. Surprisingly, we also find rare instances where\nmodels engage in scheming when only given a goal, without being strongly nudged to pursue\nit. Furthermore, we observe cases where Claude 3.5 Sonnet strategically underperforms in\nevaluations in pursuit of being helpful, a goal that was acquired during training rather than\nin-context. Together, our findings demonstrate that frontier models now possess capabili-\nties for basic in-context scheming, making the potential of AI agents to engage in scheming\nbehavior a concrete rather than theoretical concern.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2307.15217",
    "ML Subfield": [
      "Reinforcement Learning",
      "Applied ML",
      "Human Model Interaction"
    ],
    "Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback (Casper et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/StephenLCasper/status/1686036515653361664",
    "Abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems. \n"
  },
  {
    "Blog or Video": "https://www.youtube.com/watch?v=HiYWwjma3xE&t=1687s",
    "Link": "https://arxiv.org/abs/2212.09251",
    "ML Subfield": [
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "Scalable oversight"
    ],
    "Title": "Discovering Language Model Behaviors with Model-Written Evaluations (Perez et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1604883576218341376?lang=en",
    "Abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors. \n"
  },
  {
    "Link": "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
    "Category": [
      "Security"
    ],
    "Title": "Securing AI Model Weights (Nevo et al., 2024)",
    "Type": [
      "Other"
    ],
    "Abstract": "As frontier artificial intelligence (AI) models — that is, models that match or exceed the capabilities of the most advanced models at the time of their development — become more capable, protecting them from theft and misuse will become more important. The authors of this report explore what it would take to protect model _weights_ — the learnable parameters that encode the core intelligence of an AI — from theft by a variety of potential attackers.\nSpecifically, the authors (1) identify 38 meaningfully distinct attack vectors, (2) explore a variety of potential attacker operational capacities, from opportunistic (often financially driven) criminals to highly resourced nation-state operations, (3) estimate the feasibility of each attack vector being executed by different categories of attackers, and (4) define five security levels and recommend preliminary benchmark security systems that roughly achieve the security levels.\nThis report can help security teams in frontier AI organizations update their threat models and inform their security plans, as well as aid policymakers engaging with AI organizations in better understanding how to engage on security-related topics.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2307.02483",
    "ML Subfield": [
      "NLP",
      "Model Evaluation",
      "Robustness and Adversariality",
      "Applied ML"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Title": "Jailbroken: How Does LLM Safety Training Fail? (Wei et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2305.07153",
    "ML Subfield": [
      "Security"
    ],
    "Category": [
      "AI Governance"
    ],
    "Title": "Towards best practices in AGI safety and governance: A survey of expert opinion (Schuett et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/jonasschuett/status/1658025252675366913",
    "Abstract": "  A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs. \n"
  },
  {
    "Link": "https://www.youtube.com/watch?v=yl2nlejBcg0",
    "ML Subfield": [
      "Domain General",
      "Human Model Interaction"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "Researcher Perceptions of Current and Future AI (Vael Gates, 60-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/roger-grosse-studying-llm-generalization-through-influence-functions",
    "Link": "https://arxiv.org/abs/2308.03296",
    "ML Subfield": [
      "NLP",
      "Statistical Modeling"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Title": "Studying Large Language Model Generalization with Influence Functions (Grosse et al., 2023)",
    "Type": [
      "Paper",
      "Video"
    ],
    "Abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs. \n"
  },
  {
    "Link": "https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view",
    "ML Subfield": [
      "Domain General",
      "Human Model Interaction",
      "Reinforcement Learning"
    ],
    "ML Subtopic": [
      "Applied ML: AI-Based Automation",
      "Applied ML: Chatbots",
      "Applied ML: Cognitive Tasks"
    ],
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "Introduction to AI Safety, Ethics, and Society (Hendrycks, 2023)",
    "Type": [
      "Other"
    ]
  },
  {
    "Link": "https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/",
    "ML Subfield": [
      "Domain General",
      "Theory",
      "Reinforcement Learning"
    ],
    "Category": [
      "Deception"
    ],
    "Title": "ML Systems Will Have Weird Failure Modes (Steinhardt, 2022)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2308.14752",
    "ML Subfield": [
      "Applied ML",
      "Human Model Interaction",
      "Robustness and Adversariality"
    ],
    "ML Subtopic": [
      "Applied ML: Chatbots",
      "NLP: LLMs",
      "RL: Games",
      "Applied ML: Cognitive Tasks"
    ],
    "Category": [
      "Deception"
    ],
    "Title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions (Park et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society. \n"
  },
  {
    "Link": "https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/",
    "ML Subfield": [
      "Security",
      "Model Evaluation",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Model evaluations and benchmarks",
      "Security"
    ],
    "Safety Topic": [
      "Benchmarking",
      "Security"
    ],
    "Title": "Purple Llama CyberSecEval: A benchmark for evaluating the cybersecurity risks of large language models (Bhatt et al., 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "This paper presents CYBERSECEVAL, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CYBERSECEVAL provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama2, codeLlama, and OpenAI GPT large language model families, CYBERSECEVAL effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CYBERSECEVAL, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.\n"
  },
  {
    "Link": "https://www.neelnanda.io/mechanistic-interpretability/quickstart",
    "ML Subfield": [
      "Theory",
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Mechanistic Interpretability Quickstart Guide (Nanda, 2023)",
    "Type": [
      "Blog post"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2304.13734",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Interpretability / explainability",
      "Deception"
    ],
    "Safety Topic": [
      "Sycophancy"
    ],
    "Title": "The Internal State of an LLM Knows When It's Lying (Azaria and Mitchell, 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\\\% to 83\\\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios. \n"
  },
  {
    "Link": "https://www.alignment-workshop.com/sf-talks/paul-christiano-how-misalignment-could-lead-to-takeover",
    "ML Subfield": [
      "Domain General",
      "Applied ML",
      "Human Model Interaction"
    ],
    "Category": [
      "Why large-scale safety?"
    ],
    "Title": "How Misalignment Could Lead to Takeover (Paul Christiano, 30-min video)",
    "Type": [
      "Video"
    ]
  },
  {
    "Link": "https://arxiv.org/abs/2105.14111",
    "ML Subfield": [
      "Reinforcement Learning",
      "Theory"
    ],
    "ML Subtopic": [
      "RL: Games"
    ],
    "Category": [
      "Reward misspecification and goal misgeneralization"
    ],
    "Title": "Goal Misgeneralization in Deep Reinforcement Learning (Langosco et al., 2021)\n",
    "Type": [
      "Paper"
    ],
    "Abstract": " We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes. \n"
  },
  {
    "Blog or Video": "https://www.alignment-workshop.com/nola-talks/collin-burns-weak-to-strong-generalization",
    "Link": "https://arxiv.org/abs/2312.09390",
    "ML Subfield": [
      "NLP"
    ],
    "ML Subtopic": [
      "NLP: LLMs"
    ],
    "Category": [
      "Scalable oversight"
    ],
    "Title": "Weak-To-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision (Burns et al., 2023)",
    "Type": [
      "Paper",
      "Blog post",
      "Video"
    ],
    "Twitter": "https://x.com/CollinBurns4/status/1735350133926314431",
    "Abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.\n"
  },
  {
    "Link": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html",
    "Category": [
      "Interpretability / explainability"
    ],
    "Safety Topic": [
      "Mechanistic interpretability"
    ],
    "Title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet (Templeton et al., 2024)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://x.com/AnthropicAI/status/1792935506587656625"
  },
  {
    "Link": "https://arxiv.org/abs/2306.06924",
    "ML Subfield": [
      "Domain General",
      "Human Model Interaction",
      "Applied ML"
    ],
    "Category": [
      "Overviews and agendas"
    ],
    "Title": "TASRA: A Taxonomy and Analysis of Societal-Scale Risks from AI (Critch and Russell, 2023)",
    "Type": [
      "Paper"
    ],
    "Twitter": "https://twitter.com/AndrewCritchCA/status/1668476943208169473",
    "Abstract": "While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an exhaustive taxonomy of such risks. Many exhaustive taxonomies are possible, and some are useful—particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.\n"
  },
  {
    "Link": "https://arxiv.org/abs/2106.09667",
    "ML Subfield": [
      "Robustness and Adversariality",
      "Vision",
      "Applied ML"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Safety Topic": [
      "Trojans"
    ],
    "Title": "Poisoning and Backdooring Contrastive Learning (Carlini and Terzis, 2022)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable. \n"
  },
  {
    "Link": "https://arxiv.org/abs/1906.10842",
    "ML Subfield": [
      "Vision",
      "Robustness and Adversariality",
      "Security"
    ],
    "ML Subtopic": [
      "Applied ML: Autonomous Vehicles"
    ],
    "Category": [
      "Adversarial Robustness"
    ],
    "Safety Topic": [
      "Trojans"
    ],
    "Title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs (Kolouri et al., 2020)",
    "Type": [
      "Paper"
    ],
    "Abstract": "    The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper, we introduce a benchmark technique for detecting backdoor attacks (aka Trojan attacks) on deep convolutional neural networks (CNNs). We introduce the concept of Universal Litmus Patterns (ULPs), which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output (i.e., classifying the network as \\`clean' or \\`corrupted'). This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on thousands of networks with different architectures trained on four benchmark datasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST, CIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can be found here this https URL. \n"
  },
  {
    "Link": "https://arxiv.org/abs/2311.08379",
    "Category": [
      "Deception"
    ],
    "Title": "Scheming AIs: Will AIs fake alignment during training in order to get power? (Carlsmith, 2023)",
    "Type": [
      "Paper"
    ],
    "Abstract": "This report examines whether advanced AIs that perform well in training will be doing so in order to gain power later -- a behavior I call \"scheming\" (also sometimes called \"deceptive alignment\"). I conclude that scheming is a disturbingly plausible outcome of using baseline machine learning methods to train goal-directed AIs sophisticated enough to scheme (my subjective probability on such an outcome, given these conditions, is roughly 25%). In particular: if performing well in training is a good strategy for gaining power (as I think it might well be), then a very wide variety of goals would motivate scheming -- and hence, good training performance. This makes it plausible that training might either land on such a goal naturally and then reinforce it, or actively push a model's motivations towards such a goal as an easy way of improving performance. What's more, because schemers pretend to be aligned on tests designed to reveal their motivations, it may be quite difficult to tell whether this has occurred. However, I also think there are reasons for comfort. In particular: scheming may not actually be such a good strategy for gaining power; various selection pressures in training might work against schemer-like goals (for example, relative to non-schemers, schemers need to engage in extra instrumental reasoning, which might harm their training performance); and we may be able to increase such pressures intentionally. The report discusses these and a wide variety of other considerations in detail, and it suggests an array of empirical research directions for probing the topic further.\n\n"
  }
]
