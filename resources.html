---
layout: page
title: Resources
og-description: Resources for machine learning researchers, for those interested in AI governance and policy, and for a general audience.
nav-menu: true
order: 4
---

<!-- Main -->
<div id="main" class="alt">

<section>
	<div class="inner">
		<h1>Resources</h1>
		<h2 id="opportunities">Opportunities</h2>
		<!--<p>Organizations and researchers in the space, funding and job opportunities, and guides to get involved</p>-->


		<h3 id="research_opportunities">Job Opportunities (Research and Engineering)</h3>
		<ul>
			<li class="expandable">OpenAI</li>
			<ul>
				<li><a href="https://openai.com/safety/safety-systems">Safety Systems Team</a> (<a href="https://openai.com/careers/search?c=safety-systems">roles</a>)</li>
				<li><a href="https://openai.com/safety/preparedness">Preparedness Team</a> (<a href="https://openai.com/careers/search?c=preparedness">roles</a>)</li>
				<li><a href="https://openai.com/blog/introducing-superalignment">Superalignment Team</a> (<a href="https://openai.com/careers/search?c=alignment">roles</a>)</li>
				<li>Security Team</li>
				<li>Policy Research Team</li>
				<li>Trustworthy AI Team</li>
			</ul>
			<li class="expandable">Anthropic (<a href="https://www.anthropic.com/careers#open-roles">roles</a>)</li>
			<ul>
				<li><a href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety">Frontier Red Team</a></li>
				<li><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy Team</a></li>
				<li><a href="https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic">Alignment Stress Testing Team</a></li>
				<li>Interpretability Team</li>
				<li>Dangerous Capability Evaluations Team</li>
				<li>Assurance Team</li>
				<li>Security Team</li>
			</ul>
			<li class="expandable">Google DeepMind (<a href="https://deepmind.google/about/careers/#open-roles">roles</a>)</li>
			<ul>
				<li><a href="https://deepmind.google/about/responsibility-safety/#:~:text=To%20empower%20teams%20to%20pioneer,and%20collaborations%20against%20our%20AI">Responsibility & Safety Team</a></li>
				<li>Scalable Alignment Team</li>
				<li>Frontier Model and Governance Team</li>
				<li>Mechanistic Interpretability Team</li>
			</ul>
			<li><a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute">UK AI Safety Institute (AISI)</a> (<a href="https://www.linkedin.com/jobs/search/?currentJobId=3776968621&f_C=10872416&f_F=othr%2Cit%2Ceng%2Cstra&geoId=92000000&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=R">roles</a>)</li>
			<li><a href="https://far.ai/">FAR AI</a> (<a href="https://far.ai/jobs">roles</a>)</li>
			<li><a href="https://metr.org">Model Evaluations and Threat Research (METR) (<a href="https://jobs.lever.co/alignment.org">roles</a>)</li>
			<li><a href="https://www.apolloresearch.ai/">Apollo Research</a> (<a href="https://www.apolloresearch.ai/careers">roles</a>)</li>
			<!-- <li class="expandable" data-toggle="closed_orgs"><i>Currently Closed Opportunities</i></li>
			<ul>
				<li><a href="https://www.safe.ai/">Center for AI Safety (CAIS)</a> (<a href="https://safe.ai/careers">roles</a>)</li>
				<li><a href="https://www.redwoodresearch.org/">Redwood Research</a> (<a href="https://www.redwoodresearch.org/careers">roles</a>)</li>
				<li><a href="https://palisaderesearch.org/">Palisade Research</a> (<a href="https://palisaderesearch.org/work">roles</a>)
			</ul> -->

		</ul>

		<h3 id="funding">Funding Opportunities</h3>

		<ul>
			<li class="expandable expanded">Open Philanthropy</li>
			<ul>
				<li><a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/"><b>Request for proposals: benchmarking LLM agents on consequential real-world tasks</a></b></li>
				<li><a href="https://www.openphilanthropy.org/rfp-llm-impacts/">Request for proposals: studying and forecasting the real-world impacts of systems built from LLMs</a></li>
				<li><a href="https://www.openphilanthropy.org/career-development-and-transition-funding/">Career development and transition funding</a></li>
				<li><a href="https://www.openphilanthropy.org/open-philanthropy-course-development-grants/">Course development grants</a></li>
				<li><a href="https://www.openphilanthropy.org/rfp-for-projects-to-grow-our-capacity-for-reducing-global-catastrophic-risks/"> Request for proposals for projects to grow our capacity for reducing global catastrophic risks</a></li>
				<!-- <li><a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">How to Apply for Funding</a></li> -->
			</ul>			
			<li><a href="https://www.cooperativeai.com/grants/cooperative-ai">Cooperative AI Research Grants</a></li>
			<li><a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a></li>


			<li class="expandable" data-toggle="closed_funding"><i>Currently Closed Funding Opportunities</i></li>
			<ul>
				<li>OpenAI: <a href="https://openai.smapply.org/prog/agentic-ai-research-grants/">Research into Agentic AI Systems</a> and <a href="https://openai.com/blog/superalignment-fast-grants">Superalignment Fast Grants</a></li>
				<li>NSF: <a href="https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems">Safe Learning-Enabled Systems</a> (deadline: 1/16/24)</li>
				<li>Survival and Flourishing Fund (SFF): <a href="https://survivalandflourishing.fund/">Grant Rounds</a> and <a href="https://survivalandflourishing.fund/speculation-grants">Speculation Grants</a></li>
				<ul>
					<li> Note: SFF gives <a href="https://survivalandflourishing.fund/faq#does-sff-have-an-indirect--overhead-rate-limit-for-grants-to-universities">grants to universities</a>. Alternatively, SFF requires that you have a 501c3 charity (i.e. your nonprofit has 501c3 status or you have a fiscal sponsor that has 501c3 status).</li>
				</ul>
				<li><a href="https://futureoflife.org/our-work/grantmaking-work/">Future of Life Institute</a>: <a href="https://futureoflife.org/grant-program/phd-fellowships/">PhD Fellowships</a> and <a href="https://futureoflife.org/grant-program/postdoctoral-fellowships/">Postdoctoral Fellowships</a></li>
			</ul>
		</ul>


		<h3 id="compute">Compute Opportunities</h3>
		<ul>
			<li><a href="https://www.safe.ai/compute-cluster">Center for AI Safety Compute Cluster</a></li>
			<li><a href="https://txt.cohere.com/c4ai-research-grants/">Cohere for AI, subsidized access to APIs</a></li>
		</ul>

		<h3 id="workshops">Workshops</h3>
		<ul>
			<li class="expandable expanded">Upcoming Workshops</li>
			<ul>
				<li><a href="https://airtable.com/appK578d2GvKbkbDD/pagkxO35Dx2fPrTlu/form">Future events interest form</a> for the Alignment Workshop Series (previous: <a href="https://www.alignment-workshop.com/nola-2023">Dec 2023</a>, <a href="https://www.alignment-workshop.com/sf-2023">Feb 2023</a>)</li>
				<li class="expandable" data-toggle="ICLR">ICLR 2024 Workshops</li>
				<ul>
					<li><a href="https://set-llm.github.io/">Secure and Trustworthy Large Language Models</a></li>
					<li><a href="https://agiworkshop.github.io">How Far Are We From AGI?</a></li>
					<li><a href="https://iclr-r2fm.github.io/">Reliable and Responsible Foundation Models</a></li>
					<li><a href="https://sites.google.com/view/me-fomo2024">ME-FoMo: Mathematical and Empirical Understanding of Foundmation Models</a></li>
					<!--https://pml-workshop.github.io/iclr24/-->
				</ul>
			</ul>
			<li class="expandable" data-toggle="past_workshops">Past Workshops</li>
			<ul>
				<li><a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop (Dec 2023)</a>, <i>recordings available</i></li>
				<li><a href="https://www.alignment-workshop.com/sf-2023">San Francisco Alignment Workshop 2023 (Feb 2023)</a>, <i>recordings available</i></li>
				<li><a href="https://sites.google.com/mila.quebec/scaling-laws-workshop/">Neural Scaling & Alignment: Towards Maximally Beneficial AGI Workshop Series (2021-2023)</a></li>
				<li><a href="https://sites.google.com/mila.quebec/hlai-2023-boston/home">Human-Level AI: Possibilities, Challenges, and Societal Implications (June 2023)</a></li>
				<li><a href="https://futuretech.mit.edu/workshop-on-ai-scaling-and-its-implications#:~:text=The%20FutureTech%20workshop%20on%20AI,a%20range%20of%20key%20tasks%3F">Workshop on AI Scaling and its Implications (Oct 2023)</a></li>
			</ul>
		</ul>

		<h3 id="visitor-programs">AI Safety Visiting Programs / Fellowships</h3>
		<ul> 
			<li class="expandable" data-toggle="mats_description">[graduate students] <a href="https://www.matsprogram.org/">ML Alignment & Theory Scholars (MATS)</a>, <a href="https://airtable.com/appPxJ0QMqR7TElYU/pagRPwHQtcN8L0vIE/form">application</a></li>
			<ul>
				<li>The ML Alignment & Theory Scholars (MATS) Program is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment and safety. We also connect them with the Berkeley alignment research community. Our Summer Program will run Jun 17-Aug 23, 2024 and our Winter Program will run from early Jan, 2025.</li>
			</ul>
			<li class="expandable" data-toggle="closed_programs">Currently Closed Programs</li>
			<ul>
				<li class="expandable" data-toggle="visitor_program_description">[researchers] <a href="https://www.constellation.org/programs/researcher-program">Visiting Researcher Program</a> at Constellation</li>
				<ul>
					<li>The Constellation Visiting researcher program provides an opportunity for around 20 visitors motivated by reducing catastrophic risks from AI to connect with leading AI safety researchers, exchange ideas, and find collaborators while continuing their research from our offices in Berkeley, CA.</a></li>
				</ul>
				<li class="expandable" data-toggle="astra_description">[graduate students + professionals] <a href="https://www.constellation.org/programs/astra-fellowship">Astra Fellowship</a> at Constellation</li>
				<ul>
					<li>The Astra Fellowship pairs fellows with experienced advisors to collaborate on a two or three month AI safety research project. Fellows will be part of a cohort of talented researchers working out of the Constellation offices in Berkeley, CA, allowing them to connect and exchange ideas with leading AI safety researchers.</li>
				</ul>
			</ul>
		</ul>

		<h3 id="jobs">Job Board</h3>

		<div class="iframe-container">
			<iframe id="job_board" onload="iframeLoaded('iframe_loading_spinner_job_board')"  src="https://airtable.com/embed/appQwyPiYo4egdPR5/shr6n4WqjnmmoTpMf?backgroundColor=grayLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
			<div id="iframe_loading_spinner_job_board" class="iframe-loading">
				{% include loading_spinner.html %}
			</div>
			<p style="text-align: right;"><i>Filtered from the <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Bcompany_data%5D%5B0%5D=Highlighted%20organisations&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Data&refinementList%5Btags_skill%5D%5B1%5D=Engineering&refinementList%5Btags_skill%5D%5B2%5D=Research&refinementList%5Btags_skill%5D%5B3%5D=Software%20engineering">80,000 Hours Job Board</a></i></p>
		</div>


		<h3 id="alternative_opportunities">Alternative Technical Opportunities</h3>
		<ul>
			<li class="expandable" data-toggle="theoretical_research">Theoretical Research</li>
			<ul>
				<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
				<li><a href="https://intelligence.org/careers/">Machine Intelligence Research Institute (MIRI)</a> </li>
			</ul>
			<li class="expandable" data-toggle="information_security">Information Security</li>
			<ul>
				<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Information%20security">Information Security roles</a></li>
				<li><a href="https://80000hours.org/career-reviews/information-security/">Overview from a security engineer at Google</a></li>
				<li><a href="https://www.linkedin.com/in/jason-clinton-475671159/">Jason Clinton</a>'s recommended <a href="https://www.google.com/books/edition/Building_Secure_and_Reliable_Systems/Kn7UxwEACAAJ?hl=en&kptab=getbook">upskilling book</a></li> <!--<a href="https://forum.effectivealtruism.org/posts/zxrBi4tzKwq2eNYKm/ea-infosec-skill-up-in-or-make-a-transition-to-infosec-via">EA Infosec: skill up in or make a transition to infosec via this book club</a>-->
			</ul>
			<li><a href="https://jobs.80000hours.org/?query=Forecasting&refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy">Forecasting</a> (see especially <a href="https://epochai.org/careers">Epoch</a>)</li>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering">Software Engineering</a></li>
			<li class="expandable" data-toggle="technical_goverance">Technical Governance (from <a href="https://web.archive.org/web/20231012161714/https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/#4-ai-governance-and-policy-aigp"> Section 4.3</a>)</li>
			<ul>
				<li>Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>

			<li class="expandable" data-toggle="governance"> Compare with general AI Governance and Policy</li>
			<ul>
				<p>AI governance is focused on developing global norms, policies, and institutions to increase the chances that advanced AI is beneficial for humanity.</p>
				<li><a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a> by BlueDot Impact</li>
				<li><a href="https://emergingtechpolicy.org/areas/ai-policy/">AI Policy Resources</a> by Emerging Technology Policy Careers</li>
				<!-- <li>See<a href="#alternative_opportunities"> Alternative Technical Opportunities,Technical Governance</a></li> -->
				<li>Several organizations working in the space: Frontier AI Task Force, <a href="https://www.governance.ai/">Center for the Governance of AI (GovAI)</a>, OpenAI's Governance Team, <a href="https://www.longtermresilience.org/">Center for Long-Term Resilience (CLTR)</a>, <a href="https://www.rand.org/topics/science-technology-and-innovation-policy.html">RAND's Technology and Security Policy work</a>, <a href="https://www.horizonpublicservice.org/">Horizon Institute for Public Service</a>, <a href="https://cset.georgetown.edu/">Center for Security and Emerging Technology (CSET)</a></li>
				<!-- Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>-->
			</ul>
		</ul>
		</ul>
<!--
		<h3>Example of expandable lists</h3>
		<ul>
			<li>The list item after this one has the 'expandable' class, which makes its list icon become a caret, and makes it clickable to show/hide its sublist.</li>
			<li class="expandable" data-toggle="theoretical_research">Theoretical research
				<ul id="theoretical_research">
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
				</ul>
		</ul>

		<h3>Example of expandable lists already expanded</h3>
		<ul>
			<li>The list item after this one has the 'expandable' class, and also the 'expanded' class, so it's already open on page load.</li>
			<li class="expandable expanded" data-toggle="theoretical_research">Theoretical research
				<ul id="theoretical_research">
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
				</ul>
		</ul>
-->

		<h3 id="academia">Researchers working on AI safety</h3>
		<ul>
			<li><a href="https://futureoflife.org/about-us/our-people/ai-existential-safety-community/">AI Existential Safety Community</a> from Future of Life Institute</li>
			<li>See speakers from the Alignment Workshop series (<a href="https://www.alignment-workshop.com/sf-2023">SF 2023</a>, <a href="https://www.alignment-workshop.com/nola-2023">NOLA 2023</a>)</li>
		</ul>

		<h3 id="china">Interested in working in China?</h3>
		<ul>
			<li>Contact <a href="https://concordia-ai.com/">Concordia AI 安远AI</a></li>
			<li>Newsletters: <a href="https://aisafetychina.substack.com/">AI Safety in China</a>, <a href="https://chinai.substack.com/about">ChinAI Newsletter</a></li>
		</ul>
	</div>
</section>

<section class="bg-gray">
	<div class="inner">
		<h2 id="ai_safety">AI Safety Content</h2>
    
    <h3 id="papers">Selected Papers</h3>

		<div class="iframe-container">
			<iframe id="key_papers" onload="iframeLoaded('iframe_loading_spinner_key_papers')" src="https://airtable.com/embed/appOQF4xHFCwscK39/shrwwWZNAfTkrHP4h?backgroundColor=blueLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="625" style="background: transparent; border: 1px solid #ccc;"></iframe>
			<div id="iframe_loading_spinner_key_papers" class="iframe-loading">
				{% include loading_spinner.html %}
			</div>
		</div>

    <a href="https://airtable.com/appOQF4xHFCwscK39/shrQzex73hN01B1CR" class="button button-white button-small button-right">Suggest A Resource</a>

	<h3 style="clear:both"> Expanded List of Papers</h3>
	
	<a href="papers" class="button button-white fit">AI Safety Papers</a>

	<br>
    <h3 id="content">Blog posts</h3>
    <ul>
      <li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
      <li><a href="https://wp.nyu.edu/arg/why-ai-safety/">Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022)</li>
      <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">More is Different for AI</a> by Jacob Steinhardt (2022)</li>
      <!--<li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0">Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)</li>-->
      <li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper (ongoing)</li>
      <!-- <li class="expandable" data-toggle="general"><i>For a general audience</i></li>
			<ul>
				<li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper</li>
				<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)</li>
				<li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)</li>
				<li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
				<li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
				<li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)</li>
				<li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcasts and articles)</li>
				<li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
			</ul> -->
		</ul>
	</ul>

    <h3 id="newsletters">Newsletters</h3>
    <ul>
      <li><a href="https://us13.campaign-archive.com/home/?u=67bd06787e84d73db24fb0aa5&id=6c9d98ff2c">Import AI</a> by Jack Clark, <i>for keeping up to date on AI progress</i></li>
      <li><a href="https://newsletter.mlsafety.org">ML Safety Newsletter</a> by the Center for AI Safety, <i>for recent AI safety papers</i></li>
      <li><a href="https://rohinshah.com/alignment-newsletter/">Alignment Newsletter</a> by Rohin Shah, <i>for review of AI safety papers, on hiatus</i></li>
    </ul>

	<h3 id="guides">Upskilling</h3>
	<ul>
		<li class="expandable" data-toggle="guides">Guides</li>
		<ul>
			<li> <b>Research</b>: <a href="https://aisafetyfundamentals.com/blog/alignment-careers-guide">Alignment Careers Guide</a></li>
			<li class="expandable expanded"><b>Engineering</b></li>
			<ul>
				<li><a href="https://arena-roadmap.streamlit.app/">ARENA research engineering upskilling curriculum</a>
					<ul>
						<li><a href="https://arena3-chapter0-fundamentals.streamlit.app/">Chapter 0: Fundamentals</a></li>
						<li><a href="https://arena3-chapter1-transformer-interp.streamlit.app/">Chapter 1: Transformer Interpretability</a></li>
						<li><a href="https://arena3-chapter2-rl.streamlit.app/">Chapter 2: Reinforcement Learning</a></li>
						<li><a href="https://arena-ch3-training-at-scale.streamlit.app/">Chapter 3: Training at Scale</a></li>
					</ul>
				</li>
				<li><a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Leveling Up in AI Safety Research Engineering</a></li>
			</ul>
		</ul>
		<li class="expandable" data-toggle="courses">Courses</li>
		<ul>
			<li><a href="https://www.aisafetyfundamentals.com/ai-alignment-curriculum">AI Safety Fundamentals Curriculum</a> by BlueDot Impact </li>
			<li><a href="https://course.mlsafety.org/">Introduction to ML Safety</a> by the Center for AI Safety</li>
		</ul>
	</ul>
	</div>
</section>

</div>

<script>
	function iframeLoaded(spinnerID) {
		document.getElementById(spinnerID).style.display = 'none';
	}
</script>


{% if false %}
<!-- All this commented-out section is inside an if-statement so it doesn't get sent to users.-->


<!--
{% if false %}
<section id="two" class="bg-gray">
	<div class="inner">

<div id="book_a_call"><h2>Book a call</h2>
<p>If you're interested in working in AI alignment and advanced AI safety, please book a call with <a href="https://vaelgates.com">Vael Gates</a>, who leads this project and conducted the <a href=interviews>interviews</a> as part of their postdoctoral work with Stanford University.</p>

<form id="book_call_form" method="post" action="#">
	<div class="row uniform">
		<div class="6u 12u$(xsmall)">
			<input type="text" name="name" id="name" value="" placeholder="Name" />
		</div>
		<div class="6u$ 12u$(xsmall)">
			<input type="email" name="email" id="email" value="" placeholder="Email" />
		</div>
		<div class="12u$">
			<div class="select-wrapper">
				<select name="interest" id="interest">
					<option value=""> Interested in talking about... </option>
					<option value="AI Alignment Research or Engineering">AI Alignment Research or Engineering</option>
					<option value="AI Alignment Governance">AI Alignment Governance</option>
					<option value="Other">Other (please specify below)</option>
				</select>
			</div>
		</div>
		<div class="12u$">
			<textarea name="message" id="message" placeholder="Enter your message" rows="6"></textarea>
		</div>
		<div class="12u$">
			<ul class="actions">
				<li>
          <button class="button" id="send_form_button">
            <div class="button-progress-bar"></div>
            <div class="button-text">Send Message</div>
          </button>
        </li>
			</ul>
		</div>
	</div>
</form>
</div>

</div>
</section>

{% endif %}

</div>

{% if false %}
<script src="{{ "assets/js/book_call.js" | absolute_url }}" type="module"></script>
<script>
  window.contactEmail = "{{site.email}}"
</script>
{% endif %}
-->

{% endif %}
