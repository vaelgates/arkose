- node:
  name: Introduction
  url: /arguments/introduction
  askQuestion: false
  effect: calculated
- node:
  name: Generally capable AI systems
  url: /arguments/when-agi
  text: Timeline for development of generally capable AI systems
  effect: calculated
  question: When do you think these generally capable systems will exist?
  questionSubtext: Use the checkboxes and blue arrows to explore counterarguments or agreement (mirroring the original interviewer’s perspective).
  nodes:
    - node:
      name: Within 50 years
      url: /arguments/within-50-years
      effect: agree
      askQuestion: false
      overridesSiblings: true
    - node:
      name: More than 50 years
      url: /arguments/more-than-50-years
      effect: agree
      question: Would you like to hear these arguments?
      overridesSiblings: true
      nodes:
        - node:
          name: asdf
          url: /arguments/the-alignment-problem
          askQuestion: false
          text: No, let’s move on - I want to learn about potential risks
          parentListingType: button
          listInTree: false
        - node:
          name: Why these systems might come soon
          url: /arguments/agisooner
          propagateAgreement: false
          text: Yes, I would like to hear these arguments for why AGI might come soon
          parentListingType: button
    - node:
      name: Never
      url: /arguments/never
      question: Would you agree or disagree with this?
      overridesSiblings: true
      nodes:
        - node:
          name: The Alignment Problem
          text: Agree, I agree there might be such generally capable systems at some time in the future (move on to potential risks from AI)
          effect: undecidedOverride # this sets its parent node to "undecided". This is because the question is phrased in such a way that it basically indicates "I now changed my mind on the parent".
          answerLinkUrl: /arguments/the-alignment-problem
          agreeTargetUrl: /arguments/never
          listInTree: false # same justification as above
        - node:
          name: Biology is special
          url: /arguments/biology-special
          text: There is something special about biology which we will never be able to put into machines
        - node:
          name: Intelligent machines ‘just seem weird’
          url: /arguments/seems-weird
          text: I don’t know, but truly intelligent machines—that seems really weird
        - node:
          name: No true creativity
          url: /arguments/creativity
          text: AI will never be able to have true creativity
        - node:
          name: Would need to understand the brain first
          url: /arguments/understand-brain-first
          text: We would need to understand the brain first, and this is a big obstacle
        - node:
          name: We wouldn’t want that
          url: /arguments/we-dont-want-that
          text: I don’t see a reason for us to even want that
        - node:
          name: Can’t see it based on current progress
          url: /arguments/cant-see-it-based-on-current-progress
          text: I can’t see it based on current progress
        - node:
          name: Need AI paradigm shift
          url: /arguments/aiparadigm-shift-required
          text: An AI paradigm shift will be required
        - node:
          name: People would stop this
          url: /arguments/surely-this-would-be-bad-and-people-would-stop-it
          text: Surely this would be bad and people would stop it
        - node:
          name: We need embodiment
          url: /arguments/embodiment-is-necessary
          text: We need embodiment for general intelligence
        - node:
          name: AI cannot be conscious
          url: /arguments/consciousness
          text: AI cannot be conscious in the way a human is
- node:
  name: The Alignment Problem
  url: /arguments/the-alignment-problem
  effect: calculated
  nodes:
    - node:
      name: Agree
      linkName: Instrumental Incentives
      text: Agree, aligning AI would be difficult
      effect: agree
      nodeLinkUrl: /arguments/the-alignment-problem
      agreeTargetUrl: /arguments/the-alignment-problem
      answerLinkUrl: /arguments/instrumental-incentives
    - node:
      name: Disagree
      effect: disagree
      nodeLinkUrl: /arguments/the-alignment-problem
      isCheckboxOption: false
      delegateCheckboxes: true
      nodes:
        - node:
          name: Let’s test before deploying
          url: /arguments/test-before-deploying
          text: No—we would test it before deploying
        - node:
          name: Just be careful with reward function
          url: /arguments/careful-with-that-reward-function
          text: No—we would just be careful with our reward function
        - node:
          name: Alignment is easy
          url: /arguments/alignment-is-easy
          text: No—alignment is easy
        - node:
          name: Alignment will progress automatically
          url: /arguments/alignment-advances-equally
          text: No— as we build more capable systems, surely our understanding of how to align them will advance equally well
        - node:
          name: Need to know what type of AGI
          url: /arguments/need-to-know-what-type
          text: No—we need to know what type of AGI we will develop before we can talk about safety
        - node:
          name: Misuse is a bigger problem
          url: /arguments/misuse-is-a-bigger-problem
          text: No—misuse is a bigger problem than accidental misalignment
        - node:
          name: Other global risks are more dangerous
          url: /arguments/not-as-dangerous-as-other-global-risks
          text: No—this is not as dangerous as other global risks
        - node:
          name: Humans have alignment problems too
          url: /arguments/humans-have-alignment-problems-too
          text: No—humans have alignment problems too
- node:
  name: Instrumental Incentives
  url: /arguments/instrumental-incentives
  effect: calculated
  nodes:
    - node:
      name: Agree
      linkName: Threat Models
      text: Agree, the prospect of an AGI seizing power and resisting shutdown is a serious problem
      effect: agree
      nodeLinkUrl: /arguments/instrumental-incentives
      agreeTargetUrl: /arguments/instrumental-incentives
      answerLinkUrl: /arguments/threat-models
    - node:
      name: Disagree
      effect: disagree
      nodeLinkUrl: /arguments/instrumental-incentives
      isCheckboxOption: false
      delegateCheckboxes: true
      nodes:
        - node:
          name: Consciousness required
          url: /arguments/no-self-preservation-without-consciousness
          text: Consciousness won’t happen (and is required for self-preservation)
        - node:
          name: We could stop that physically
          url: /arguments/we-could-stop-that-physically
        - node:
          name: Current systems don’t do that
          url: /arguments/current-systems-dont-do-that
        - node:
          name: Wouldn’t design it that way
          url: /arguments/wouldnt-design-that-way
          text: We wouldn’t design it that way
        - node:
          name: Human Oversight
          url: /arguments/human-oversight
          text: This won’t be a problem, we will have human oversight
- node:
  name: Threat Models
  url: /arguments/threat-models
  text: Threat Models—how might AGI be dangerous?
  effect: calculated
- node:
  name: The importance of more safety work
  url: /arguments/the-importance-of-more-safety-work
  effect: calculated
  nodes:
    - node:
      name: Agree
      text: Agree, safety work is important
      effect: agree
      nodeLinkUrl: /arguments/the-importance-of-more-safety-work
      agreeTargetUrl: /arguments/the-importance-of-more-safety-work
      answerLinkUrl: /arguments/conclusion
    - node:
      name: Disagree
      effect: disagree
      nodeLinkUrl: /arguments/the-importance-of-more-safety-work
      isCheckboxOption: false
      delegateCheckboxes: true
      nodes:
        - node:
          name: Regulators will take care of this
          url: /arguments/regulators-will-take-care-of-this
        - node:
          name: Nothing we can do at the moment
          url: /arguments/not-useful-currently
          text: Work on AGI safety is not useful because there is nothing we can do at the moment
        - node:
          name: This is not urgent
          url: /arguments/not-urgent-currently
          text: I’m not convinced this is urgent now
        - node:
          name: Worry about safety later
          url: /arguments/lets-build-worry-later
          text: Let’s build AI, worry about safety later
        - node:
          name: Current safety work isn’t useful
          url: /arguments/not-currently-tractable
          text: Yeah there are people working on this but I don’t understand why their work would be useful
- node:
  name: Conclusion
  url: /arguments/conclusion
  effect: calculated
