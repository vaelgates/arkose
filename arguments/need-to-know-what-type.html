---
layout: argument
title: "Need to know what type of AGI"
breadcrumbs: The Alignment Problem:the-alignment-problem,Need to know what type of AGI:need-to-know-what-type
---

<blockquote>
How did the AGI come about… I think it's pretty difficult. My intuition, that could be wrong, is that it's pretty difficult to reason about how to make it safe if we don't know what the AGI... If the AGI is a deep neural network or is a set of logic rules that are somehow just chained together into a more and more complicated loop that results in AGI or something. 
</blockquote>

<ul><li>If AGI comes through entirely different paradigms than current AI systems, that casts doubt on how might mean we cannot have much insight we can have here and now into into what makes it un safe.</li>
<li>However, it might be that AGI comes soon, and it might come through existing paradigms.</li>
<ul><li>Consider how much progress has been made simply by scaling up existing approaches, and how the benefits of scaling just seem to continue even into the >500 billion neuron range.</li>
<li>Also, consider the sudden emergence of new capabilities in language models as scale increases.</li>
</ul><li>Regardless of that, it seems worth pursuing keeping an eye on alignment research, - even if AGI does not come through existing paradigms. While the specifics of the problem might be viewed in a different light in the future, we can already foresee that it may can be a problem which will take considerable time and resources to solve. Since we may or may not have time and resources available between it becoming clear what sort of AGI we will develop and that AGI reaching a dangerous level of capability, we should start making those investments now. and thus should start paying attention now.</li>
<li>It is difficult to reason about a hypothetical future kind of technology that has never existed before. For example, people in the 1950 could not have foreseen the deep learning revolution.</li>
<li>However, even a small risk of a catastrophic new technology would mean it’s definitely worth paying attention to now , rather than waiting - even if we do can not know for certain if our present-day discussions on safety will significantly actually influence how safe future AI will be.</li>
<li>That’s why research on AGI alignment focus s es, at least some of the time, on high-level theoretical or philosophical very general arguments about goal-seeking behavior and agents. The hope is that this research will generalize to AI paradigms that are very different from our current one.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
