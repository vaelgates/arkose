---
layout: argument
title: "We would need to know what type of AGI before talking about safety"
breadcrumbs: The Alignment Problem:the-alignment-problem,We would need to know what type of AGI before talking about safety:need-to-know-what-type
---

<blockquote>
How did the AGI come about, I think it's pretty difficult. My intuition, that could be wrong, is that it's pretty difficult to reason about how to make it safe if we don't know what the AGI... If the AGI is a deep neural network or is a set of logic rules that are somehow just chained together into a more and more complicated loop that results in AGI or something. 

</blockquote>

<ul><li>If AGI comes through entirely different paradigms than current AI systems, that might mean we cannot have much insight into what makes it safe.</li>
<li>However, it might be that AGI comes soon, and it might come through existing paradigms.</li>
<ul><li>Consider how much progress has been made simply by scaling up existing approaches, and how the benefits of scaling just seem to continue even into the >500 billion neuron range.</li>
<li>Also, consider the sudden emergence of new capabilities in language models as scale increases.</li>
</ul><li>Regardless of that, it seems worth keeping an eye on alignment - even if AGI does not come through existing paradigms. While the specifics of the problem might be viewed in a different light in the future, we can already foresee that it can be a problem and thus should start paying attention now.</li>
<li>It is difficult to reason about a hypothetical future kind of technology that has never existed before. For example, people in the 1950 could not have foreseen the deep learning revolution.</li>
<li>However, even a small risk of a catastrophic new technology would mean it’s definitely worth paying attention to now, rather than waiting - even if we cannot know for certain if our present-day discussions on safety will actually influence how safe future AI will be.</li>
<li>That’s why research on AGI alignment focusses, at least some of the time, on very general arguments about goal-seeking behaviour and agents. The hope is that this research will generalize to AI paradigms that are very different from our current one.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
