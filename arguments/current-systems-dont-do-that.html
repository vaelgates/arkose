---
layout: argument
title: "Current systems don’t do that"
breadcrumbs: Instrumental Incentives:instrumental-incentives,Current systems don’t do that:current-systems-dont-do-that
---

<blockquote>
I feel like I don't see why this would ever happen or how this would ever happen in a model, simply because it's not what it was intended to do, and I feel like going out of that scope of initial tasks rarely happens. 
</blockquote>

<ul><li>While current AI systems do not display behavior indicating instrumental incentives have emerged, it is possible that future systems may.</li>
<li>In fact, we would expect certain instrumental incentives to emerge from the kinds of systems people are trying to build. All that is necessary to get self-preservation is:</p>
<ol><li>The system has knowledge of itself and about how it could influence the world.</li>
<li>The system is capable of advanced planning and is able to act in ways that maximize its chances of achieving its goals.</li>
</ol></li>
<li>Emergent capabilities are already an observed reality. Recent systems like Google PaLM have demonstrated sudden capability gain upon increases in model size. It is difficult to say which additional capabilities could arise in the future as a result of model-size increase, or other factors.</li>
<li>In a NeurIPS poster from 2021 (<a href='https://neurips.cc/virtual/2021/poster/28400' target='_blank'>video</a>, <a href='https://arxiv.org/pdf/1912.01683.pdf' target='_blank'>PDF paper</a>), Turner et al develop a formal theory of statistical tendencies of optimal processes. From the abstract: “To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes (MDPs), we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.”</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
