---
layout: argument
title: "Current systems don’t do that"
breadcrumbs: Instrumental Incentives:instrumental-incentives,Current systems don’t do that:current-systems-dont-do-that
---

<blockquote>

<div>I feel like I don't see why this would ever happen or how this would ever happen in a model, simply because it's not what it was intended to do, and I feel like going out of that scope of initial tasks rarely happens. </div>

</blockquote>

<ul><li>Current AI systems won’t act in that way, but future systems might.</li>
<li>In fact, given certain assumptions, we would expect that systems act in accordance with certain instrumental incentives. [textblock:AllThatIsRequiredForSelfPreservation]</li>
<li>Recent systems like Google PaLM have demonstrated sudden capability gain upon increases in model size. It is difficult to say which additional capabilities could arise in the future as a result</li>
<li>TODO: mention theories on emergence of agency (waiting for response from an expert I contacted)</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div>&#10149; <a href='instrumental-incentives#argnav'>Go back</a></div>
<div>&#9993; <a href='#feedback'>Send Feedback</a></div>
