---
layout: argument
title: "The Alignment Problem"
breadcrumbs: The Alignment Problem:the-alignment-problem
---
<ul><li>Current ML systems can fail in surprising and unexpected ways. <a href='https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml' target='_blank'>Many examples have been collected</a></li>
<li>Suppose we have a future “CEO AI” that’s running a company. Their board of directors might give the AI the goal: “Maximize profits without exploiting people, don’t run out of money, and avoid side effects that people would consider objectionable”.</li>
<li>Currently we find it very challenging to translate these human values, preferences and intentions into mathematical formulations that can be optimized by systems. This might continue to be a problem in the future.</li>
<li>Mistakes in goal formulation could be dangerous and have far-reaching consequences. For example, the AI might optimize for what looks good, instead of what actually fulfills human values. Or it might actively cause harm to humans, if that helps it achieve its goals.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div><em>Would you agree that highly intelligent systems might fail to optimize exactly what their designers intended them to, and this is dangerous?</em></div>
<div></div>
<div>&#10149; <a href='test-before-deploying.html'>It’s no problem, we would test it before deploying</a></div>
<div>&#10149; <a href='careful-with-that-reward-function.html'>It’s no problem, we would just be careful with our reward function</a></div>
<div>&#10149; <a href='wouldnt-be-functional.html'>It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen</a></div>
<div>&#10149; <a href='would-inevitably-be-solved.html'>This problem would be inevitably solved in order to even develop AGI</a></div>
<div>&#10149; <a href='alignment-advances-equally.html'>But as we build more capable systems, surely our understanding of how to align them will advance equally well</a></div>
<div>&#10149; <a href='need-to-know-what-type.html'>Need to know what type of AGI before we can talk about safety</a></div>
<div>&#10149; <a href='misuse-is-a-bigger-problem.html'>Misuse is a bigger problem</a></div>
<div>&#10149; <a href='this-is-not-as-dangerous-as-other-global-risks.html'>This is not as dangerous as other global risks</a></div>
<div>&#10149; <a href='humans-have-alignment-problems-too.html'>Humans have alignment problems too</a></div>
<div>&#10149; <a href='instrumental-incentives.html'>OK, I agree there might be a problem here. Let’s move on.</a></div>
<div>&#9993; <a href='#feedback'>Send Feedback</a></div>
