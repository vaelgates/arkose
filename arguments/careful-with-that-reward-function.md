---
layout: argument
title: "It’s no problem, we would just be careful with our reward function"
breadcrumbs: The Alignment Problem:the--alignment--problem,It’s no problem we would just be careful with our reward function:careful-with-that-reward-function
---
<blockquote>Right. That's hard. So with the specific example that we're talking about, "maximizing profit", "minimizing human exploitation", I suppose then it would depend on what the objective is. Is the system designed to maximize profit? Because current systems seem to be maximizing profit at the cost of human exploitation. So I think that's how systems are going to be designed because that's what we think as a society, what should be done or what's acceptable, so I don't think it's that the problem is that we are designing systems that is going to do exactly that, or not going to do exactly what we tell them to do, but I think we just have our goals or objectives wrong or incorrect. (nlb02_Sam, Pos. 19)</blockquote>
<ul><li>If we do have systems that use one reward function, it seems unlikely that it will capture everything we care about</li>
<ul><li>Consider the possibility that we’ll end up with a  system optimizing for profit instead of optimizing for human fulfilment</li>
</ul><li>As a society, we are not generally aiming towards developing tech that captures everything we care about. We don’t do that with our energy supply or the design of social media apps, and we probably won’t do that in AGI development either.</li>
<li>Even if the other problems were solved: What if someone makes a mistake in programming that reward function?</li>
<li>What if the system is allowed to self-modify and changes its own reward function?</li>
