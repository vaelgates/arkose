---
layout: argument
title: "Instrumental Incentives"
breadcrumbs: Instrumental Incentives:instrumental-incentives
---
<ul><li>Let's go back to the hypothetical of a CEO AI running a company. Recall that it is difficult to put the goals we would want it to have into a mathematical formulation. There is another problem that arises in this situation, which this chapter will address.</li>
<li>Suppose we do manage to encode our goals in some form or another. We would tell the CEO AI to maximize profits while adhering to certain ethical standards and human values.</li>
<li>In order to be effective, the AI would need to have some model of its place in the world and the way its actions influence the environment. This would include the fact that it is running on a computer somewhere, and that it can be switched off.</li>
<li>Getting switched off would prevent it from achieving its goals. Therefore, it might try to find ways to prevent shutdown (for example, by making a backup copy of itself).</li>
<li>Such behavior does not need to be programmed in. It would emerge under certain conditions: If the system is capable enough, models the world reasonably well and optimizes for almost any goal, it will want to prevent attempts to be shut down.</li>
<li>This does not require the AI system to care about “itself” in the sense that humans care about their own survival. All it requires is the understanding that its goals are unachievable if the system is turned off.</li>
<li>Furthermore: In order to solve problems faster and achieve its goals with higher likelihood, the AI system will try to gather additional resources and gain more power.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
