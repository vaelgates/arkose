---
layout: argument
title: "Would need to understand the brain first"
breadcrumbs: Generally capable AI systems:when-agi,Never:never,Would need to understand the brain first:understand-brain-first
---

<blockquote>
But at the same time, having human-like capabilities is very, very hard because... Primarily because we do not understand the brain as it exists. Now, of course, there has been a different trajectory where people have sort of said, "Okay, we don't care about what the brain does or how it works, we just want to keep on playing with the permutation and then combination of how the layers are arranged, what layers are to be put in, and that is how we come up with a network. And then that just solves the task at hand. So that's one way to do it. And that's perfectly all right. But if you were to mimic the brain as is, we need to have a lot more studies and we need to have a lot more going on the neuroscience front and on the cognitive psychology front. Vision scientists need to come in and really bridge that gap, that is when I think we can achieve somewhere close to artificial general intelligence.  
</blockquote>


<blockquote>
I'm not so sure that we'll have a general AI because this all started with the false analogy that artificial neural networks are analogous to biological neural networks. And the thing is we don't really understand how the brain really works,  in depth. We have some idea that there are different parts of the brain and this and that happens but we don't really know. We cannot replicate it, we cannot create an artificial biological brain. So saying that an artificial neural network simulates the brain, and if you'll just keep increasing the number of neurons, number of layers and throwing a lot of compute power and a lot of data to do something, I don't think we'll have a general AI. We really need to understand how the brain works if we are talking about that. 
</blockquote>

Here are some examples of what we can do with current machine learning techniques (without having gained much understanding of the way the brain works):<br/>
<ul><li>PaLM (2022) <a href='https://ourworldindata.org/grapher/ai-training-computation?time=2017-08-04..2022-07-01' target='_blank'>was the most compute intensive model ever trained at the time of release</a>. This model achieved state-of-the-art few-shot performance across numerous difficult NLP tasks, including natural language inference, common-sense reasoning, in-context reading comprehension, and answering questions. It is also able to explain novel jokes.</li>
</ul><p><figure><img src='{{site.baseurl}}{% link assets/images/arguments/9A36C1759D7EA7B70A424A7E6D4EDA0A.png %}' referrerpolicy='no-referrer'/><figcaption markdown='1'>PaLM’s capabilities expansion as a function of model size. Note that new capabilities appear suddenly as the model grows.
</figcaption></figure></p>
<p><figure><img src='{{site.baseurl}}{% link assets/images/arguments/22C55F4C3CFD67F74FEC6CA19DB7F56E.png %}' referrerpolicy='no-referrer'/><figcaption markdown='1'>Explaining Jokes using PaLM (2-shot). Source: <a href='https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf' target='_blank'>Google</a>.
</figcaption></figure></p>
<ul><li>Importantly, simply scaling the model, without incorporating new insights or understanding, resulted in significant changes in its capabilities. PaLM gained abilities like explaining jokes and answering physics questions without anyone understanding how the human brain does them.</li>
</ul><li>Dall-E 2 shows creativity and the ability to understand concepts.</li>
<li><figure><img src='{{site.baseurl}}{% link assets/images/arguments/3D83536B320986F415E1552745DEBFA6.png %}' referrerpolicy='no-referrer'/><figcaption markdown='1'>Dall-E 2 samples. Source: <a href='https://cdn.openai.com/papers/dall-e-2.pdf' target='_blank'>OpenAI</a> 
</figcaption></figure></li>
<li>A <a href='https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play' target='_blank'>2021 Deepmind paper</a> describes agents with emergent generalized skills on novel tasks. The agents were trained on a variety of tasks in a virtual environment, then performed well in novel games they’d never encountered.</li>

<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
