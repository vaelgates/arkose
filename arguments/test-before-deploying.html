---
layout: argument
title: "It’s no problem, we would test it before deploying"
breadcrumbs: The Alignment Problem:the--alignment--problem,We would test before deploying:test-before-deploying
---
<div><blockquote>0:25:46.4 Interviewee: If we are saying that the whole system is designed perfectly and it's us who are feeding in the wrong goals, then isn't it the human's problem that they then try to use it correctly? And if the system is not designed correctly, then we should have first tested it to make sure that it doesn't go towards the edge cases and start harming us. If humans are the one who design the system, they should know at least when it will work and some cases when it won't work. And we should have created some kind of a test. And also some cases like you cannot go beyond this, if you go beyond this and try to do this, we shut you off. (oju8k_VaelLabel, Pos. 30)</blockquote></div>
<ul><li>AI systems frequently fail when encountering out-of-distribution data</li>
<li>By necessity, an AGI in the real world would often operate far outside its training envelope</li>
<li>An AGI system, in order to have general intelligence, will probably need to be aware about its own inner workings and its place in the world. This would mean it might be able to know that it’s being trained, and it could intentionally display safe behaviour in training, hiding its true intentions until later. This sounds far-fetched, but is a natural consequence of assuming that future systems will have high amounts of intelligence and an accurate model of the world.</li>
<li>There is a long history of dangerous technology being deployed prematurely, before the technology was understood well enough to be safe. Some examples:</li>
<ul><li>Radioactive toothpaste being sold in Germany in the 1920s</li>
<li>Early passenger aircraft (like the de Havilland Comet ) having rectangular windows, before it was understood that rectangular features put too much stress on the airframe - leading to several disasters.</li>
<li>The Therac-25 radiation treatment machine which killed several people due to a programming error.</li>
<li>The dangerous reactor design which contributed to the Chernobyl disaster.<br/></li>
</ul><li>Additionally, if the use of a technology could result in wide-scale negative outcomes, testing procedures need to be exceptionally thorough to prevent such outcomes, since unilateral action from the more careless/clueless or ill-intentioned user could result in disaster (consider also compounding errors, as is true of e.g. <a href='https://youtu.be/_ptmjAbacMk?t=799'>this analysis of the Bhopal disaster</a>. It would not be surprising in today’s world if testing was not sufficiently thorough to protect from the worst-case.</li>
