---
layout: argument
title: "Consciousness won’t happen"
breadcrumbs: Instrumental Incentives:instrumental-incentives,Consciousness won’t happen:no-self-preservation-without-consciousness
---

<blockquote>
A human is conscious and thus worries about their own death. So do animals. But a computer won’t be conscious in the same way. So I think these issues of self-preservation are not going to be present in a machine, at all. 

</blockquote>

<div>Our argument does not require the agent to have a sense of self-preservation (in the sense that they care about themselves). All that is necessary to get self-preservation is</div>
<ol><li>that the system has knowledge of itself and about how it could influence the world</li>
<li>that the system does advanced planning and is able to find ways of action that maximize the chance it will achieve its goals.</li>
</ol>
These two factors lead to a strategy of self-preservation. See also the earlier section where we address the question of consciousness: [link:Consciousness]
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div>&#10149; <a href='instrumental-incentives.html#argnav'>Go back</a></div>
<div>&#10149; <a href='#feedback'>Send Feedback</a></div>
