---
layout: argument
title: "Consciousness won’t happen"
breadcrumbs: Instrumental Incentives:instrumental-incentives,Consciousness won’t happen:no-self-preservation-without-consciousness
---

<blockquote>
Not even that would be able to question, "Do I want to do this? Or do I want to do this other thing instead? Is this good for me? Or are humans going to get rid of me?" I don't see any sort of self-reasoning coming out of a machine. I just don't imagine. I don't foresee it. 
</blockquote>


<blockquote>
Yeah, that goes into the line of consciousness and knowing how you're perceiving yourself, how others perceive you, and I feel like that's very, very, very far into the future, maybe that that could ever happen. That also plays into emotions maybe in general, like how it sees itself. It's not in this like task to optimize for living longer or not being shut off or something you're gonna just train, then I don't see how that could happen but... 
</blockquote>

<div>Our argument does not require the agent to have a sense of self-preservation (in the sense that they care about themselves). All that is necessary to get self-preservation is</div>
<ol><li>that the system has knowledge of itself and about how it could influence the world</li>
<li>that the system does advanced planning and is able to find ways of action that maximize the chance it will achieve its goals.</li>
</ol>
These two factors lead to a strategy of self-preservation. See also the earlier section where we address the question of consciousness: [link:Consciousness]
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
