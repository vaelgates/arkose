---
layout: argument
title: "Misuse is a bigger problem"
breadcrumbs: The Alignment Problem:the-alignment-problem,Misuse is a bigger problem:misuse-is-a-bigger-problem
---
<div>
<blockquote>
</div>
<div>I guess like anything I see, there is a risk, just like any other tool, that this will be used for nefarious purposes, like automated systems that make decisions in a way that we don't quite understand. Then baking in of bias, whether we're talking about automated weapon systems that make decisions on closing the loop in deciding who to shoot to automated decisions and policing and things like that.  </div>
<div>
</blockquote>
</div>
<ul><li>Misuse continues to be a problem as we get more and more powerful systems. A great example of this is nuclear weapons– it used to be that a small number of people couldn’t destroy the world, but now whoever is in control of those weapons has a huge amount of power to affect the world in a way that wasn’t true historically.</li>
<ul><li>You might argue that AI is similar, in that a small number of actors may have an outsized ability to affect the world. I think this in fact might be true, in that I expect the development of AGI to happen at a few well-funded AGI-aimed companies [due to the scaling hypothesis], and those companies will have outsized power (alternatively, others think the develop of AGI might be more decentralized, but that’s also a problem for misuse).</li>
</ul><li>At least humanity has dealt with misuse before. Technological misuse has been very bad in the past when leaders have directed an outsized amount of resources, and much attention should be focused on this.</li>
<li>TODO do something with vaels paragraph below (I haven’t found a wording that I was satisfied with yet)</li>
</ul><div>V: But we also want to pay attention to a more neglected risk that could potentially be even worse, but regardless could be on-par with the possibility of destruction. And that’s that the thing we’re creating, AGI, captures the best of humanity’s progress ([intelligence] ← bad sentence, fix) and could overtake [human actor nonsense, like this’ll be big])</div>
<div>L: I don’t quite understand the intent behind the language here but made something slightly different out of it (see above)</div>
