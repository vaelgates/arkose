---
layout: argument
title: "Nothing we can do at the moment"
breadcrumbs: The importance of more safety work:the-importance-of-more-safety-work,Nothing we can do at the moment:not-useful-currently
---
<p>Some argue that future AGI will use techniques we don’t know about yet. Therefore, it would be useless to try and work on safety now.</p>
<ul><li>Some AI safety researchers consider it plausible that AGI will be developed mostly using present-day techniques, and that it will happen soon. In that case, there seems to be a very strong case that safety techniques should be researched now, building upon the capabilities we do have (even if they do not constitute “general” intelligence yet). Furthermore, work in AI alignment helps us understand current systems better and thus use them more responsibly and effectively.</li>
<li>While future AGI will use future techniques, it is possible these will have some relation to AI systems currently in use. Therefore, if we develop alignment strategies that work on current systems, we could lay valuable groundwork for the alignment of future systems.</li>
<li>Much present-day work on AI safety is only weakly dependent on the technical details of the architecture of AGI. (See below for some examples of work currently being done). Therefore, our ignorance about these details is not a huge problem. Additionally, the field of AI safety research is really broad and covers many different approaches. It’s likely that at least some of them will be quite useful—even in the future with new techniques.</li>
<ul><li>Any progress on alignment, even without knowing how future AGI is built, would be a big step in the right direction. In the words of AI safety researcher <a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>Paul Cristiano writes</a>: “For now, finding any plausible approach to alignment, that works for any setting of unknown details, would be a big accomplishment. With such an approach in hand we could start to ask how sensitive it is to the unknown details, but it seems premature to be pessimistic before even taking that first step.”</li>
<li>Mechanistic interpretability is an attempt to reverse engineer the detailed computations performed by a model. In the past, this has been done mostly on CNN vision models. Researchers at AI safety research company Anthropic have made progress on <a href='https://transformer-circuits.pub/2021/framework/index.html' target='_blank'>mechanistic interpretability for Transformer language models (Olsson et al, 2022)</a>. David Bau’s lab at Northeastern University has developed methods for finding specific factual associations in transformer language models and then editing them (<a href='https://arxiv.org/abs/2202.05262' target='_blank'>Meng at al, 2022</a>).</li>
<li>Redwood Research is working on <a href='https://arxiv.org/abs/2205.01663' target='_blank'>reliability through adversarial training</a>, on a state-of-the-art language model.</li>
<a href='https://www.davidscottkrueger.com/' target='_blank'><li>David Krueger’s group</a> at the University of Cambridge is working on reward modeling and reward gaming, among other research directions.</li>
<a href='../resources.html' ><li>More resources</a></li>
</ul><li>Currently, there are so few people working on AI safety that it seems worthwhile to increase the effort quite a bit , considering the magnitude of the problem. Also, consider how important AI will become over the next decades even before AGI is invented. <br/><li>At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
<li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordinating around not creating AGI at all to avoid catastrophe.</li>
</ul><p>Further Reading:</p>

<a href='https://arxiv.org/abs/2209.00626' target='_blank'>“The alignment problem from a deep learning perspective”</a> by Richard Ngo et al
<a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>“Prosaic AI Alignment”</a> by AI safety researcher Paul Cristiano 
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
