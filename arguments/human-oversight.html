---
layout: argument
title: "This won’t be a problem, we will have human oversight"
breadcrumbs: Instrumental Incentives:instrumental-incentives,This won’t be a problem we will have human oversight:human-oversight
---

<blockquote>

<div>“Ok, an AI system might try to prevent itself from being switched off. But it won’t succeed, and we will uncover the deception” </div>
“This could happen in theory, but would not happen in practice - surely we will oversee its operation and find a way to turn it off if necessary” 
</blockquote>

<div>Current ML models are, in many cases, black boxes. There is a lot of work going into interpretability, and it is likely we will get better at understanding these models. That being said, we are far from having clear insight into large state-of-the-art models. If research on AI capabilities continues progressing, it is likely that when TAI arrives, we will not yet have the ability to truly understand what it does internally. Hypothetically, if the TAI system has enough insight into its own inner workings, it might even rewrite itself in a way that makes it even more difficult to understand.</div>
<div>To avoid itself being switched off, a future AI system would have a wide range of options:</div>
<ul><li>Hide information that would cause human operators to be skeptical about the systems next steps.</li>
<li>Convince or bribe its human operator.</li>
<li>Display human-approved, conservative behaviour. Then wait for the point where the system has been deployed to many different geographically separated machines.</li>
<li>Gain access to servers on the internet and run copies on them. (Even average human hackers are able to do this regularly)</li>
</ul><div>An AGI could be smarter than a human, which means it might out-think its operators, anticipate all kinds of ways in which it could be stopped - and design strategies against that.</div>
<div>If the AI is incentivized against its operators, we are basically doomed. The way to stop it is not to see what it does and then react by trying to shut it down. Instead, we must find a way to design it so that it is aligned with our interests.</div>
<div>The real world contains a very wide action space. Shoring up one part of that action space really does not help against a powerful malevolent AI.</div>
<div>IT security is far from a solved issue, current human-designed IT systems often have security holes that other humans can find. A human-designed sandbox for an AI has probably some security holes that a smarter-than-human AI could identify and exploit.</div>
<div>Of course, even if we design it to be aligned with our interests, we should still make sure we have good safety measures in place - because there are always unexpected failure modes with tech.</div>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div>&#10149; <a href='instrumental-incentives.html#argnav'>Go back</a></div>
<div>&#9993; <a href='#feedback'>Send Feedback</a></div>
