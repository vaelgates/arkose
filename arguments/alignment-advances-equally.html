---
layout: argument
title: "Alignment will progress automatically"
breadcrumbs: The Alignment Problem:the-alignment-problem,Alignment will progress automatically:alignment-advances-equally
---
<p>There’s an argument that alignment advances will proceed at the same pace as capabilities advances. If alignment is trying to have an AI system be aligned with human goals, then isn’t that also the point of “capabilities”, or advancing AI in general? Would they not proceed apace?</p>
<ul><li>This is definitely a possibility! Maybe capabilities would be directly tied to alignment in some way. Maybe the development of advanced AI systems will go just fine.</li>
</ul><p>However, there is the distinct possibility that things will not be fine. One reason for that is instrumental incentives : An advanced AI system, regardless of its goals, will probably have certain incentives relating to its own capabilities. It might, for example, employ strategies to avoid it from being switched off, and try to get access to more compute. We will cover this possibility in a later chapter.</p>

<blockquote>
I'm actually far less worried about the technical side of this. I just finished reading this book about von Neumann, that's a little cute biography of him, and there's a part where he says, supposedly, that people who think mathematics is complicated only say that because they don't know how complicated life is. And I'm totally messing with the phrasing, but something like that. I actually think any technical problems in this area will be solved relatively easily compared to the problem of figuring out what human values we want to insert into these. 
</blockquote>

<ul><li>Economic incentives mean things are often deployed without being perfectly safe, especially if people don’t think it needs to be super carefully checked.</li>
<li>Before the first test of the atomic bomb, there were some concerns that an energy release of this magnitude might cause a chain reaction within the atmosphere - turning the surface of earth into a barren wasteland. Further calculations were done and showed that this is not possible. However, the calculation was only checked by a small number of researchers and it could have been possible, though unlikely, that there was a mistake in the calculation or the underlying assumptions. <a href='https://blogs.scientificamerican.com/cross-check/bethe-teller-trinity-and-the-end-of-earth/' target='_blank'>Further reading</a></li>
<li>Current AI systems have significant ethical problems, and some of these problems are only discovered in production.</li>
<li>Often, it is not possible to fix all these problems (for example, ChatGPT can be instructed to write toxic material despite efforts at preventing that)</li>
<li>It is possible that an AGI system would behave reasonably well in testing but then produce catastrophic results in the real world.<br/></li>
<li><li>An AGI system, in order to have general intelligence, will probably need to be aware about its own inner workings and its place in the world. This would mean it might be able to know that it’s being trained, and it could intentionally display safe behavior in training, hiding its true intentions until later. This sounds far-fetched, but is a natural consequence of assuming that future systems will have high amounts of intelligence and an accurate model of the world.</li>
<li>Maybe alignment will be solved during the normal cause of AI research - but maybe not! We cannot say that for certain, given what we see with current technologies - some of them turn out to be quite safe inherently, others are risky even if properly managed.</li>
<li>Alignment and Capabilities are not the same. You can have very advanced systems with bad alignment, for example a GPT-style text generator that seems really smart - but constructs elaborate falsehoods that seem true at first glance.</li>
<li>Even if it seems likely that alignment will advance alongside capabilities, we don’t have any guarantee about that. The danger from a badly aligned system seems really high - so it’s probably worth worrying about that.</li>
<li>Safety often lags behind in the deployment of innovative technology. <li>There is a long history of dangerous technology being deployed prematurely, before the technology was understood well enough to be safe. Some examples:</li>
<ul><li>Radioactive toothpaste being sold in Germany in the 1920s</li>
<li>Early passenger aircraft (like the de Havilland Comet ) had rectangular windows, before it was understood that rectangular features put too much stress on the airframe - leading to several disasters.</li>
<li>The Therac-25 radiation treatment machine which killed several people due to a programming error.</li>
<li>The dangerous reactor design which contributed to the Chernobyl disaster.<br/></li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
