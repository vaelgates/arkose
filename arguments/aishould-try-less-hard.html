---
layout: argument
title: "It would be simple to instruct the AI to try less hard"
breadcrumbs: Instrumental Incentives:instrumental-incentives,It would be simple to instruct the AI to try less hard:aishould-try-less-hard
---

<blockquote>
I see that some problems arise from extreme optimization, from maximization of a certain utility function. But we could simply write an AGI that tries less hard in a sense. This would fix all of these concerns. 

</blockquote>

<div>It might be possible to do that, however many of the simple and obvious solutions might not work:</div>
<ul><li>AI systems which try to maximize a certain goal (maximizers) are dangerous.</li>
<ul><li>Suppose we would want them to collect as many postage stamps as possible, and that’s their only goal - it would be willing to take extreme actions just to increase that number a tiny bit further</li>
</ul><li>We could make our utility function bounded, for example: Collect more than 100 stamps, you don’t get extra points for stamps beyond 100.</li>
<ul><li>Then the agent might take any action that would result in a number of stamps that’s bigger than 100, and some of these actions might be dangerous.</li>
<li>Also, since the real world is uncertain (e.g. the package with the postage stamps might get lost), the agent might take extreme steps to maximize the probability of actually getting over 100 - for example, ordering many millions of stamps.</li>
</ul><li>We could develop an AI system that tries to make sure it has exactly 100 stamps - not more, not less</li>
<ul><li>But again, since the real world is uncertain, it might take extreme action of trying to protect its stash of 100 stamps, making sure nobody is able to steal a stamp, and counting & recounting them over and over again.</li>
</ul><li>We could try to fix this by incentivizing short plans. The plan with the fewest number of steps is less likely to be catastrophic (for example, just ordering stamps on ebay)</li>
<ul><li>But there are simple plans that can be catastrophic. For example: Get into your own source code, and change yourself to a maximizer. That’s a simple plan that would make the agent achieve its goals - and lead to all the risks outlined above.</li>
</ul><li>One proposed solution is Quantilizers, which tries to combine human imitation and expected utility maximization:</li>
<ul><li>Have the AI come up with a large distribution of possible strategies that would help it achieve its goal</li>
<li>Calculate expected utility for each possible strategy</li>
<li>Calculate how likely it is that a human would use this strategy.</li>
<li>Use a strategy with a good trade-off between expected utility and “human-likeness”</li>
<li>Again, there are failure modes: For example, one strategy that a human might consider is building a utility maximizer. This is quite likely in the future, when out-of-the-box utility maximizers might be available as downloadable PC programs. Therefore, the quantilizer might also choose this strategy.</li>
</ul><li>This shows that many simple approaches to AI alignment don’t work, and they often have surprising failure modes.</li>
</ul><div>References:</div>
<div>https://intelligence.org/files/QuantilizersSaferAlternative.pdf</div>
<div>Part 1 https://www.youtube.com/watch?v=Ao4jwLwT36M</div>
<div>Part 2 https://www.youtube.com/watch?v=gdKMG6kTl6Y</div>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div>&#10149; <a href='instrumental-incentives.html#argnav'>Go back</a></div>
<div>&#9993; <a href='#feedback'>Send Feedback</a></div>
