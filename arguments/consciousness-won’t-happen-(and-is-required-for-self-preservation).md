---
layout: argument
title: "Consciousness won’t happen (and is required for self-preservation)"
breadcrumbs: Instrumental Incentives:instrumental--incentives,Consciousness won’t happen (and is required for self-preservation):consciousness-won’t-happen-(and-is-required-for-self-preservation)
---
<blockquote>A human is conscious and thus worries about their own death. So do animals. But a computer won’t be conscious in the same way. So I think these issues of self-preservation are not going to be present in a machine, at all. (made up)
</blockquote>
Our argument does not require the agent to have a sense of self-preservation (in the sense that they care about themselves). [textblock:AllThatIsRequiredForSelfPreservation]All that is necessary to get self-preservation is
<ol><li>that the system has knowledge of itself and about how it could influence the world</li>
<li>that the system does advanced planning and is able to find ways of action that maximize the chance it will achieve its goals.</li>
</ol>[/textblock]
These two factors lead to a strategy of self-preservation. See also the earlier section where we address the question of consciousness: [link:Consciousness]
