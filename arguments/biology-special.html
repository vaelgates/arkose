---
layout: argument
title: "There is something special about biology which we will never be able to put into machines"
breadcrumbs: WhenAGI:when-agi,Never:never,There is something special about biology:biology-special
---
<div><blockquote>There is certainly something special about biology. We still cannot replicate many biological systems. (made up)</blockquote></div>
<div>Consider that current AI systems can do things that would have seemed magical just a few years ago. For example:</div>
<ul><figure><img src='{% link assets/images/arguments/AEF3DE56603309886155530EA395EE38.png %}' referrerpolicy='no-referrer'/><figcaption markdown='1'>Image generated by DALL-E 2 based on the text prompt “teddy bears shopping for groceries in ancient Egypt”. DALL-E (2021) generates images based on text prompts while exhibiting what could be reasonably described as “true creativity”
</figcaption></figure>
<figure><img src='{% link assets/images/arguments/341480D2BBB75796A367698A9E01B11C.png %}' referrerpolicy='no-referrer'/><figcaption markdown='1'>Google Imagen (2022) produces even more realistic depictions of even wilder scenarios. This is “Sprouts in the shape of text &#39;Imagen&#39; coming out of a fairytale book.”
</figcaption></figure>
<li>GPT-3 (2020) is able to write convincing articles, generate code based on natural language, and perform well on a wide range of text-based tasks.</li>
<li>PaLM (2022) improving upon GPT-3 and other state-of-the-art systems mostly by increasing the number of parameters and training on better hardware - without revolutionary new insights into the nature of human intelligence.</li>
<ul><li>The system improves upon the state-of-the-art in 28 of 29 tested NLP tasks.</li>
<li>It is able, among other things, to explain an original joke in two-shot prompts.</li>
<li>In code generation, it equals the performance of Codex 12B (which was fine-tuned on that task) while using 50 times less Python code.</li>
