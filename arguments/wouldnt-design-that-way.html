---
layout: argument
title: "Wouldn’t design it that way"
breadcrumbs: Instrumental Incentives:instrumental-incentives,Wouldn’t design it that way:wouldnt-design-that-way
---

<blockquote>
So we get to build the thing, and we get to build it however we like, and so it's perfectly reasonable to put hard constraints on its behavior. And in fact, I would be shocked if we didn't. So I do think, yes, you could imagine a system where it was doing clever things because we have built it in such a way where we only provide a utility function. And then we do dumb stuff, like ask it things that contradict its utility function. Then it’s not unexpected that it would try and get around you. But we get to build this thing completely. I guess maybe some of the end-to-end deep learning people don't understand that, because they don't want to build it completely. They just want to learn all the stuff. But we can put in whatever we want, we literally get to write the code. You get to say, “if I ask you a question with the keyword X in it, you have to do a complete system dump of everything.” Of course, we could do that. And so you could say, "Here's a magic word, and there's no way to overwrite it. If I say ‘sesame’ you have to shut down on the spot." And you could hard-code that in at the level of circuitry – which is a thing that is done by the way, for many industrial robotic systems. If you hit the red button, the power drops and there's no way around that. That's just built-in, of course. And so I think that any level of thoughtful engineering could get around that problem. And I think that if you look at safety systems in industrial robotics – and here I don't mean intelligent robots, I mean like robots used to build cars – those safety systems are actually very highly engineered and regulated and well thought out. Leading to very, very low numbers of injuries per million hours worked. So that kind of regulation, the way to think about that, exists. And I think it's totally reasonable that we could build that in, and we have a long time to think about doing it. So that doesn't make me as nervous. 
</blockquote>

<ul><li>The companies that are racing towards AGI have a strong economic incentive to build a first version as quickly as possible, without thinking through all the possible consequences.</li>
<li>Although industrial robots, for example, are subject to strict safety regulations, the field of AI is so rapidly evolving that it may be difficult for regulations to keep pace. Furthermore, even if there are some regulations in place for AI, it is likely that the development of AGI will catch us off guard, and existing regulations may not be sufficient to address the situation.</li>
<li>We all know that companies always wait for full safety before they release their product, right? :-)</li>
<li>Any naively built AGI might have problems of the sort mentioned here.</li>
<li>We currently do not know how to prevent “instrumental incentives” type concerns.</li>
<li>Compared to how fast capabilities are happening, there aren’t enough people working on solving the alignment problem. <br/><li>At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
