---
layout: argument
title: "The importance of more safety work"
breadcrumbs: The importance of more safety work:the-importance-of-more-safety-work
---
<ul><br/><li>At the moment, very few people work on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI:<br/>see <a href=' https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en '>one estimate</a>, <a href=' https://80000hours.org/problem-profiles/artificial-intelligence/ '>another one</a> Obviously, there is no proof that TAI will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety is grossly neglected. There is no reason to expect the safety community to suddenly grow by itself.<br/></li>
<li>While AI might come in 100 years or later, there is some chance it will come much sooner. There is some chance we will be surprised and suddenly faced with potentially disastrous unaligned AGI.</li>
<li>Even if it takes 100-200 years, it is not at all clear that alignment will be good enough by then.</li>
</ul><div>Further Reading:</div>
“Prosaic AI Alignment” by AI safety researcher Paul Cristiano https://ai-alignment.com/prosaic-ai-control-b959644d79c2 
<div>V: The "difficulty of the alignment problem" has a bunch of subtopics-- difficulty of the technical problem, whether it'll be solved by default,  "who will work on this" (which is what the "economic incentives, work on AI safety version – safety community is growing less quickly than the capabilities community" argument is aimed at), difficulty of the governance / implementation.)</div>
<div><a href='{site.baseurl}/arguments/regulators-will-take-care-of-this.html'>Regulators will take care of this</a></div>
<div><a href='{site.baseurl}/arguments/not-useful-currently.html'>Work on AGI safety is not useful because there is nothing we can do at the moment</a></div>
<div><a href='{site.baseurl}/arguments/not-urgent-currently.html'>I’m not convinced this is urgent now</a></div>
<div><a href='{site.baseurl}/arguments/lets-build-ai-and-then-worry-about-safety-later.html'>Lets build AI and then worry about safety later</a></div>
<div><a href='{site.baseurl}/arguments/alignment-is-easy-this-will-definitely-be-solved-in-time.html'>Alignment is easy - This will definitely be solved in time</a></div>
<div><a href='{site.baseurl}/arguments/not-currently-tractable.html'>Yeah there are people working on this but I don’t understand why their work would be useful</a></div>
