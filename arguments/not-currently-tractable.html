---
layout: argument
title: "I don’t see how current safety work is useful"
breadcrumbs: The importance of more safety work:the-importance-of-more-safety-work,I don’t see how current safety work is useful:not-currently-tractable
---

<blockquote>
Yeah, I just don't think at this point, it's like a particularly scientifically tractable question. I agree, it would probably be hard, but it's not an answerable question because I think to perhaps answer this question, you would need to know what this AI... The shape of this AI system perhaps or what this would look like, and we just don't have a sense of what this would look like. Imagine 300 years ago, people wondering about cars and thinking about what would happen if they got a flat tire. You would need to know what a tire is and how this tire works, or what the material that this tire is made of. So yeah, I agree it's a hard problem, it's just like... Yeah, not tractable at this point, or it's not easy to even formulate this as a tackable research question… 
</blockquote>

<p>Work on AGI alignment is definitely not easy. Alignment research The field is in a pre-paradigmatic stage, and there is no consensus on when and how AGI might emerge.</p>
<p>That being said, there is a remarkable variety of research projects on the way that have produced concrete outputs. Some of them are more tractable than others. Collectively Put together , there has been a tremendous amount of progress over the past 10 years. While no “alignment solution” exists, we now understand the problem quite a bit better.</p>
<p>Philosophy served as an incubator for many fields of science in the past. Currently, some parts of alignment research are quite speculative and thus fall somewhat into the realm of philosophy. But others consist of really clear and straightforward research questions, applied to actual ML systems.</p>
<p>Here are some approaches that seem useful, and descriptions of the specific progress that has been made:</p>
</li>
<li>TODO: Explain a couple of approaches that are reasonably similar to present-day AI and easy to explain in a couple of paragraphs. Also make sure to mention a quick win for some of them, something tangible that was found.</li>
<li>
Finally: <li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordinat ing around not creating AGI at all ion to avoid catastrophe.
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
