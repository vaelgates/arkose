---
layout: argument
title: "Work not tractable currently"
breadcrumbs: Pursuing Safety Work:pursuing-safety-work,Work not tractable currently:not-currently-tractable
---

<blockquote><p>
Yeah, I just don't think at this point, it's like a particularly scientifically tractable question. I agree, it would probably be hard, but it's not an answerable question because I think to perhaps answer this question, you would need to know what this AI... The shape of this AI system perhaps or what this would look like, and we just don't have a sense of what this would look like. Imagine 300 years ago, people wondering about cars and thinking about what would happen if they got a flat tire. You would need to know what a tire is and how this tire works, or what the material that this tire is made of. So yeah, I agree it's a hard problem, it's just like... Yeah, not tractable at this point, or it's not easy to even formulate this as a tackable research question… 
</p></blockquote>

<p>Work on AGI alignment is definitely not easy. Alignment research is in a pre-paradigmatic stage, and there is no consensus on when and how AGI might emerge. That being said, if we believe there is a serious risk, it would seem prudent to at least try and solve the challenges - instead of waiting and doing nothing.</p>
<p>There is a remarkable variety of research projects on the way that have produced concrete outputs. Some of them are more tractable than others. Collectively, there has been a tremendous amount of progress over the past 10 years. While no “alignment solution” exists, we now understand the problem quite a bit better.</p>
<p>Philosophy served as an incubator for many fields of science in the past. Currently, some parts of alignment research are quite speculative and thus fall somewhat into the realm of philosophy. But others consist of really clear and straightforward research questions, applied to actual ML systems.</p>
<p>Here are some approaches that seem useful, and descriptions of the specific progress that has been made:</p>
<li>Mechanistic interpretability is an attempt to reverse engineer the detailed computations performed by a model. In the past, this has been done mostly on CNN vision models. Researchers at AI safety research company Anthropic have made progress on <a href='https://transformer-circuits.pub/2021/framework/index.html' target='_blank'>mechanistic interpretability for Transformer language models (Olsson et al., 2022)</a>. David Bau’s lab at Northeastern University has developed methods for finding specific factual associations in transformer language models and then editing them (<a href='https://arxiv.org/abs/2202.05262' target='_blank'>Meng et al., 2022</a>).</li>
<li>Redwood Research is working on <a href='https://arxiv.org/abs/2205.01663' target='_blank'>reliability through adversarial training</a>, on a state-of-the-art language model.</li>
<li>Leading AI companies <a href='https://openai.com/blog/our-approach-to-alignment-research/' target='_blank'>OpenAI</a> and <a href='https://deepmindsafetyresearch.medium.com/' target='_blank'>DeepMind</a> have teams working on different alignment research approaches. There are also academics and academic groups like <a href='https://humancompatible.ai/research' target='_blank'>CHAI</a>, the <a href='https://wp.nyu.edu/arg/' target='_blank'>NYU Alignment Research Group</a>, Dan <a href='https://people.eecs.berkeley.edu/~hendrycks/' target='_blank'>Hendrycks</a> and <a href='https://jsteinhardt.stat.berkeley.edu/' target='_blank'>Jacob Steinhardt</a> at UC Berkeley, and <a href='https://www.davidscottkrueger.com/' target='_blank'>David Krueger’s group</a> at the University of Cambridge working on different approaches.</li>
<a href='../resources.html' ><li>More resources</a></li>
Finally: <li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordinating around not creating AGI at all to avoid catastrophe.</li>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
