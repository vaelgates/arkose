---
layout: argument
title: "Current safety work isn’t useful"
breadcrumbs: The importance of more safety work:the-importance-of-more-safety-work,Current safety work isn’t useful:not-currently-tractable
---

<blockquote>
Yeah, I just don't think at this point, it's like a particularly scientifically tractable question. I agree, it would probably be hard, but it's not an answerable question because I think to perhaps answer this question, you would need to know what this AI... The shape of this AI system perhaps or what this would look like, and we just don't have a sense of what this would look like. Imagine 300 years ago, people wondering about cars and thinking about what would happen if they got a flat tire. You would need to know what a tire is and how this tire works, or what the material that this tire is made of. So yeah, I agree it's a hard problem, it's just like... Yeah, not tractable at this point, or it's not easy to even formulate this as a tackable research question… 
</blockquote>

<p>Work on AGI alignment is definitely not easy. The field is in a pre-paradigmatic stage, and there is no consensus on when and how AGI might emerge.</p>
<p>That being said, there is a remarkable variety of research projects on the way that have produced concrete outputs. Some of them are more tractable than others. Put together, there has been a tremendous amount of progress over the past 10 years. While no “alignment solution” exists, we now understand the problem quite a bit better.</p>
<p>Philosophy served as an incubator for many fields of science in the past. Currently, some parts of alignment research are quite speculative and thus fall somewhat into the realm of philosophy. But others consist of really clear and straightforward research questions, applied to actual ML systems.</p>
<p>Here are some approaches that seem useful, and descriptions of the specific progress that has been made:</p>
</li>
<li>(We are still working on this paragraph)</li>
<li>
Finally: <li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordinating around not creating AGI at all to avoid catastrophe.
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
