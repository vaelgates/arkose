---
layout: page
title: What Can I Do?
image: assets/images/pic01.jpg
nav-menu: true
order: 4
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>What can I do?</h1>
		</header>

<p>Reducing potential risk from advanced AI systems is an unsolved, difficult task. The <a html="https://www.alignmentforum.org/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as">communities that are working on this</a> are relatively small, and the pathways for what is helpful are uncertain. However, here are some candidates for reducing risk:</p> 

<div class="row">
	<div class="6u 12u$(small)">
	<p><a href="#technical" class="button fit">AI Alignment Research & Eng.</a> Making progress on technical AI alignment (research and engineering)</p>
	</div>
	<div class="6u 12u$(small)">
	<p><a href="#governance" class="button fit">AI Governance</a> Developing global norms, policies, and institutions to increase the chances that advanced AI is beneficial</p>
	</div>
</div>
<div class="row">
	<div class="6u 12u$(small)">
	<p><a href="#" class="button fit disabled">Support</a> Providing support for others working in AI alignment (e.g. <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_role_type%5D%5B0%5D=Operations&refinementList%5Btags_role_type%5D%5B1%5D=Management">operations roles</a>)</p>
	</div>
	<div class="6u 12u$(small)">
	<p><a href="#" class="button disabled fit">Discussion</a> Engaging in discussion about these risks with colleagues</p>
	</div>
</div>

<div class="row">
	<div class="12u 12u$(small)">
	<p><a href="resources" class="button fit">Learn more about AI Alignment</a> The technical fields of AI alignment and AI governance are still in their formative stages, making it important to thoroughly <a href="resources">understand the theoretical and empirical problems of alignment, and current work in these areas</a>.</p>
	</div>
</div>

<br>
<div id="calltoaction" class="box box-blue special">
	<p> If the arguments for working to reduce risks from advanced AI systems feel substantive to you, the field is pre-paradigmatic and <a href="https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help">needs many more thoughtful researchers, engineers, and support people</a>. We encourage you to investigate the resources below. Finally, if you would like guidance or connections and <b>you are interested in conducting research in AI alignment:</b></p>
	<a href="#book_a_call" class="button fit">book a call</a>
</div>
<hr>

<div id="technical"><h2> Technical AI Alignment Research and Engineering </h2>

<h3> Overview of the space </h3>

<p> There are different subareas and research approaches within the field of AI alignment, and you may be a better fit for some than others. 
<ul>
	<li>One of the major rough splits is:</li>
		<ul> 
			<li> <i>theoretical research</i> (e.g. <a href="https://alignment.org/">Alignment Research Center</a>, <a href="https://intelligence.org/">MIRI</a>) versus </li>
			<li> <i>empirical research and engineering</i> (e.g. <a href="https://www.redwoodresearch.org/">Redwood Research</a>, <a href="https://www.anthropic.com/">Anthropic</a>, <a href="https://deepmindsafetyresearch.medium.com/">DeepMind's alignment teams</a>, <a href="https://openai.com/blog/our-approach-to-alignment-research/">OpenAI's safety team</a>).</li>
		</ul> 
	<li>Academia (e.g. UC Berkeley's <a href="https://humancompatible.ai/">CHAI</a>, NYU's <a href="https://wp.nyu.edu/arg/">Alignment Research Group</a>, <a href="https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt</a>, <a href="https://www.davidscottkrueger.com/">David Krueger</a>, <a href="https://people.eecs.berkeley.edu/~hendrycks/">Dan Hendrycks</a>) and non-profit research organizations (e.g. <a href="https://www.cooperativeai.com/foundation">Cooperative AI Foundation</a>, <a href="https://far.ai/">FAR</a>) often fall somewhere in between theory and empirical alignment work. (Empirical work often needs access to state of the art models and so is resource intensive.)</li>
	<li>Engineering aimed at AI alignment is almost always in industry, and <a href="https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers">ML engineering</a> and research engineers are especially in-demand, but there's also <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_role_type%5D%5B0%5D=Software%20Engineering">a range</a> of engineering roles, especially security, and also software.</li>
	<li> There's also work in technical AI governance (e.g. Governance team at OpenAI) and <a href="https://epochai.org/">forecasting</a>. Finally, there are some <a href="https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency">independent researchers</a>, outside of academia and formal organizations, who publish on the <a href="https://www.alignmentforum.org/">AI Alignment Forum</a> and <a href="https://www.lesswrong.com/tag/ai#AI_Alignment">LessWrong</a> and tend to do more theory work.</li>
	<li>A provisionary list of alignment / safety organizations and examples of their work, as of Fall 2022: <a href="https://docs.google.com/document/d/1gimXyGj4nTU9TFJ6svlpmMtEWGbTrMoNYfzZMi8siAA/edit?usp=sharing">Shortform</a>, <a href="https://docs.google.com/document/d/1SXhls4pCFdJ6PbRnlmNiF3GhTSx3qq2SkDRsKGKb1O4/edit?usp=sharing">Longform</a>.</li>
	<li> For an orientation to the space's cultural components: <a href="https://www.alignmentforum.org/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as">AI Safety and Neighboring Communities: A Quick-Start Guide, as of Summer 2022</a> by Sam Bowman (NYU).</li>
</ul></p>

<h3> Funding sources // Job board </h3>
<ul>
	<li>Apply for funding from <a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">Open Philanthropy</a> and <a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a></li>
	<li> <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_role_type%5D%5B0%5D=Research&refinementList%5Btags_role_type%5D%5B1%5D=Software%20Engineering">Job Board</a> by 80,000 Hours</li>
</ul>

<h3> Guides to getting involved </h3>
<ul>
	<li> <b>Research</b>: <a href="https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/">FAQ: Advice for AI Alignment Researchers</a> by Rohin Shah (DeepMind) or <a href="https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment"> How to pursue a career in technical AI alignment</a></li>
	<li><b>Engineering</b>:  <a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Levelling Up in AI Safety Research Engineering</a></li>
	<li><b>Overall</b>: <a href="https://forum.effectivealtruism.org/posts/pbiGHk6AjRxdBPoD8/ai-safety-starter-pack">AI safety starter pack</a> and <a href="#book_a_call">book a call</a></li>
</ul>

<div class="box">
<h3> Interested in working in China? </h3>
<p> AI alignment is a relatively new subfield, and research is currently centered in the US and Europe. State of the art research development is not just taking place in the US and Europe, however, and there is particular need for new technical approaches and research into the AI alignment problem taking place in China.</p>
<ul>
	<li>If you're interested in working in technical alignment in China, please <a href="#book_a_call">book a call</a> or get in contact with <a href="https://www.tian-xia.com/">Tianxia 天下</a> and <a href="https://concordia-consulting.com/">Concordia Consulting 安远咨询</a>.</li>
	<li>Newsletter on China's AI landscape: <a href="https://chinai.substack.com/about">ChinAI Newsletter</a></li>
	<li><a href="https://80000hours.org/career-reviews/china-related-ai-safety-and-governance-paths/">Overview</a> by 80,000 Hours</li>
</ul>
</div>
</div>
<hr>






<div id="governance"><h2> AI Governance </h2>

<p>The Center for the Governance of AI (<a href="https://www.governance.ai/">GovAI</a>) describes the AI governance problem as "the problem of devising global norms, policies, and institutions to best ensure the beneficial development and use of advanced AI" (<a href="https://uploads-ssl.webflow.com/614b70a71b9f71c9c240c7a7/61d48553bf2faf58c3900bd2_GovAI-Research-Agenda.pdf">GovAI Research Agenda</a>). "AI governance" or "longtermist AI governance" is distinct from "AI policy" in its primary focus on advanced AI — the hypothetical general purpose technology — rather than current AI as it exists today. The field believes that different actions, risks, and opportunities come about when focusing on advanced AI systems as compared to more contemporary issues, though <a href="https://www.allandafoe.com/opportunity">AI governance and AI policy naturally interface with each other and have overlapping domains</a>. AI governance has historical roots in the study of <a href="https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence">existential risk</a> and the <a href="https://www.effectivealtruism.org/">Effective Altruism</a> community, so much current research draws from these communities, but the field is expanding with time.</p>
<ul>
	<li>Read through the <a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a> (highly recommended)</li>
	<ul>
		<li>One highlight: <a href="https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview">The longtermist AI governance landscape: a basic overview</a> (<a href="https://forum.effectivealtruism.org/topics/ai-governance">related posts</a>)</li>
	</ul>
	<li>If you're interested in a career in US AI policy: <a href="https://80000hours.org/articles/us-ai-policy/">Overview</a> by 80,000 Hours; <a href="https://www.openphilanthropy.org/open-philanthropy-technology-policy-fellowship/">Fellowship</a> by Open Philathrophy; <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_role_type%5D%5B0%5D=Policy">Job Board</a> by 80,000 Hours</li>
	<li>If you're interested in law: <a href="https://www.legalpriorities.org/">Legal Priorities Project</a>, and <a href="https://works.bepress.com/ghadfield/">Gillian Hadfield (U. Toronto)</a></li>
</ul>
</div>
<hr>






<div id="book_a_call"><h2>Book a call</h2>
<p>If you're interested in working in AI alignment and advanced AI safety, please book a call with <a href="https://vaelgates.com">Vael Gates</a>, who leads this project and conducted the <a href=interviews>interviews</a> as part of their postdoctoral work with Stanford University.</p>

<form id="book_call_form" method="post" action="#">
	<div class="row uniform">
		<div class="6u 12u$(xsmall)">
			<input type="text" name="name" id="name" value="" placeholder="Name" />
		</div>
		<div class="6u$ 12u$(xsmall)">
			<input type="email" name="email" id="email" value="" placeholder="Email" />
		</div>
		<div class="12u$">
			<div class="select-wrapper">
				<select name="interest" id="interest">
					<option value=""> Interested in talking about... </option>
					<option value="AI Alignment Research or Engineering">AI Alignment Research or Engineering</option>
					<option value="AI Alignment Governance">AI Alignment Governance</option>
					<option value="Other">Other (please specify below)</option>
				</select>
			</div>
		</div>
		<div class="12u$">
			<textarea name="message" id="message" placeholder="Enter your message" rows="6"></textarea>
		</div>
		<div class="12u$">
			<ul class="actions">
				<li>
          <button class="button" id="send_form_button">
            <div class="button-progress-bar"></div>
            <div class="button-text">Send Message</div>
          </button>
        </li>
			</ul>
		</div>
	</div>
</form>
</div>

</div>
</section>
</div>

<script src="{{ "assets/js/book_call.js" | absolute_url }}" type="module"></script>
<script>
  window.contactEmail = "{{site.email}}"
</script>