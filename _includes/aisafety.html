
<script src="{{ '/assets/js/list.min.js' | relative_url }}"></script>

<div id="main" class="alt">
  <section>
    <div class="inner">
  <header class="major">
      <h1>AI Safety Papers</h1>
  </header>

      <h2>Learn about large-scale risks from advanced AI</h2>

 <!--      <h3 id="papers">Selected Papers</h3> -->

  <!--    <div class="iframe-container">
        <iframe id="key_papers" onload="iframeLoaded('iframe_loading_spinner_key_papers')" src="https://airtable.com/embed/appOQF4xHFCwscK39/shrwwWZNAfTkrHP4h?backgroundColor=blueLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="685" style="background: transparent; border: 1px solid #ccc;"></iframe>
        <div id="iframe_loading_spinner_key_papers" class="iframe-loading">
          {% include loading_spinner.html %}
        </div>
      </div> -->
      <!-- and then this goes after </section>
        <script>
        function iframeLoaded(spinnerID) {
          document.getElementById(spinnerID).style.display = 'none';
        }
        </script>
      -->

      <ul>
        <li class="expandable expanded" data-toggle="overviews"><span style="background-color: lightgrey" class="color-bubble"><b>Overviews</b></span></li>
        <ul>
          <p>What types of risks from advanced AI might we face? These papers provide an overview of anticipated problems and relevant technical research directions.</p>
          <li><a href="https://arxiv.org/abs/2209.00626"><span style="background-color: lightgrey" class="color-bubble">(Ngo et al., 2022)</span> The Alignment Problem from a Deep Learning Perspective</a><a href="https://x.com/RichardMCNgo/status/1603862969276051457" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://www.safe.ai/ai-risk"><span style="background-color: lightgrey" class="color-bubble">(Hendrycks et al., 2023)</span> An Overview of Catastrophic AI Risks</a><a href="https://x.com/DanHendrycks/status/1671894767331061763" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2302.10329"><span style="background-color: lightgrey" class="color-bubble">(Chan et al., 2023)</span> Harms from Increasingly Agentic Algorithmic Systems</a><a href="https://x.com/_achan96_/status/1656690639264792579" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://llm-safety-challenges.github.io/"><span style="background-color: lightgrey" class="color-bubble">(Anwar et al., 2024)</span> Foundational Challenges in Assuring Alignment and Safety of Large Language Models</a><a href="https://x.com/DavidSKrueger/status/1779900511627452467" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
        </ul>
      </ul>
      <ul>
        <li class="expandable" data-toggle="evaluations"><span style="background-color: rgb(248, 190, 176)" class="color-bubble"><b>Dangerous Capability Evaluations</b></span></li>
        <ul>
          <p>To ensure that AI systems are safe, we need societal agreement on what is <i>unsafe</i>. Developing technical benchmarks for what dangerous capabilities look like in advanced AI systems is a necessary first step to support policymakers and labs deploying powerful models.</p>
          <li><a href="https://arxiv.org/abs/2212.09251"><span style="background-color: rgb(248, 190, 176)" class="color-bubble">(Perez et al., 2023)</span> Discovering Language Model Behaviors with Model-Written Evaluations</a><a href="https://x.com/AnthropicAI/status/1604883576218341376" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://metr.org/blog/2023-09-26-rsp/"><span style="background-color: rgb(248, 190, 176)" class="color-bubble">(METR, 2023)</span> Responsible Scaling Policies (RSPs)</a>, example: <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Anthropic's RSP</a></li>
          <li><a href="https://arxiv.org/abs/2403.13793"><span style="background-color: rgb(248, 190, 176)" class="color-bubble">(Phuong et al., 2024)</span> Evaluating Frontier Models for Dangerous Capabilities</a><a href="https://x.com/tshevl/status/1770744344669990981" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li>Resources:
          <ul>
            <li> <a href="https://metr.github.io/autonomy-evals-guide/">METR's Autonomy Evaluation Resources</a><a href="https://x.com/METR_Evals/status/1768684026792190410" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
            <li><a href="https://github.com/UKGovernmentBEIS/inspect_ai">UK AISI "Inspect"</a><a href="https://x.com/soundboy/status/1788910977003504010" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
            <li><a href="https://cdn.openai.com/o1-system-card-20241205.pdf">OpenAI o1 System Card</a>, sections 4.4.3 and 4.4.4</li>
          </ul>
        </ul>
      </ul>
      <ul>
       <li class="expandable" data-toggle="interpretability"><span style="background-color: moccasin;" class="color-bubble"><b>Interpretability</b></span></li>
        <ul>
              <p>One approach to improving safety is to improve our understanding of how AI models arrive at their outputs, to enable more robust monitoring and modification. In particular, attempting to understand models just by examining their weights (“mechanistic interpretability”) may be robust to deceptive or sycophantic behaviour from future systems.</p>
          <li><a href="https://distill.pub/2020/circuits/zoom-in/"><span style="background-color: moccasin" class="color-bubble">(Olah et al., 2020)</span> Zoom In: An Introduction to Circuits</a></li>
          <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><span style="background-color: moccasin" class="color-bubble">(Templeton et al., 2024)</span> Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a><a href="https://x.com/AnthropicAI/status/1792935506587656625" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2403.19647"><span style="background-color: moccasin" class="color-bubble">(Marks et al., 2024)</span> Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a><a href="https://x.com/saprmarks/status/1775513423402692685" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2310.01405"><span style="background-color: moccasin" class="color-bubble">(Zou et al., 2023)</span> Representation Engineering: A Top-Down Approach to AI Transparency</a><a href="https://x.com/andyzou_jiaming/status/1684766170766004224" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
         <!-- <li><a href="https://arxiv.org/abs/2202.05262"><span style="background-color: moccasin" class="color-bubble">(Meng et al., 2022)</span> Locating and Editing Factual Associations in GPT</a></li>-->
        </ul>
      </ul>

      <ul>
        <li class="expandable" data-toggle="robustness"><span style="background-color: rgb(192,238,192)" class="color-bubble"><b>Robustness</b></span></li>
        <ul>
              <p>Today and in the future, AI needs to be robust to adversarial attacks and unexpected distributional shifts. As models become more capable, ensuring they will represent human intentions across many settings is even more important: monitoring will become harder, there will be more incentive to jailbreak models, and models themselves may even become deceptive.</p>
<!--          <li><a href="https://arxiv.org/abs/2205.01663"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Ziegler et al., 2022)</span> Adversarial training for high-stakes reliability</a></li>-->
          <li><a href="https://arxiv.org/abs/2202.03286"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Perez et al., 2022)</span> Red Teaming Language Models with Language Models</a></li>
          <li><a href="http://arxiv.org/abs/2307.15043"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Zou et al., 2023)</span> Universal and Transferable Adversarial Attacks on Aligned Language Models</a><a href="https://x.com/andyzou_jiaming/status/1684766189443227648" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2306.15447"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Carlini et al., 2023)</span> Are aligned neural networks adversarially aligned?</a></li>
        </ul>
      </ul>

<!--       <ul>
      <li class="expandable" data-toggle="reward_misspecification"><span style="background-color: rgb(192,238,192)" class="color-bubble"><b>Reward Misspecification and Goal Misgeneralization</b></span></li>
        <ul>
          <p>How do we ensure that AI systems represent the goals we intend them to at deployment and under distributional shift, when we cannot maximally specify our preferences during training and many goals are compatible with the training data? Proxy reward signals generally correlate with designers' true objectives, but AI systems can break this correlation when they strongly optimize towards reward objectives.</p>
          <li><a href="https://arxiv.org/abs/2210.01790"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Shah et al., 2022)</span> Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals</a></li>
          <li><a href="https://arxiv.org/abs/2201.03544"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Pan, Bhatia and Steinhardt, 2022)</span> The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models</a></li>
        </ul>
      </ul> -->
      <!-- paleturquoise -->
      <ul>
        <li class="expandable" data-toggle="scalable_oversight"><span style="background-color: lavender" class="color-bubble"><b>Scalable Oversight</b></span></li>
        <ul>
          <p>As models approach or surpass human performance, it becomes more challenging to decide which actions are safe or unsafe. One approach to solving this problem is to have a simpler model either assess safety of a more complex model directly or aid a human overseer in spotting unsafe outputs.</p>
          <li><a href="https://arxiv.org/abs/2211.03540"><span style="background-color: lavender" class="color-bubble"> (Bowman et al., 2022)</span> Measuring Progress on Scalable Oversight for Large Language Models</a><a href="https://x.com/AnthropicAI/status/1590019597109202946" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2212.08073"><span style="background-color: lavender" class="color-bubble">(Bai et al., 2022)</span> Constitutional AI: Harmlessness from AI Feedback</a><a href="https://x.com/AnthropicAI/status/1603791161419698181" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2312.09390"><span style="background-color: lavender" class="color-bubble">(Burns et al., 2023)</span> Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision</a><a href="https://x.com/CollinBurns4/status/1735350133926314431" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2402.06782"><span style="background-color: lavender" class="color-bubble">(Khan et al., 2024)</span> Debating with More Persuasive LLMs Leads to More Truthful Answers</a></li>
        </ul>
      </ul>
      <ul>
        <li class="expandable" data-toggle="deception"><span style="background-color: rgb(241, 199, 221)" class="color-bubble"><b>Deception</b></span></li>
        <ul>
          <p>There are reasons to think that advanced AI systems may demonstrate deceptive or sycophantic behavior in training, but pursue their own goals in deployment. The papers below investigate whether this could be detected, whether present-day systems are capable of this kind of deception, and what could be done about it in the future.</p>
          <li><a href="https://arxiv.org/abs/2412.14093"><span style="background-color: rgb(241, 199, 221)" class="color-bubble">(Greenblatt et al., 2025)</span> Alignment faking in large language models</a><a href="https://x.com/AnthropicAI/status/1869427646368792599" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://www.apolloresearch.ai/research/scheming-reasoning-evaluations"><span style="background-color: rgb(241, 199, 221)" class="color-bubble">(Meinke et al., 2024)</span> Frontier Models are Capable of In-context Scheming</a><a href="https://x.com/apolloaisafety/status/1864735819207995716" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2312.06942"><span style="background-color: rgb(241, 199, 221)" class="color-bubble">(Greenblatt et al., 2024)</span> AI Control: Improving Safety Despite Intentional Subversion</a></li>
          <li>See also:</li>
          <ul>
            <li><a href="https://cdn.openai.com/o1-system-card-20241205.pdf">OpenAI o1 System Card</a>, section 4.4.3</li>
          </ul>
        </ul>
      </ul>
      <br>


      <!-- background-color: pink -->
      <!-- background-color: peachpuff -->
      <!-- background-color: moccasin -->
      <!-- background-color: rgb(192,238,192) -->
      <!-- background-color: lavender -->

      <h4 class="expandable" data-toggle="other_ai_safety_content">Talks, blog posts, newsletters, and upskilling</h4>
      <ul>

      <h4 class="expandable" id="highlighted_talk_series">Highlighted talk series</h4>
      <ul>
        <li><a href="https://www.alignment-workshop.com/bay-area-2024"><b>Bay Area Alignment Workshop (Oct 2024)</b></a>, <i>recordings available</i></li>
        <li><a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop (Dec 2023)</a>, <i>recordings available</i></li>
        <li><a href="https://pli.princeton.edu/events/princeton-ai-alignment-and-safety-seminar">Princeton AI Alignment and Safety Seminar</a></li>
        <br>
      </ul>

      <h4 class="expandable" id="content">Blog posts</h4>
        <ul>
          <li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
          <li><a href="https://wp.nyu.edu/arg/why-ai-safety/">Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022)</li>
          <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">More is Different for AI</a> by Jacob Steinhardt (2022)</li>
          <!--<li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0">Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)</li>-->
          <li><a href="https://theaidigest.org/">AI Digest: Visual explainers on AI progress and its risks</a></li>
          <li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper (ongoing)</li>
          <!-- <li class="expandable" data-toggle="general"><i>For a general audience</i></li>
          <ul>
            <li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper</li>
            <li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)</li>
            <li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)</li>
            <li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
            <li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
            <li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)</li>
            <li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcasts and articles)</li>
            <li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
          </ul> -->
      </ul>

        <h4 class="expandable" id="newsletters">Newsletters</h4>
        <ul>
          <li><a href="https://importai.substack.com">Import AI</a> by Jack Clark, <i>for keeping up to date on AI progress</i></li>
          <li><a href="https://newsletter.safe.ai/">AI Safety Newsletter</a> by the Center for AI Safety, <i>for AI safety papers and policy updates</i></li>
          <li><a href="https://www.transformernews.ai/">Transformer</a> by Shakeel Hashim, <i>for AI news with a safety focus</i></li>
          <li><a href="https://newsletter.danielpaleka.com/">AI Safety Takes</a> by Daniel Paleka, <i>for a PhD student's overview</i></a></li>
          <br>
        </ul>

      <h4 class="expandable" id="guides">Upskilling</h4>
      <ul>
        <li class="expandable" data-toggle="guides">Guides</li>
        <ul>
          <li> <b>Research</b>: <a href="https://aisafetyfundamentals.com/blog/alignment-careers-guide">Alignment Careers Guide</a></li>
          <li class="expandable expanded"><b>Engineering:</b></li>
          <ul>
            <li><a href="https://arena-roadmap.streamlit.app/">ARENA research engineering upskilling curriculum</a></li>
            <li><a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Leveling Up in AI Safety Research Engineering</a></li>
          </ul>
        </ul>
        <li class="expandable" data-toggle="courses">Courses</li>
        <ul>
          <li><a href="https://www.aisafetyfundamentals.com/ai-alignment-curriculum">AI Safety Fundamentals Curriculum</a> by BlueDot Impact </li>
          <li><a href="https://course.mlsafety.org/">Introduction to ML Safety</a> by the Center for AI Safety</li>
        </ul>
        <li class="expandable" data-toggle="tools">Tools</li>
        <ul>
          <li><a href="https://www.neuronpedia.org/">Neuronpedia</a>, a tool for visualising SAEs</li>
          <li><a href="https://blog.eleuther.ai/autointerp/">Autointerp</a>, a library for automatically generating labels for SAE features </li>
          <li><a href="https://github.com/METR/vivaria?tab=readme-ov-file">Vivaria</a>, a tool for running evaluations and agent elicitation research.</li>
        </ul>
      </ul>
      </ul>

      </ul>
    </ul>
    <h4 class="expandable" id="technical_governance"><b>Technical AI Governance</b></h4>
		<ul>
			<li>Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>
			<li><a href="https://docs.google.com/document/d/1KNkIrp1nr6ETXAFpu5k8wYz51sJlDyDDnTXtWwDcKBw/edit?ref=blog.heim.xyz">Blog post (Anonymous): "AI Governance Needs Technical Work"</a></li>
      <li><a href="https://blog.heim.xyz/technical-ai-governance/">Blog post by Lennart Heim: "Technical AI Governance"</a> (focuses on compute governance), <a href="https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/">Podcast: "Lennart Heim on the compute governance era and what has to come after"</a></li>
      <li><a href="https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/">Blog post by Luke Muehlhauser, "12 Tentative Ideas for US AI Policy"</a></li>
      <li><a href="https://80000hours.org/career-reviews/become-an-expert-in-ai-hardware/">Perspectives on options given AI hardware expertise</a> (80,000 Hours)</li>
      <li><i>Arkose is looking for further resources about technical governance, as this is a narrow set; please send recommendations to team@arkose.org!</i></li>
			<!-- Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>-->
		</ul>

    </div>
  </section>


<!--     <a href="https://airtable.com/appOQF4xHFCwscK39/shrQzex73hN01B1CR" class="button button-white button-small button-right">Suggest A Resource</a> -->

      <!--     <h3 style="clear:both"> Expanded List of Papers</h3>
    
    <a href="papers" class="button button-white fit">AI Safety Papers</a> -->










  <section class="bg-gray" id="find-more-ai-safety-paper">
    <div class="inner">
      <h2>Find more AI safety papers</h2>
      <div class="inner">
        <p>Curated with input from the experts on our <a href="https://arkose.org/#panel">Strategic Advisory Panel</a>, this collection of papers, blog posts, and videos contains top and recent AI safety papers. For more information on the categories listed, please reference this <a href="/glossary">glossary</a>.</p>
      </div>
      <div id="paper-list">
        <div class="row mt1 mb1">
          <div class="12u">
            <input type="search" class="search form-control" placeholder="Search" />
          </div>
        </div>
        <div class="row">
          {% include paper_filter.html field_id="category" field_display="Category" options=empty %}
          {% comment %}
            {% include paper_filter.html field_id="ml_subtopic" field_display="ML Subtopic" options=empty %}
            {% include paper_filter.html field_id="safety_topic" field_display="Safety Topic" options=empty %}
          {% endcomment %}
          {% include paper_filter.html field_id="type" field_display="Type" options=empty %}
          <div class="12u glossary"></div>
          <div class="12u filter-list"></div>
        </div>
        <div class="results-count">
          <span class="results-count-text"><span class="results-count-number">0</span> results</span>
        </div>
        <ul class="list"></ul>
      </div>
    </div>
  </section>
</div>

<div class="hidden">
  <li id="paper-item" class="paper-item">
    <div class="title-container">
      <img class="type-icon image" />
      <h3 class="Title"></h3>
    </div>
    <div class="detail-opener">
      <svg aria-label="open" aria-hidden="true" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="#666" stroke-width="2">
        <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
      </svg>
    </div>
    <div class="detail">
      <div class="CategoryDisplay"></div>
      <div class="TypeDisplay"></div>
      <div class="TwitterDisplay"></div>
      <div class="Blog or VideoDisplay"></div>
      <div class="Supplementary MaterialDisplay"></div>
      <div class="Transcripts / Audio / SlidesDisplay"></div>
      <div class="AbstractDisplay"></div>
    </div>
  </li>
</div>

<script>
  $(function() {
    var papersList;
    const filters = {
      "Category": new Set(),
      "Safety Topic": new Set(),
      "ML Subfield": new Set(),
      "ML Subtopic": new Set(),
      "Type": new Set(),
    }
    const filterOptions = {
      "Category": new Set(),
      "Safety Topic": new Set(),
      "ML Subfield": new Set(),
      "ML Subtopic": new Set(),
      "Type": new Set(),
    }

    const options = {
      valueNames: [
        'Title',
        'Safety TopicDisplay',
        'Category',
        'CategoryDisplay',
        'Type',
        'TypeDisplay',
        'Twitter',
        'TwitterDisplay',
        "Blog or Video",
        "Blog or VideoDisplay",
        "Supplementary Material",
        "Supplementary MaterialDisplay",
        "Transcripts / Audio / Slides",
        "Transcripts / Audio / SlidesDisplay",
        "ML Subfield",
        "ML SubfieldDisplay",
        "ML Subtopic",
        "ML SubtopicDisplay",
        "Abstract",
        "AbstractDisplay",
        { attr: 'src', name: 'image' }
      ],
      item: 'paper-item'
    };

    const glossary = {
      "Overviews and Agendas": "Resources that identify risks from advanced AI and research directions that might help. See also: <span class='add-category' data-value='Why large-scale safety?'>Why Large-Scale Safety</span>.",
      "Why Large-Scale Safety": "Resources that explore why risks from advanced AI might be particularly impactful to work on. See also: <span class='add-category' data-value='Overviews and agendas'>Overviews and Agendas</span>.",
      "Governance": "Resources on the technical work that could support regulation for AI safety.",
      "Scalable Oversight": "Resources on a proposed research direction focused on using simple AI systems to aid supervision of more capable AI.",
      "Deception": "A proposed risk of capable AI systems; resources in this category assess this risk or propose mitigations against it.",
      "Reward Misspecification and Goal Misgeneralization": "Relevant for advanced as well as present-day systems. This category collects resources relevant to these problems as they may present in more advanced systems.",
      "Interpretability": "A field related to explainability which aims to reconstruct the behavior of a model by inspecting its weights, similar to neuroscience. This may allow us to predict or mitigate risks in advanced systems.",
      "Model Evaluations and Benchmarks": "Resources about 'dangerous capability evaluations': benchmarks where scoring highly is bad, such as the ability to execute cybersecurity attacks. These may be particularly useful for governance of frontier models.",
      "Reinforcement Learning": "Resources that discuss the development of agentic AI systems, often via reinforcement learning, and their associated risks.",
      "Security": "Resources that cover research which includes preventing jailbreaking, removing backdoors, and cybersecurity methods to secure model weights.",
      "Theory": "Resources that explore various branches of theoretical research applied to AI safety, examining properties of hypothetical ML systems.",
      "Unlearning and Knowledge Editing": "Resources on altering what models 'know' as a method of altering their behavior. See also: <span class='add-category' data-value='Interpretability / explainability'>Interpretability</span>.",
      "Adversarial Robustness": "Deals with ensuring models behave safely even when an adversary is trying to bypass safety mitigations. See also: <span class='add-category'>Non-Adversarial Robustness</span>.",
      "Non-Adversarial Robustness": "Research that investigates methods of ensuring models are safe under a wide variety of situations, though this research does not assume an adversary (unlike <span class='add-category'>Adversarial Robustness</span>)."
    }

    let papers = [];

    function isDarkMode() {
      if (window.matchMedia &&
        window.matchMedia('(prefers-color-scheme: dark)').matches)
        return true;

      return document.documentElement.getAttribute('data-theme') === 'dark';
    }

    function listify(papersJson) {
      papersJson.forEach(item => { if (!item['Type']) { item['Type'] = ["Other"] } });
      papers = papersJson.filter(paper => paper.Title && paper.Title.length > 0 && paper['Type'])
                        .map(function(paper) {
        const modeStr = isDarkMode() ? '-dark' : ''

        if (!paper.Type) {
          paper.image = `/assets/images/medium/other${modeStr}.svg`;
        } else if (paper.Type.length > 1 && paper.Type.includes("Paper")) {
          paper.image = `/assets/images/medium/paper${modeStr}.svg`;
        } else {
          paper.image = `/assets/images/medium/${paper.Type[0].toLowerCase()}${modeStr}.svg`;
        }
        paper['Category'] = paper['Category']
        paper['CategoryDisplay'] = paper['Category'].map(item => `<span class="field-single category-single">${item}</span>`).join('');
        paper['Safety Topic'] = paper['Safety Topic'] ?? [];
        paper['Safety TopicDisplay'] = paper['Safety Topic'].map(item => `<span class="field-single safety-topic-single">${item}</span>`).join('');
        paper['ML SubfieldDisplay'] = paper['ML Subfield'] && paper['ML Subfield'].map(tag => `<span class="field-single ml-subfield-single">${tag}</span>`).join('');
        paper['ML Subtopic'] = paper['ML Subtopic'] ?? [];
        paper['ML SubtopicDisplay'] = paper['ML Subtopic'].map(tag => `<span class="field-single ml-subtopic-single">${tag}</span>`).join('');
        paper['TypeDisplay'] = paper['Type'].map(tag => `<span class="field-single type-single">${tag}</span>`).join('');
        paper.Title = linkify(paper.Link, paper.Title.replace("\n", ''));
        paper['Transcripts / Audio / SlidesDisplay'] = linkify(paper['Transcripts / Audio / Slides'], 'Transcripts / Audio / Slides');
        paper['Supplementary MaterialDisplay'] = linkify(paper['Supplementary Material'], 'Supplementary Material');
        paper['TwitterDisplay'] = linkify(paper['Twitter'], '{% include twitter_icon.html %}', 'no-underline twitter-link');
        paper['AbstractDisplay'] = paper.Abstract ? `<h4>Abstract</h4>${paper.Abstract}` : '';
        return paper;
      });

      papersList = new List('paper-list', options, papers);

      $(document).on('change', '.filter-dropdown input', function(e) {
        var filter = this.value;
        var filterType = $(this).closest('.filter-dropdown').data('filter-type');
        if (filters[filterType].has(filter)) {
          filters[filterType].delete(filter);
        } else {
          filters[filterType].add(filter);
        }
        updateList();
      });

      $(document).on('click', '.add-category', function(e) {
        var category = $(this).data('value') || this.innerText;
        if (!filters['Category'].has(category)) {
          $(`.filter-dropdown-category input[type=checkbox][value='${category}'`).prop('checked', true).trigger('change')
        }
        updateList();
      });

      // Get options for each filter from the JSON
      Object.keys(filterOptions).forEach(function(filterType) {
        papers.forEach(function(paper) {
          paper[filterType] && paper[filterType].forEach(function(filter) {
            filterOptions[filterType].add(filter);
          });
        });
      });

      // Replace the filter options of each ".filter-dropdown ul" with the ones from the JSON
      Object.keys(filterOptions).forEach(function(filterType) {
        var filterDropdown = $('.filter-dropdown-' + slugify(filterType) + ' ul');
        Array.from(filterOptions[filterType]).sort().forEach(function(option) {
          filterDropdown.append(`<li data-option="${option}">
            <input type="checkbox" id="option-${slugify(filterType)}-${option}" name="option-${slugify(filterType)}-${option}" value="${option}">
            <label for="option-${slugify(filterType)}-${option}">
              ${option}
              <span class="filter-option-count"></span>
            </label>
          </li>`);
        });
      });
      
      // On page load, read the query params and set the filters
      var params = new URLSearchParams(window.location.search);
      if (params.size === 0) {
        // Formerly, we'd default the filter Type to Paper if no filters were set. That code follows, but we've
        // commented it out for now.
        // filters['Type'].add('Paper');
        // $('input[name="option-type-Paper"]').prop('checked', true);
      } else {
        Object.keys(filters).forEach(function(filterType) {
          if (!params.has(filterType)) return

          params.getAll(filterType)[0].split(',').forEach(function(filter) {
            if (filter === '') return; // Type param can have an empty string to indicate it should *not* apply the default Type=paper

            filters[filterType].add(filter);
            $('input[name="option-' + slugify(filterType) + '-' + filter + '"]').prop('checked', true);
          });
        });
        document.getElementById('find-more-ai-safety-paper').scrollIntoView({ behavior: 'smooth' });
      }
      updateList();
      updateResultsCount();

      $('input.search').on('change', function(e) {
        updateFilterOptionCounts();
      });

      papersList.on('updated', function (list) {
        updateResultsCount();
      });
    }

    fetch('/export/papers.json')
      .then(response => response.json())
      .then(data => {
        listify(data)
      })
      .catch(error => console.error('Error loading JSON:', error));

    function resetList(){
      papersList.search();
      papersList.filter();
      papersList.update();
      $(".filter-all").prop('checked', true);
      $('.filter').prop('checked', false);
      $('.search').val('');
    };

    function updateFilters() {
      papersList.filter(function (paper) {
        return Object.keys(filters).every(function(filterType) {
          if (filters[filterType].size == 0) return true
          return paper.values()[filterType].some(item => filters[filterType].has(item))
        });
      });
    }

    function updateResultsCount(){
      $('.results-count-number').text(papersList.matchingItems.length);
    }

    function updateShownFilters(){
      var filterList = $('.filter-list');
      filterList.empty();
      Object.keys(filters).forEach(function(filterType) {
        filters[filterType].forEach(function(filter) {
          filterList.append(`<span class="filter-item filter-item-${slugify(filterType)}" data-filter-type="${filterType}">${filter}</span>`);
        });
      });
    }

    function updateShownGlossary(){
      var glossaryList = $('.glossary');
      glossaryList.empty();
      Object.keys(filters).forEach(function(filterType) {
        filters[filterType].forEach(function(filter) {
          if (filter === 'Interpretability / explainability') filter = 'Interpretability';
          if (filter === 'AI Governance') filter = 'Governance';
          const matchableFilter = Object.keys(glossary).find(key => key.toLowerCase().replace(/[^a-zA-Z\s]/g, '') === filter.toLowerCase().replace(/[^a-zA-Z\s]/g, ''));
          filter = matchableFilter;
          if (!glossary[matchableFilter]) return;
          glossaryList.append(`<div class="glossary-item" data-filter-type="${filterType}"><span class="bold">${filter}</span>: ${glossary[matchableFilter]}</div>`);
        });
      });
    }

    function updateFilterTitleCount(){
      Object.keys(filters).forEach(function(filterType) {
        const filterSlug = slugify(filterType)
        if (filters[filterType].size) {
          $('.filter-' + filterSlug + ' .dropdown-title').addClass('filter-active');
          // $('.filter-' + filterSlug + ' .dropdown-title .filter-type-count').text('(' + filters[filterType].size + ')');
        } else {
          $('.filter-' + filterSlug + ' .dropdown-title').removeClass('filter-active');
          // $('.filter-' + filterSlug + ' .dropdown-title .filter-type-count').text('');
        }
      });
    }

    function updateQueryParams(){
      var params = new URLSearchParams(window.location.search);

      Object.keys(filters).forEach(function(filterType) {
        params.delete(filterType);
        if (filters[filterType].size)
          params.append(filterType, Array.from(filters[filterType]));
      });
      var newRelativePathQuery = window.location.pathname + (params.toString() ? '?' : '') + params.toString();
      if (newRelativePathQuery != window.location.pathname + window.location.search)
        history.pushState(null, '', newRelativePathQuery);
    }

    function titlesMatch(title1, title2) {
      return title1.match(/<a.*>(.*)<\/a>/)?.[1] == title2.match(/<a.*>(.*)<\/a>/)?.[1]
    }

    function updateFilterOptionCounts() {
      Object.keys(filterOptions).forEach(function(optionFilterType) {
        filterOptions[optionFilterType].forEach(function(option) {
          const filterOptionCount = $(`.filter-${slugify(optionFilterType)} li[data-option="${option}"] .filter-option-count`);

          // If the filter is already set, just remove the count
          if (filters[optionFilterType].has(option)) {
            filterOptionCount.text('');
            return;
          }

          // When there are no options set for a filter, adding an option *reduces* options, which is easy to calculate and is the
          // first option here.
          if (filters[optionFilterType].size == 0) {
            const remainingItems = papersList.matchingItems.filter(item => {
              if (!item.values()[optionFilterType]) return true;
              return item.values()[optionFilterType].includes(option)
            });
            filterOptionCount.text('(' + remainingItems.length + ')');
          } else {
            // When there are options set for a filter, adding an option *increases* options, which we calculate here
            const unmatchedItems = papers.filter(paper => !papersList.matchingItems.some(item => titlesMatch(paper.Title, item.values().Title)))
            const newFilteredItems = unmatchedItems.filter(item => { return item[optionFilterType].includes(option) })

            // now apply the other filters
            const remainingFilteredItems = newFilteredItems.filter(item => {
              return Object.keys(filters).every(function(filterType) {
                if (filters[filterType].size == 0) return true
                if (filterType == optionFilterType) return true
                return item[filterType].some(item => filters[filterType].has(item))
              });
            });

            // now apply the search
            const searchTerm = $('input.search').val();
            // iterate through every field on each item and see if it matches the search term
            const remainingItems = remainingFilteredItems.filter(item => {
              return Object.keys(item).some(field => {
                return item[field].toString().toLowerCase().includes(searchTerm.toLowerCase())
              })
            })

            filterOptionCount.text(`(${remainingItems.length})`);
          }
        });
      });
    }

    function updateList(){
      updateFilters();
      papersList.update();
      updateFilterOptionCounts();
      updateShownFilters();
      updateShownGlossary();
      updateFilterTitleCount();
      updateQueryParams();
    }

    function linkify(s, linkText, cssClass = '') {
      if (!s) return '';
      return `<a href="${s}" target="_top" class="${cssClass}">${linkText || s}</a>`
    }

    function slugify(s) {
      return s.toLowerCase().replace(' ', '_').replace(/\s/g, '_').replace(/[^a-z0-9-_]/g, '');
    }

    // Clicks on the dropdown title show/hide the dropdown
    $(".dropdown-title-box").on("click", function(e) {
      e.stopPropagation();
      var target = $(this).data("target");
      if ($("#" + target).hasClass('active')) {
        $('.filter-dropdown').removeClass('active');
      } else {
        $('.filter-dropdown').removeClass('active');
        $("#" + target).addClass("active");
      }
    });

    // Clicks outside a dropdown close it
    $('body').on('click', function(e) {
      if ($(e.target).closest('.filter-dropdown').length) return;

      $('.filter-dropdown').removeClass('active');
    });

    // Clicks on the filter items remove them
    $('body').on('click', '.filter-item', function(e) {
      var filter = $(e.target).closest('.filter-item').text();
      var filterType = $(e.target).closest('.filter-item').data('filter-type');
      $('input[name="option-' + slugify(filterType) + '-' + filter + '"]').prop('checked', false);
      filters[filterType].delete(filter);
      updateList();
    });

    // Clicks on the list items toggle them open/closed
    $('body').on('click', '.paper-item', function(e) {
      if ($(e.target).closest('a').length) return;

      e.stopPropagation();
      $(e.target).closest(".paper-item").toggleClass("active");
      $(e.target).closest(".paper-item").find('.detail').slideToggle(200);
    });
  });
</script>
