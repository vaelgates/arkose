
<script src="/assets/js/list.min.js"></script>

<div id="main" class="alt">
  <section>
    <div class="inner">
  <header class="major">
      <h1>AI Safety Papers</h1>
  </header>

      <h2>Learn about large-scale risks from advanced AI</h2>

 <!--      <h3 id="papers">Selected Papers</h3> -->

  <!--    <div class="iframe-container">
        <iframe id="key_papers" onload="iframeLoaded('iframe_loading_spinner_key_papers')" src="https://airtable.com/embed/appOQF4xHFCwscK39/shrwwWZNAfTkrHP4h?backgroundColor=blueLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="685" style="background: transparent; border: 1px solid #ccc;"></iframe>
        <div id="iframe_loading_spinner_key_papers" class="iframe-loading">
          {% include loading_spinner.html %}
        </div>
      </div> -->
      <!-- and then this goes after </section>
        <script>
        function iframeLoaded(spinnerID) {
          document.getElementById(spinnerID).style.display = 'none';
        }
        </script>
      -->

      <ul>
        <li class="expandable expanded" data-toggle="overviews"><span style="background-color: lightgrey" class="color-bubble"><b>Overviews</b></span></li>
        <ul>
          <p>What types of risks from advanced AI might we face? These papers provide an overview of anticipated problems and relevant technical research directions.</p>
          <li><a href="https://arxiv.org/abs/2209.00626"><span style="background-color: lightgrey" class="color-bubble">(Ngo et al., 2022)</span> The Alignment Problem from a Deep Learning Perspective</a><a href="https://x.com/RichardMCNgo/status/1603862969276051457" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://www.safe.ai/ai-risk"><span style="background-color: lightgrey" class="color-bubble">(Hendrycks et al., 2023)</span> An Overview of Catastrophic AI Risks</a><a href="https://x.com/DanHendrycks/status/1671894767331061763" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2302.10329"><span style="background-color: lightgrey" class="color-bubble">(Chan et al., 2023)</span> Harms from Increasingly Agentic Algorithmic Systems</a><a href="https://x.com/_achan96_/status/1656690639264792579" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://llm-safety-challenges.github.io/"><span style="background-color: lightgrey" class="color-bubble">(Anwar et al., 2024)</span> Foundational Challenges in Assuring Alignment and Safety of Large Language Models</a><a href="https://x.com/DavidSKrueger/status/1779900511627452467" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
        </ul>
      </ul>
      <ul>
        <li class="expandable" data-toggle="evaluations"><span style="background-color: pink" class="color-bubble"><b>Model Evaluations</b></span></li>
        <ul>
          <p>To ensure that advanced AI systems are safe, we need societal agreement on what is <i>unsafe</i> so that we can make appropriate tradeoffs with AI's anticipated benefits. Developing technical benchmarks for what dangerous capabilities are in advanced AI systems, as part of "model evaluations", is a first necessary step to concretize these tradeoffs for policymakers, researchers, and the public.</p>
          <li><a href="https://arxiv.org/abs/2212.09251"><span style="background-color: pink" class="color-bubble">(Perez et al., 2023)</span> Discovering Language Model Behaviors with Model-Written Evaluations</a><a href="https://x.com/AnthropicAI/status/1604883576218341376" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2403.13793"><span style="background-color: pink" class="color-bubble">(Phuong et al., 2024)</span> Evaluating Frontier Models for Dangerous Capabilities</a><a href="https://x.com/tshevl/status/1770744344669990981" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://metr.org/blog/2023-09-26-rsp/"><span style="background-color: pink" class="color-bubble">(METR, 2023)</span> Responsible Scaling Policies (RSPs)</a>, example: <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Anthropic's RSP</a><a href="https://x.com/AnthropicAI/status/1792598295388279124" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li>Resources:
          <ul>
            <li> <a href="https://metr.github.io/autonomy-evals-guide/">METR's Autonomy Evaluation Resources</a><a href="https://x.com/METR_Evals/status/1768684026792190410" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
            <li><a href="https://github.com/UKGovernmentBEIS/inspect_ai">UK AISI "Inspect"</a><a href="https://x.com/soundboy/status/1788910977003504010" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          </ul>
        </ul>
      </ul>
      <ul>
       <li class="expandable" data-toggle="interpretability"><span style="background-color: moccasin;" class="color-bubble"><b>Interpretability</b></span></li>
        <ul>
              <p>If we do not understand how AI models arrive at their outputs, we cannot robustly monitor or modify them. We can ask models to describe their reasoning, but models may be synchophantic or deceptive, especially as they become more capable. One approach is to understand model processes just by examining their weights -- though the major challenges with this approach are superposition and scaling.</p>
          <li><a href="https://distill.pub/2020/circuits/zoom-in/"><span style="background-color: moccasin" class="color-bubble">(Olah et al., 2020)</span> Zoom In: An Introduction to Circuits</a></li>
          <li><a href="https://transformer-circuits.pub/2022/toy_model/index.html"><span style="background-color: moccasin" class="color-bubble"> (Elhage et al., 2022)</span> Toy models of superposition</a><a href="https://x.com/AnthropicAI/status/1570087876053942272" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"><span style="background-color: moccasin" class="color-bubble">(Bricken et al., 2023)</span> Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a><a href="https://x.com/ch402/status/1709998674087227859" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><span style="background-color: moccasin" class="color-bubble">(Templeton et al., 2024)</span> Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a><a href="https://x.com/AnthropicAI/status/1792935506587656625" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2310.01405"><span style="background-color: moccasin" class="color-bubble">(Zou et al., 2023)</span> Representation Engineering: A Top-Down Approach to AI Transparency</a><a href="https://x.com/andyzou_jiaming/status/1684766170766004224" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
        </ul>
      </ul>

      <ul>
        <li class="expandable" data-toggle="robustness"><span style="background-color: rgb(192,238,192)" class="color-bubble"><b>Robustness and Generalization</b></span></li>
        <ul>
              <p>Today and in the future, AI needs to be robust to adversarial attacks and generalize well with incomplete data on human preferences. As models become more capable, ensuring models will represent human intentions across distributional shift is even more important... since humans will be less economically incentivized and less capable of monitoring the processes generating AI outputs, and models will become better at deception.</p>
          <li><a href="https://arxiv.org/abs/2401.05566"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Hubinger et al., 2024)</span> Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a><a href="https://x.com/AnthropicAI/status/1745854907968880970" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="http://arxiv.org/abs/2307.15043"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Zou et al., 2023)</span> Universal and Transferable Adversarial Attacks on Aligned Language Models</a><a href="https://x.com/andyzou_jiaming/status/1684766189443227648" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2306.15447"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Carlini et al., 2023)</span> Are aligned neural networks adversarially aligned?</a></li>
        </ul>
      </ul>

<!--       <ul>
      <li class="expandable" data-toggle="reward_misspecification"><span style="background-color: rgb(192,238,192)" class="color-bubble"><b>Reward Misspecification and Goal Misgeneralization</b></span></li>
        <ul>
          <p>How do we ensure that AI systems represent the goals we intend them to at deployment and under distributional shift, when we cannot maximally specify our preferences during training and many goals are compatible with the training data? Proxy reward signals generally correlate with designers' true objectives, but AI systems can break this correlation when they strongly optimize towards reward objectives.</p>
          <li><a href="https://arxiv.org/abs/2210.01790"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Shah et al., 2022)</span> Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals</a></li>
          <li><a href="https://arxiv.org/abs/2201.03544"><span style="background-color: rgb(192,238,192)" class="color-bubble">(Pan, Bhatia and Steinhardt, 2022)</span> The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models</a></li>
        </ul>
      </ul> -->
      <!-- paleturquoise -->
      <ul>
      <li class="expandable" data-toggle="scalable_oversight"><span style="background-color: lavender" class="color-bubble"><b>Scalable Oversight</b></span></li>
        <ul>
          <p>How do we supervise systems that are more capable than human overseers? Perhaps we can use aligned AI overseers to oversee more capable AI, but the core problems still remain.</p>
          <li><a href="https://arxiv.org/abs/2211.03540"><span style="background-color: lavender" class="color-bubble"> (Bowman et al., 2022)</span> Measuring Progress on Scalable Oversight for Large Language Models</a><a href="https://x.com/AnthropicAI/status/1590019597109202946" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2212.08073"><span style="background-color: lavender" class="color-bubble">(Bai et al., 2022)</span> Constitutional AI: Harmlessness from AI Feedback</a><a href="https://x.com/AnthropicAI/status/1603791161419698181" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
          <li><a href="https://arxiv.org/abs/2312.09390"><span style="background-color: lavender" class="color-bubble">(Burns et al., 2023)</span> Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision</a><a href="https://x.com/CollinBurns4/status/1735350133926314431" class="no-underline twitter-link twitter-link-small">{% include twitter_icon.html %}</a></li>
        </ul>
      </ul>
      <br>


      <!-- background-color: pink -->
      <!-- background-color: peachpuff -->
      <!-- background-color: moccasin -->
      <!-- background-color: rgb(192,238,192) -->
      <!-- background-color: lavender -->

      <h4 class="expandable" data-toggle="other_ai_safety_content">Talks, blog posts, newsletters, and upskilling</h4>
      <ul>

      <h4 class="expandable" id="highlighted_talk_series">Highlighted talk series</h4>
      <ul>
        <li><a href="https://www.alignment-workshop.com/nola-2023"><b>New Orleans Alignment Workshop (Dec 2023)</b></a>, <i>recordings available</i></li>
        <li><a href="https://pli.princeton.edu/events/princeton-ai-alignment-and-safety-seminar">Princeton AI Alignment and Safety Seminar</a></li>
        <br>
      </ul>

      <h4 class="expandable" id="content">Blog posts</h4>
        <ul>
          <li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
          <li><a href="https://wp.nyu.edu/arg/why-ai-safety/">Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022)</li>
          <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">More is Different for AI</a> by Jacob Steinhardt (2022)</li>
          <!--<li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0">Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)</li>-->
          <li><a href="https://theaidigest.org/">AI Digest: Visual explainers on AI progress and its risks</a></li>
          <li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper (ongoing)</li>
          <!-- <li class="expandable" data-toggle="general"><i>For a general audience</i></li>
          <ul>
            <li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper</li>
            <li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)</li>
            <li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)</li>
            <li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
            <li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
            <li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)</li>
            <li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcasts and articles)</li>
            <li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
          </ul> -->
      </ul>

        <h4 class="expandable" id="newsletters">Newsletters</h4>
        <ul>
          <li><a href="https://importai.substack.com">Import AI</a> by Jack Clark, <i>for keeping up to date on AI progress</i></li>
          <li><a href="https://newsletter.mlsafety.org">ML Safety Newsletter</a> and <a href="https://newsletter.safe.ai/">AI Safety Newsletter</a> by the Center for AI Safety, <i>for AI safety papers and policy updates</i></li>
          <li><a href="https://www.transformernews.ai/">Transformer</a> by Shakeel Hashim, <i>for AI news with a safety focus</i></li>
          <li><a href="https://newsletter.danielpaleka.com/">AI Safety Takes</a> by Daniel Paleka, <i>for a PhD student's overview</i></a></li>
          <br>
        </ul>

      <h4 class="expandable" id="guides">Upskilling</h4>
      <ul>
        <li class="expandable" data-toggle="guides">Guides</li>
        <ul>
          <li> <b>Research</b>: <a href="https://aisafetyfundamentals.com/blog/alignment-careers-guide">Alignment Careers Guide</a></li>
          <li class="expandable expanded"><b>Engineering:</b></li>
          <ul>
            <li><a href="https://arena-roadmap.streamlit.app/">ARENA research engineering upskilling curriculum</a></li>
            <li><a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Leveling Up in AI Safety Research Engineering</a></li>
          </ul>
        </ul>
        <li class="expandable" data-toggle="courses">Courses</li>
        <ul>
          <li><a href="https://www.aisafetyfundamentals.com/ai-alignment-curriculum">AI Safety Fundamentals Curriculum</a> by BlueDot Impact </li>
          <li><a href="https://course.mlsafety.org/">Introduction to ML Safety</a> by the Center for AI Safety</li>
        </ul>
        <li class="expandable" data-toggle="tools">Tools</li>
        <ul>
          <li><a href="https://www.neuronpedia.org/">Neuronpedia</a>, a tool for visualising SAEs</li>
        </ul>
      </ul>
      </ul>

      </ul>
    </ul>
    <h4 class="expandable" id="technical_governance"><b>Technical AI Governance</b></h4>
		<ul>
			<li>Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>
			<li><a href="https://docs.google.com/document/d/1KNkIrp1nr6ETXAFpu5k8wYz51sJlDyDDnTXtWwDcKBw/edit?ref=blog.heim.xyz">Blog post (Anonymous): "AI Governance Needs Technical Work"</a></li>
      <li><a href="https://blog.heim.xyz/technical-ai-governance/">Blog post by Lennart Heim: "Technical AI Governance"</a> (focuses on compute governance), <a href="https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/">Podcast: "Lennart Heim on the compute governance era and what has to come after"</a></li>
      <li><a href="https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/">Blog post by Luke Muehlhauser, "12 Tentative Ideas for US AI Policy"</a></li>
      <li><a href="https://80000hours.org/career-reviews/become-an-expert-in-ai-hardware/">Perspectives on options given AI hardware expertise</a> (80,000 Hours)</li>
      <li><i>Arkose is looking for further resources about technical governance, as this is a narrow set; please send recommendations to team@arkose.org!</i></li>
			<!-- Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>-->
		</ul>

    </div>
  </section>


<!--     <a href="https://airtable.com/appOQF4xHFCwscK39/shrQzex73hN01B1CR" class="button button-white button-small button-right">Suggest A Resource</a> -->

      <!--     <h3 style="clear:both"> Expanded List of Papers</h3>
    
    <a href="papers" class="button button-white fit">AI Safety Papers</a> -->










  <section class="bg-gray" id="find-more-ai-safety-paper">
    <div class="inner">
      <h2>Find more AI safety papers</h2>
      <div class="inner">
<!--         <h3> Find AI safety publications relevant to your area of expertise.</h3>
 -->        <p>Curated with input from the experts on our <a href="/#panel">Strategic Advisory Panel</a>, this collection of papers, blog posts, and videos contains top and recent AI safety papers, categorized by AI safety and machine learning (ML) subfields.</p>
      </div>
      <div id="paper-list">
        <div class="row mt1 mb1">
          <div class="12u">
            <input type="search" class="search form-control" placeholder="Search" />
          </div>
        </div>
        <div class="row">
          {% include paper_filter.html field_id="safety_category" field_display="Safety Category" options=empty %}
          {% include paper_filter.html field_id="ml_subfield" field_display="ML Subfield" options=empty %}
          <!-- {% include paper_filter.html field_id="ml_subtopic" field_display="ML Subtopic" options=empty %}
          {% include paper_filter.html field_id="safety_topic" field_display="Safety Topic" options=empty %} -->
          {% include paper_filter.html field_id="type" field_display="Type" options=empty %}
          <div class="12u filter-list"></div>
        </div>
        <div class="results-count">
          <span class="results-count-text"><span class="results-count-number">0</span> results</span>
        </div>
        <ul class="list"></ul>
      </div>
    </div>
  </section>
</div>

<div class="hidden">
  <li id="paper-item" class="paper-item">
    <div class="title-container">
      <img class="type-icon image" />
      <h3 class="Title"></h3>
    </div>
    <div class="detail-opener">
      <svg aria-label="open" aria-hidden="true" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="#666" stroke-width="2">
        <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
      </svg>
    </div>
    <div class="detail">
      <div class="ML SubfieldDisplay"></div>
      <div class="ML SubtopicDisplay"></div>
      <div class="Safety CategoryDisplay"></div>
      <div class="Safety TopicDisplay"></div>
      <div class="TypeDisplay"></div>
      <div class="TwitterDisplay"></div>
      <div class="Blog or VideoDisplay"></div>
      <div class="Supplementary MaterialDisplay"></div>
      <div class="Transcripts / Audio / SlidesDisplay"></div>
      <div class="AbstractDisplay"></div>
    </div>
  </li>
</div>

<script>
  $(function() {
    var papersList;
    const filters = {
      "Safety Category": new Set(),
      "Safety Topic": new Set(),
      "ML Subfield": new Set(),
      "ML Subtopic": new Set(),
      "Type": new Set(),
    }
    const filterOptions = {
      "Safety Category": new Set(),
      "Safety Topic": new Set(),
      "ML Subfield": new Set(),
      "ML Subtopic": new Set(),
      "Type": new Set(),
    }

    const options = {
      valueNames: [
        'Title',
        'Safety TopicDisplay',
        'Safety Category',
        'Safety CategoryDisplay',
        'Type',
        'TypeDisplay',
        'Twitter',
        'TwitterDisplay',
        "Blog or Video",
        "Blog or VideoDisplay",
        "Supplementary Material",
        "Supplementary MaterialDisplay",
        "Transcripts / Audio / Slides",
        "Transcripts / Audio / SlidesDisplay",
        "ML Subfield",
        "ML SubfieldDisplay",
        "ML Subtopic",
        "ML SubtopicDisplay",
        "Abstract",
        "AbstractDisplay",
        { attr: 'src', name: 'image' }
      ],
      item: 'paper-item'
    };

    let papers = [];

    function isDarkMode() {
      if (window.matchMedia &&
        window.matchMedia('(prefers-color-scheme: dark)').matches)
        return true;

      return document.documentElement.getAttribute('data-theme') === 'dark';
    }

    function listify(papersJson) {
      papers = papersJson.filter(paper => paper.Title && paper.Title.length > 0 && paper['ML Subfield'] && paper['Type'])
                        .map(function(paper) {
        const modeStr = isDarkMode() ? '-dark' : ''

        if (!paper.Type) {
          paper.image = `/assets/images/medium/other${modeStr}.svg`;
        } else if (paper.Type.length > 1 && paper.Type.includes("Paper")) {
          paper.image = `/assets/images/medium/paper${modeStr}.svg`;
        } else {
          paper.image = `/assets/images/medium/${paper.Type[0].toLowerCase()}${modeStr}.svg`;
        }
        paper['Safety CategoryDisplay'] = paper['Safety Category'].map(item => `<span class="field-single safety-category-single">${item}</span>`).join('');
        paper['Safety Topic'] = paper['Safety Topic'] ?? [];
        paper['Safety TopicDisplay'] = paper['Safety Topic'].map(item => `<span class="field-single safety-topic-single">${item}</span>`).join('');
        paper['ML SubfieldDisplay'] = paper['ML Subfield'].map(tag => `<span class="field-single ml-subfield-single">${tag}</span>`).join('');
        paper['ML Subtopic'] = paper['ML Subtopic'] ?? [];
        paper['ML SubtopicDisplay'] = paper['ML Subtopic'].map(tag => `<span class="field-single ml-subtopic-single">${tag}</span>`).join('');
        paper['TypeDisplay'] = paper['Type'].map(tag => `<span class="field-single type-single">${tag}</span>`).join('');
        paper.Title = linkify(paper.Link, paper.Title.replace("\n", ''));
        paper['Transcripts / Audio / SlidesDisplay'] = linkify(paper['Transcripts / Audio / Slides'], 'Transcripts / Audio / Slides');
        paper['Supplementary MaterialDisplay'] = linkify(paper['Supplementary Material'], 'Supplementary Material');
        paper['TwitterDisplay'] = linkify(paper['Twitter'], '{% include twitter_icon.html %}', 'no-underline twitter-link');
        paper['AbstractDisplay'] = paper.Abstract ? `<h4>Abstract</h4>${paper.Abstract}` : '';
        return paper;
      });

      papersList = new List('paper-list', options, papers);

      $(document).on('change', '.filter-dropdown input', function(e) {
        var filter = this.value;
        var filterType = $(this).closest('.filter-dropdown').data('filter-type');
        if (filters[filterType].has(filter)) {
          filters[filterType].delete(filter);
        } else {
          filters[filterType].add(filter);
        }
        updateList();
      });

      // Get options for each filter from the JSON
      Object.keys(filterOptions).forEach(function(filterType) {
        papers.forEach(function(paper) {
          paper[filterType].forEach(function(filter) {
            filterOptions[filterType].add(filter);
          });
        });
      });

      // Replace the filter options of each ".filter-dropdown ul" with the ones from the JSON
      Object.keys(filterOptions).forEach(function(filterType) {
        var filterDropdown = $('.filter-dropdown-' + slugify(filterType) + ' ul');
        Array.from(filterOptions[filterType]).sort().forEach(function(option) {
          filterDropdown.append(`<li data-option="${option}">
            <input type="checkbox" id="option-${slugify(filterType)}-${option}" name="option-${slugify(filterType)}-${option}" value="${option}">
            <label for="option-${slugify(filterType)}-${option}">
              ${option}
              <span class="filter-option-count"></span>
            </label>
          </li>`);
        });
      });
      
      // On page load, read the query params and set the filters
      var params = new URLSearchParams(window.location.search);
      if (params.size === 0) {
        // Formerly, we'd default the filter Type to Paper if no filters were set. That code follows, but we've
        // commented it out for now.
        // filters['Type'].add('Paper');
        // $('input[name="option-type-Paper"]').prop('checked', true);
      } else {
        Object.keys(filters).forEach(function(filterType) {
          if (!params.has(filterType)) return

          params.getAll(filterType)[0].split(',').forEach(function(filter) {
            if (filter === '') return; // Type param can have an empty string to indicate it should *not* apply the default Type=paper

            filters[filterType].add(filter);
            $('input[name="option-' + slugify(filterType) + '-' + filter + '"]').prop('checked', true);
          });
        });
        document.getElementById('find-more-ai-safety-paper').scrollIntoView({ behavior: 'smooth' });
      }
      updateList();
      updateResultsCount();

      $('input.search').on('change', function(e) {
        updateFilterOptionCounts();
      });

      papersList.on('updated', function (list) {
        updateResultsCount();
      });
    }

    fetch('/export/papers.json')
      .then(response => response.json())
      .then(data => {
        listify(data)
      })
      .catch(error => console.error('Error loading JSON:', error));

    function resetList(){
      papersList.search();
      papersList.filter();
      papersList.update();
      $(".filter-all").prop('checked', true);
      $('.filter').prop('checked', false);
      $('.search').val('');
    };

    function updateFilters() {
      papersList.filter(function (paper) {
        return Object.keys(filters).every(function(filterType) {
          if (filters[filterType].size == 0) return true

          return paper.values()[filterType].some(item => filters[filterType].has(item))
        });
      });
    }

    function updateResultsCount(){
      $('.results-count-number').text(papersList.matchingItems.length);
    }

    function updateShownFilters(){
      var filterList = $('.filter-list');
      filterList.empty();
      Object.keys(filters).forEach(function(filterType) {
        filters[filterType].forEach(function(filter) {
          filterList.append(`<span class="filter-item filter-item-${slugify(filterType)}" data-filter-type="${filterType}">` + filter + '</span>');
        });
      });
    }

    function updateFilterTitleCount(){
      Object.keys(filters).forEach(function(filterType) {
        const filterSlug = slugify(filterType)
        if (filters[filterType].size) {
          $('.filter-' + filterSlug + ' .dropdown-title').addClass('filter-active');
          // $('.filter-' + filterSlug + ' .dropdown-title .filter-type-count').text('(' + filters[filterType].size + ')');
        } else {
          $('.filter-' + filterSlug + ' .dropdown-title').removeClass('filter-active');
          // $('.filter-' + filterSlug + ' .dropdown-title .filter-type-count').text('');
        }
      });
    }

    function updateQueryParams(){
      var params = new URLSearchParams(window.location.search);
      Object.keys(filters).forEach(function(filterType) {
        params.delete(filterType);
        if (filters[filterType].size)
          params.append(filterType, Array.from(filters[filterType]));
      });
      var newRelativePathQuery = window.location.pathname + (params.toString() ? '?' : '') + params.toString();
      if (newRelativePathQuery != window.location.pathname + window.location.search)
        history.pushState(null, '', newRelativePathQuery);
    }

    function titlesMatch(title1, title2) {
      return title1.match(/<a.*>(.*)<\/a>/)?.[1] == title2.match(/<a.*>(.*)<\/a>/)?.[1]
    }

    function updateFilterOptionCounts() {
      Object.keys(filterOptions).forEach(function(optionFilterType) {
        filterOptions[optionFilterType].forEach(function(option) {
          const filterOptionCount = $(`.filter-${slugify(optionFilterType)} li[data-option="${option}"] .filter-option-count`);

          // If the filter is already set, just remove the count
          if (filters[optionFilterType].has(option)) {
            filterOptionCount.text('');
            return;
          }

          // When there are no options set for a filter, adding an option *reduces* options, which is easy to calculate and is the
          // first option here.
          if (filters[optionFilterType].size == 0) {
            const remainingItems = papersList.matchingItems.filter(item => { return item.values()[optionFilterType].includes(option) })
            filterOptionCount.text('(' + remainingItems.length + ')');
          } else {
            // When there are options set for a filter, adding an option *increases* options, which we calculate here
            const unmatchedItems = papers.filter(paper => !papersList.matchingItems.some(item => titlesMatch(paper.Title, item.values().Title)))
            const newFilteredItems = unmatchedItems.filter(item => { return item[optionFilterType].includes(option) })

            // now apply the other filters
            const remainingFilteredItems = newFilteredItems.filter(item => {
              return Object.keys(filters).every(function(filterType) {
                if (filters[filterType].size == 0) return true
                if (filterType == optionFilterType) return true

                return item[filterType].some(item => filters[filterType].has(item))
              });
            });

            // now apply the search
            const searchTerm = $('input.search').val();
            // iterate through every field on each item and see if it matches the search term
            const remainingItems = remainingFilteredItems.filter(item => {
              return Object.keys(item).some(field => {
                return item[field].toString().toLowerCase().includes(searchTerm.toLowerCase())
              })
            })

            filterOptionCount.text(`(${remainingItems.length})`);
          }
        });
      });
    }

    function updateList(){
      updateFilters();
      papersList.update();
      updateFilterOptionCounts();
      updateShownFilters();
      updateFilterTitleCount();
      updateQueryParams();
    }

    function linkify(s, linkText, cssClass = '') {
      if (!s) return '';
      return `<a href="${s}" target="_top" class="${cssClass}">${linkText || s}</a>`
    }

    function slugify(s) {
      return s.toLowerCase().replace(' ', '_').replace(/\s/g, '_').replace(/[^a-z0-9-_]/g, '');
    }

    // Clicks on the dropdown title show/hide the dropdown
    $(".dropdown-title-box").on("click", function(e) {
      e.stopPropagation();
      var target = $(this).data("target");
      if ($("#" + target).hasClass('active')) {
        $('.filter-dropdown').removeClass('active');
      } else {
        $('.filter-dropdown').removeClass('active');
        $("#" + target).addClass("active");
      }
    });

    // Clicks outside a dropdown close it
    $('body').on('click', function(e) {
      if ($(e.target).closest('.filter-dropdown').length) return;

      $('.filter-dropdown').removeClass('active');
    });

    // Clicks on the filter items remove them
    $('body').on('click', '.filter-item', function(e) {
      var filter = $(e.target).closest('.filter-item').text();
      var filterType = $(e.target).closest('.filter-item').data('filter-type');
      $('input[name="option-' + slugify(filterType) + '-' + filter + '"]').prop('checked', false);
      filters[filterType].delete(filter);
      updateList();
    });

    // Clicks on the list items toggle them open/closed
    $('body').on('click', '.paper-item', function(e) {
      if ($(e.target).closest('a').length) return;

      e.stopPropagation();
      $(e.target).closest(".paper-item").toggleClass("active");
      $(e.target).closest(".paper-item").find('.detail').slideToggle(200);
    });
  });
</script>
