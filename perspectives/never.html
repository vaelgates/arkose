---
layout: argument
title: "Never"
breadcrumbs: Generally Capable AI:when-generally-capable-ai,Never:never
---

<blockquote><p>
No, I don't think we'll get there. I think it's too broad. Everything that we see currently in AI that works well is really specific and trained for a specific task, and once you try to even just go multi-task and try to solve like three tasks together, it always goes south. So I think a general solution like that is not going to work. 
<p class='interview-attribution'>&#40;from: <a href='../interviews'>Interviews with AI Researchers</a>&#41;</p></p></blockquote>


<blockquote><p>
On the other hand, the other type of research that I think is also connected to my own research is that we need to get more inspired by the human brain and by animal brains. So when, during evolution, brains have also scaled, right? But they didn't scale uniformly. It was not like we got a mouse brain and just scale it in all different dimensions to get to the human brain. Human brain scaled compared to the mouse brain, but they scaled differently. They scaled in different dimensions differently. So we need to really understand the scaling underlying brains’ evolution, throughout evolution, in order to know how exactly we need to scale these models to get different types of abilities in different environments, different environmental pressures, and so on. So, yeah, I think in a combination of all these approaches, we might get a better chance of getting to this kind of artificial intelligence or artificial general intelligence. 
<p class='interview-attribution'>&#40;from: <a href='../interviews'>Interviews with AI Researchers</a>&#41;</p></p></blockquote>


<blockquote><p>
[Something like a text summarization machine] can learn something outside what it was fed, but I don't still think that [it has] the vision, the potential to see something like the intuition, the intuition that a human has that “this might work”, and “this might not work”. I think it comes with a lot of things, with emotions, with our personal emotions, our personal experience. [We can feed our personal experiences into a computer] but I think there's an emotional sixth sense kind of thing through which CEOs make a lot of decisions that if they have a limited capital, how much to invest on which one. There is a lot of intuition game going on [...] For example, one of its venture is actually on loss, so the robot might not ever try to push any capital on it, but a CEO might have the vision that, "No, I should still go on pushing on that direction" and once it works it will give so much profit that it would be the best option for me. So I don't think that robots can make those kinds of decisions. 
<p class='interview-attribution'>&#40;from: <a href='../interviews'>Interviews with AI Researchers</a>&#41;</p></p></blockquote>


<blockquote><p>
I think humans probably cannot create something which has this cognitive capacity, at least, the same level as we have. So I think we'll have something which is specific for each task, but this general, the cognitive intelligence… I know it will be too hard to be a scientist or a CEO. I think it's never, never going to happen. 
<p class='interview-attribution'>&#40;from: <a href='../interviews'>Interviews with AI Researchers</a>&#41;</p></p></blockquote>

<p>The interview snippets above show arguments for why AGI might never be developed. Below, we will explore possible counterarguments. Subsequent pages will follow a similar structure.</p>
<p>It can be challenging to determine which technologies are possible or impossible, as we often lack concrete evidence. This makes technology forecasting notoriously difficult. However, we believe that AGI will eventually be possible for the following reasons:</p>

<ol><li>There is a long history of people mistakenly believing possible things to be impossible. Consider these examples:</li>
<ol><li>Just two years before the first flight of the Wright Brothers, Wilbur Wright—by his own account—predicted flight was at least fifty years away.</li>
<li>In 1933, Ernest Rutherford declared that the idea of harnessing atomic energy was “moonshine.” Shortly thereafter—the very next day, according to some accounts - Leo Szilard discovered the possibility of a nuclear chain reaction.</li>
<li>In 1939, Enrico Fermi proclaimed that the chain reaction was just a “remote possibility”—four years later, Fermi himself oversaw the world’s first nuclear reactor.</li>
<li>In an informal review, <a href='https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/' target='_blank'>predictions of the “Big Three” science fiction writers</a> of the 20 th century were categorized and evaluated, and only 31% of their predictions turned out to be correct.</li>
</ol><li>Many people did not anticipate the amount of progress that has been made during the deep learning revolution, and progress seems to still be accelerating.</li>
<ol><li>For example, <a href='https://bounded-regret.ghost.io/ai-forecasting/' target='_blank'>predictions made by experienced forecasters in 2021</a> about AI progress on state-of-the-art accuracy on the MATH and Massive Multilanguage Understanding datasets were wildly inaccurate. <a href='https://bounded-regret.ghost.io/ai-forecasting-one-year-in/' target='_blank'>Actual improvement after one year was much higher than predicted</a>. These forecasts were commissioned by researcher Jacob Steinhardt.</li>
</ol><li>There has been a significant investment of resources - including time, money, and people - in AI. <a href='https://venturebeat.com/ai/report-ai-investments-see-largest-year-over-year-growth-in-20-years/' target='_blank'>This includes a significant increase in investment since 2010 due to international economic interests.</a>.</li>
<li>Evolution produced intelligence in a pretty “dumb” way: it arose as a byproduct of biological agents roaming in a complex environment and competing for fitness advantage.</li>
</ol>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<p><em></em></p>
<p></p>
