---
layout: argument
title: "Introduction"
breadcrumbs: Introduction:introduction
---
<em><p>Explore Your AI Risk Perspectives: An Interactive Walkthrough of Researchers' Most Frequent Interview Responses</em></p>
<p>Welcome to this interactive walkthrough, exploring perspectives about potential risks from advanced AI. The discussions in this walkthrough stem from a series of interviews with 97 AI researchers, wherein the interviewer probed researchers’ thoughts about the future of AI and arguments for potential risk from advanced systems. In this guide, you’ll encounter their most common responses, direct quotes, and a range of arguments and counterarguments. At the bottom of each page, we encourage you to mark your own opinion: do you agree with the claims? Why or why not?</p>
<p>On the final page, you’ll see a visual summary of your opinions, and how they compare to other visitors. If you don’t want your responses recorded, you can still navigate throughout the site by using the top navigation bar.</p>
<p>We hope this will help you explore some of the ongoing discussion on the development and deployment of highly capable AI systems. We welcome your feedback and encourage you to get in touch with us, especially if your own views are not represented here. To read more about the interviews, including analysis and transcripts, see the <a href='../interviews' >Interviews</a>.</p>
<p>…</p>
<em><p>Note:</em> <a href='https://bounded-regret.ghost.io/more-is-different-for-ai/' target='_blank'>Jacob Steinhardt describes two common approaches to thinking about future AI systems</a> : the “Engineering” approach and the “Philosophy” approach. The Engineering approach “tends to be empirically-driven, drawing experience from existing or past ML systems”, while the Philosophy approach “tends to think more about the limit of very advanced systems”. He says that “in my experience, people who strongly subscribe to the Engineering worldview tend to think of Philosophy as fundamentally confused and ungrounded, while those who strongly subscribe to Philosophy think of most Engineering work as misguided and orthogonal (at best) to the long-term safety of ML.” However, he concludes that “Neither of these approaches is satisfying and we actually have no single good approach to thinking about risks from future ML systems,” and suggests ML researchers should lean on both when thinking about the future. In this walkthrough, we encourage readers to engage with both approaches, and we try to supplement thought experiments with real-world examples whenever possible.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
