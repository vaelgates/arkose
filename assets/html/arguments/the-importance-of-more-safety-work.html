<div class="page-data" data-page-title="The importance of more safety work"></div><ul><br/><li>At the moment, very few people work on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (<br/>see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety is grossly neglected. There is no reason to expect the safety community to suddenly grow by itself.<br/></li>
<li>While AI might come in 100 years or later, there is some chance it will come much sooner. There is some chance we will be surprised and suddenly faced with potentially disastrous unaligned AGI.</li>
<li>Even if it takes 100-200 years, it is not at all clear that alignment will be good enough by then.</li>
</ul><p>Further Reading:</p>

<a href='https://arxiv.org/abs/2209.00626' target='_blank'>“The alignment problem from a deep learning perspective”</a> by Richard Ngo et al
<a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>“Prosaic AI Alignment”</a> by AI safety researcher Paul Cristiano 
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
