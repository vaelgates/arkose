<div class="page-data" data-page-title="The Alignment Problem"></div><ul><li>Current ML systems can fail in surprising and unexpected ways. <a href='https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml' target='_blank'>Many examples have been collected</a></li>
<li>Suppose we have a future “CEO AI” that’s running a company. Their board of directors might give the AI the goal: “Maximize profits without exploiting people, don’t run out of money, and avoid side effects that people would consider objectionable.”</li>
<li>Currently we find it very challenging to translate these human values, preferences and intentions into mathematical formulations that can be optimized by systems. This might continue to be a problem in the future.</li>
<li>Mistakes in goal formulation could be dangerous and have far-reaching consequences. For example, the AI might optimize for what looks good, instead of what actually fulfills human values. Or it might actively cause harm to humans, if that helps it achieve its goals.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
