<div class="page-data" data-page-title="Nothing we can do at the moment"></div><div>Argument: “Future AGI will use techniques we don’t know about yet. Therefore, it is useless to try and work on safety now.”</div>
<ul><li>Some AI safety researchers consider it entirely plausible that AGI will be developed mostly with present-day techniques, and rather soon. In that case, there would be no question that safety techniques must be researched now.</li>
<li>While future AGI will use future techniques, it is possible these will have some relation to the AI systems currently in use. Therefore, if we develop alignment strategies that work on current systems, we could lay valuable groundwork for the alignment of future systems.</li>
<li>At the very least, some work in AI alignment helps us understand current systems better and thus use them more responsibly.</li>
<li>There is a reasonable chance, according to some AI safety researchers, that future AGI will actually be built mostly upon present-day AI techniques, without requiring much new insight at all. This is also called “prosaic AGI”.</li>
<ul><li>If this is the case, then we can probably get started working on alignment right now using the techniques we do have (even if they do not constitute “general” intelligence yet)</li>
</ul><li>Much present-day work on AI safety is actually only weakly dependent on the details and new innovations that would be gained from working towards AGI. (See below for some examples of work currently being done). Therefore, our ignorance about these details is not a huge problem.</li>
<li>Any progress on alignment, even without knowing how future AGI is built, would be a big step in the right direction. As AI safety researcher [https://ai-alignment.com/prosaic-ai-control-b959644d79c2]}(Paul Cristiano writes): “For now, finding any plausible approach to alignment, that works for any setting of unknown details, would be a big accomplishment. With such an approach in hand we could start to ask how sensitive it is to the unknown details, but it seems premature to be pessimistic before even taking that first step.”</li>
<li>Currently, there are so few people working on AI safety that it seems worthwhile to increase the effort quite a bit - considering the magnitude of the problem. Also, consider how important AI will become over the next decades even before AGI is invented. <br/><li>At the moment, very few people work on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (<br/>see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety is grossly neglected. There is no reason to expect the safety community to suddenly grow by itself.<br/></li>
<li>The field of AI safety research is really broad and covers many different approaches. It’s likely that at least some of them will be quite useful - even in the future with new techniques:</li>
<ul><li></li>
<li>TODO: Explain a couple of approaches that are reasonably similar to present-day AI and easy to explain in a couple of paragraphs. Also make sure to mention a quick win for some of them, something tangible that was found.</li>
<li></li>
</ul><li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordination to avoid catastrophe.</li>
</ul><div>Further Reading:</div>

<a href='https://arxiv.org/abs/2209.00626' target='_blank'>“The alignment problem from a deep learning perspective”</a> by Richard Ngo et al
<a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>“Prosaic AI Alignment”</a> by AI safety researcher Paul Cristiano 
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
