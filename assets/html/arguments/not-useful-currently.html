<div class="page-data" data-page-title="Nothing we can do at the moment"></div><p>Some argue that future AGI will use techniques we don’t know about yet. Therefore, it would be useless to try and work on safety now.</p>
<ul><li>Some AI safety researchers consider it plausible that AGI will be developed mostly using present-day techniques, and that it will happen soon. In that case, there seems to be a very strong case that safety techniques should be researched now, building upon the capabilities we do have (even if they are not constitute “general” intelligence yet).</li>
<li>While future AGI will use future techniques, it is possible these will have some relation to AI systems currently in use. Therefore, if we develop alignment strategies that work on current systems, we could lay valuable groundwork for the alignment of future systems.</li>
<li>Furthermore, work in AI alignment helps us understand current systems better and thus use them more responsibly and effectively.</li>
<li>Much present-day work on AI safety is actually only weakly dependent on the technical details of the architecture of AGI. (See below for some examples of work currently being done). Therefore, our ignorance about these details is not a huge problem.</li>
<ul><li>Any progress on alignment, even without knowing how future AGI is built, would be a big step in the right direction. As AI safety researcher [https://ai-alignment.com/prosaic-ai-control-b959644d79c2]}(Paul Cristiano writes): “For now, finding any plausible approach to alignment, that works for any setting of unknown details, would be a big accomplishment. With such an approach in hand we could start to ask how sensitive it is to the unknown details, but it seems premature to be pessimistic before even taking that first step.”</li>
</ul><li>Currently, there are so few people working on AI safety that it seems worthwhile to increase the effort quite a bit , considering the magnitude of the problem. Also, consider how important AI will become over the next decades even before AGI is invented. <br/><li>At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
<li>The field of AI safety research is really broad and covers many different approaches. It’s likely that at least some of them will be quite useful - even in the future with new techniques:</li>
<ul><li></li>
<li>(We are still working on this paragraph)</li>
<li></li>
</ul><li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordinating around not creating AGI at all to avoid catastrophe.</li>
</ul><p>Further Reading:</p>

<a href='https://arxiv.org/abs/2209.00626' target='_blank'>“The alignment problem from a deep learning perspective”</a> by Richard Ngo et al
<a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>“Prosaic AI Alignment”</a> by AI safety researcher Paul Cristiano 
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
