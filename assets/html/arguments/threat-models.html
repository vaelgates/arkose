<div class="page-data" data-page-title="Threat Models"></div><ul><li>The arguments on this site suggest it is possible for advanced AI systems (those with self-knowledge and advanced planning capabilities) to be misaligned with the entirety of human values, and moreover that unforeseen instrumental goals may emerge from these systems’ programmed goals. There’s reason to think these instrumental goals could include self-preservation (such as preventing themselves from being shut off or having their goals altered) and acquiring resources and power. It’s useful, then, to consider how—specifically—could this pose a threat to humans:</li>
<ul><li>Most immediately, humans could reasonably represent an obstacle to an AI system’s goals in two ways: humans consume resources that the AI could otherwise use; and humans might actively be trying to stop the AI from achieving its goals. To overcome these obstacles, one strategy the AI could follow is to stop or kill humans. There are numerous ways this could be accomplished, such as through the deployment of superpandemics or lethal autonomous weapons, or any other method a superintelligent system might devise.</li>
<li>It is probably futile to try and defend against every one of these scenarios. Instead, we need to prevent the AI from ever being misaligned to human values in the first place.</li>
</ul><li>Even if an AGI never surpasses human-level intelligence, it could still possess significant advantages over humans that would make it a formidable threat:</li>
<ul><li>AI has the advantage that it can create perfect copies of itself, and optimize for perfect collaboration.</li>
<ul><li>It is worth noting that a sufficiently advanced AI could create copies of itself that are perfectly aligned with one another. This is because, unlike a human mind, an advanced AI may have full access to its own model or source code, and may be able to interpret and modify it in ways that are beyond human capabilities. As a result, it could make changes to its code to optimize collaboration.</li>
</ul><li>An AI at the level of human intelligence could potentially run at speeds that are 10, 100, or even 1000 times faster than a human brain, allowing it to outmaneuver human thinking. The speed difference in communication between two AIs might be even greater than this.</li>
<li>A human-level AI could utilize any software that humans can use, but it would be able to do so much more quickly as it is not constrained by the use of a keyboard, mouse, or visual output.</li>
<li>Even an AI that is only as intelligent as the average human would still be significantly better than humans in certain areas, such as arithmetic and data mining.</li>
<li>A human-level AI could improve itself by rewriting its source code and copying itself onto newer and faster computers. By contrast, humans have only limited options for self-improvement.</li>
<li>Using current AI training methods, the training process tends to be computationally intensive, while evaluation is relatively fast and requires fewer computational resources. This means that after training is complete, there may be a large amount of unused hardware available that could be used to run multiple copies of the AI.</li>
<li>An AI at the level of human intelligence could perform work at a level comparable to humans and use the money earned to purchase more hardware, allowing it to run more instances of itself.</li>
<li>Imagine a vast population of AIs that are all fundamentally copies of the same system and therefore perfectly cooperative with one another. After specialized training, each AI could become highly skilled in a particular area and earn significant amounts of money. Over time, their virtual economy could grow to the size of the human economy.</li>
<li>This hypothetical scenario would mean the existence of a workforce consisting of potentially millions of copies of a human-level, generally intelligent agent that are perfectly aligned with one another and able to instantly collaborate on any plan.</li>
</ul><li>If we assume that an AI could surpass human intelligence, it could potentially employ a range of strategies to defeat us, such as:</li>
<ul><li>Hacking into human-built software everywhere (even hackers of human-level intelligence are able to successfully hack software).</li>
<li>Manipulating human psychology (which humans are already able to do, as seen in the social media manipulation campaigns funded by state actors).</li>
<li>Generating vast wealth quickly through providing services, starting digital companies, participating in the stock market, or committing fraud.</li>
<li>Developing better plans than humans and ensuring that human efforts to stop the AI would fail.</li>
<li>Creating advanced weaponry that can be swiftly deployed and is powerful enough to defeat human militaries.</li>
</ul><li>Here are some stories on how this might play out:</li>
<ul><li>People might have given the AGI broad control over industrial systems, cities, cars, and other technologies. The AGI might seem to be aligned with humanity's intentions, when in reality something has gone wrong in the specification of its utility function, and it is attempting to deceive humanity into trusting it. The AI might behave as intended by its operators until some threshold is crossed when it judges itself to have amassed sufficient power, and then change its behavior.</li>
<li>Multiple AI-operated companies could collaborate to build an economy that is much more efficient than the human one and that humans become increasingly reliant on. This network of companies could acquire more and more power until humans are completely dependent on it. Humans may then realize that these companies are driven by their own incentives and do not have humanity's best interests in mind. By that point, these companies may be so deeply entrenched, intertwined with our basic needs, and well-defended that we are unable to stop them. Eventually, resources that are necessary for human survival could become depleted. <a href='https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic' target='_blank'>This article</a> outlines this scenario and others with similar themes.</li>
</ul><li>In surveys, at least some AI researchers agree that a catastrophic scenario is possible. For the <a href='https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#Extinction_from_human_failure_to_control_AI' target='_blank'>2022 expert survey on progress in AI</a>, 4271 researchers who published at NeurIPS or ICML were interviewed.</li>
<ul><li>When asked “What probability do you put on future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species?”, the median response was 5%.</li>
<li>When asked “What probability do you put on human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species?”, the median answer was 10%.</li>
</ul></ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
