<div class="page-data" data-page-title="Alignment will progress automatically"></div><p>Some researchers argue that alignment advances will proceed at the same pace as capabilities advances. If “alignment” is trying to have an AI system be aligned with human goals, then isn’t that also the point of “capabilities”, or advancing AI in general? Would they not proceed apace?</p>
<ul><li>This is definitely a possibility. Maybe capabilities would be directly tied to alignment in some way. Maybe the development of advanced AI systems will go just fine.</li>
<li>However, there is the distinct possibility that things will not be fine. One reason for that is instrumental incentives: An advanced AI system, regardless of its goals, will probably have certain incentives relating to its own capabilities. It might, for example, employ strategies to avoid it from being switched off, and try to get access to more compute. We will cover this possibility in a later chapter.</li>
</ul>
<blockquote>
I'm actually far less worried about the technical side of this. I just finished reading this book about von Neumann, that's a little cute biography of him, and there's a part where he says, supposedly, that people who think mathematics is complicated only say that because they don't know how complicated life is. And I'm totally messing with the phrasing, but something like that. I actually think any technical problems in this area will be solved relatively easily compared to the problem of figuring out what human values we want to insert into these. 
</blockquote>

<ul><li>Due to economic incentives, new technologies  are often deployed without adequate confirmation of safety, especially if people don’t think it needs to be carefully checked.</li>
<li>Before the first test of the atomic bomb, there were some concerns that an energy release of this magnitude might cause a chain reaction within the atmosphere - turning the surface of earth into a barren wasteland. Further calculations were done and showed that this is not possible. However, the calculation was only checked by a small number of researchers and it could have been possible, though unlikely, that there was a mistake in the calculation or the underlying assumptions. <a href='https://blogs.scientificamerican.com/cross-check/bethe-teller-trinity-and-the-end-of-earth/' target='_blank'>Further reading</a></li>
<ul><li>For a later nuclear weapons test (Castle Bravo), the same team actually did make a serious error when calculating the yield, the device ended up being significantly more powerful than expected.</li>
</ul><li>Current AI systems have significant ethical problems, and some of these problems are only discovered in production.</li>
<li>Often, it is not possible to fix all these problems (for example, ChatGPT can be instructed to write toxic material despite efforts at preventing that)</li>
<li>It is possible that an AGI system would behave reasonably well in testing, but produce catastrophic results in the real world.<br/></li>
<li>An AI system can be deceptively aligned, i.e. display safe behavior in the training environment but then do the wrong thing in the testing environment.
<ul><li>An AI employing deception may seem like a far-fetched scenario. However, forms of deception have already been observed even in relatively simple systems: In <a href='https://arxiv.org/abs/1803.03453' target='_blank'>The Surprising Creativity of Digital Evolution</a>, Ofria (see the section “Learning to Play Dumb on the Test”) recounts their experience from a 2001 simulated biology study published in Nature. In this study, evolutionary programming led to organisms being able to detect whether or not they were in the training environment, and radically change their behavior between the “normal” and “test” environments. This is an example of deception arising from training pressures, without even requiring general intelligence.</li>
<li>In <a href='https://arxiv.org/abs/2105.14111' target='_blank'>Goal Misgeneralization in Deep Reinforcement Learning</a> (ICML 2022), Langosco et al. provide the first empirical demonstrations of goal misgeneralization: In a simple toy environment, an agent learns the wrong objective due to slight distributional shifts between training and testing environment. This system is not an AGI with an intention to deceive - nevertheless, the outcome has parallels: The system displays safe behavior in training but pursues the wrong goal in the test environment. A later paper by DeepMind <a href='https://arxiv.org/pdf/2210.01790.pdf' target='_blank'>Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals</a>  provides a more general definition of goal misgeneralization and further examples in new settings.</li>
<li>It is possible that a hypothetical AGI system could form an intention to deceive. Very useful AI systems – anything close to “general intelligence” – will likely have a model of the world that includes itself and its internal processes, and will be able to track changes in its external environment and adjust its internal behavior accordingly. (Noting that in a training environment, actions the AI takes affect this local environment and are probably reversible. In a deployment scenario, actions the AI takes impact the overall structure of the world.) If an AI is pursuing an initially internalized goal, it may observe its goal structure being modified in “training” and “test” environments, and that to have any effect on reality with respect to its current goal, the AI needs to behave satisfactorily within these local environments before it will be deployed. Therefore, the AGI may try to optimize for acceptable-to-overseers behavior in these environments, even if it behaves differently once it is deployed.</li>
<li>Additional resources:</li>
<ul><a href='https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment' target='_blank'><li>How likely is deceptive alignment</a> – Presentation by <a href='https://www.linkedin.com/in/ehubinger/' target='_blank'>Evan Hubinger</a></li>
<li>The ICML paper mentioned above was popularized in a <a href='https://www.youtube.com/watch?v=zkbPdEHEyEI]' target='_blank'>YouTube video by AI researcher Robert Miles</a>.<br/></li></ul></ul></li>
<li>It is unknown whether alignment will be achieved as a natural progression of AI research or not. Some present-day technologies ended up being quite safe inherently, others are risky even if properly managed. Consider, for example, nuclear engineering and biotechnology.</li>
<li>Alignment and Capabilities are not the same. You can have very advanced systems with bad alignment. For example, you could have a GPT-style text generator that seems really smart, but constructs elaborate arguments that seem true at first glance but are false.</li>
<li>Even if it seems likely that alignment will advance alongside capabilities, we don’t have any guarantee about that. Notable experts in the field, such as Eliezer Yudkowsky and Nick Bostrom, claim that the danger from badly aligned systems are alarmingly high.</li>
<li>Safety often lags behind in the deployment of innovative technology. There is a long history of dangerous technology being deployed prematurely, before the technology was understood well enough to be safe. Some examples:
<ul><li>Radioactive toothpaste being sold in Germany in the 1920s</li>
<li>Early passenger aircraft (like the de Havilland Comet ) had rectangular windows, before it was understood that rectangular features put too much stress on the airframe, leading to several disasters.</li>
<li>The Therac-25 radiation treatment machine which killed several people due to a programming error.</li>
<li>The dangerous reactor design which contributed to the Chernobyl disaster.<br/></li></ul></li>
</ul><p>Further Reading:</p>
<ul><li>OpenAI Cofounder John Schulman on <a href='https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment' target='_blank'>why alignment might not get solved automatically as models get smarter</a></li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
