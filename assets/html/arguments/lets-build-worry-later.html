<div class="page-data" data-page-title="Build AI now, worry later"></div>
<blockquote>
 To be honest, I haven't thought about the safety part of AI, and I never, almost never saw… that's why I have no motivation to be honest, because like, I don't know. I'm feeling like, let's build AI then we will care about the safety part. I'm thinking in that way– that's not the best way of thinking, I know, and you are much more aware than me about the future– but yeah, to be honest, I'm not that interested in working on that. 
</blockquote>


<blockquote>
Interviewee: Yeah, again it's so speculative, but it's hard to imagine. In a lot of ways the best case scenario is the one where it eventually levels out well below the level of human intelligence. Probably the best case scenario.
<p>VG: Wow, okay, cool. Do you think we're pretty much doomed? Because you said the phrasing of a risk seems weird, because does that mean that as soon as we get AI that's more intelligent than humans, it's just done? There's no hope left?</p>
<p>Interviewee: Yeah, I mean, I think historically power causes people and things to destroy things that are less powerful. So, it's very rare that that's not the dynamic that plays out. What happens if you create something other than humans that are more powerful than us? How could that go well?</p>
<p>VG: Well, dang, alright. I'm like, man, are you working capabilities, on increasing capabilities?</p>
<p>Interviewee: Well, sure. Yeah. Everything we do in this research is about improving.</p>
<p>VG: Yeah. Okay, cool. But how does that jive with, if we ever achieve the thing we're trying to achieve, then all humans die?</p>
Interviewee: Well, okay. I'm not really a consequentialist. So here's what I'll tell you. If I didn't work on it, it would make no difference. That's number one. And number two, I think that artificial intelligence research is, I really think some of the greatest research being done in the world and it is world historic. It is an incredible titanic undertaking. And I think that the research itself is a beautiful, wonderful thing. I can't think of another part of society that functions as well as the world of artificial intelligence research. It's this incredible community of brilliant people with the right amount of competition and collaboration. You have these sort of astonishing places like Google DeepMind where it doesn't make any money. There's no pressure to use their technology, their research for practical reasons. It is purely interested in and in a very pure, philosophically pure way, this one very clearly defined— Well, as clearly defined a problem as a thing can get while still being interesting— So, I couldn't imagine not being a part of that. But yeah, it just is what it is, right? I don't believe in this thing that you should choose your career based on what you think is gonna make the best difference. I don't think people know what makes a difference most of the time. Most of the time the difference that people make is much worse than the difference they wanted to make. That is just how the world works. 
</blockquote>

<p>Counter-arguments:</p>
<ul><li>AI can have enormous benefits, and many in the AI safety space would consider it a tragedy (and some feel that it would be catastrophically bad) if we never achieved advanced aligned AI. AI applications are also amazingly creative, beautiful, convenient, and innovative, and are already advancing human knowledge, health, and potential. It’s a fascinating space making a lot of rapid progress.</li>
<li>However, considering the nature of bringing about AI that’s more intelligent than humans, one could be worried that (if the biggest concerns about instrumental incentives and alignment have merit) humanity has to succeed at an incredibly complicated technical and governance task on the first try , and otherwise could potentially suffer unprecedented consequences.</li>
<li>Considering how few people work on something that could be a catastrophic risk, it makes sense to be prudent and dedicate more human and physical resources to working on safety.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
