<div class="page-data" data-page-title="Introduction"></div><em><p>Explore Your AI Risk Perspectives: An Interactive Walkthrough of Researchers' Most Frequent Interview Responses</em></p>
<p>Welcome to this interactive walkthrough, exploring perspectives about potential risks from advanced AI. The discussions in this walkthrough stem from a series of <a href='../interviews' >interviews with 97 AI researchers</a>. In these interviews, the interviewer probed researchers’ thoughts about the future of AI, focusing on their responses to arguments for potential risk from advanced systems.</p>
<p>In this walkthrough of arguments about risk and researcher responses to those raised perspectives, you’ll encounter AI researchers’ most common responses, direct quotes, and a range of arguments and counterarguments. At the bottom of each page, we encourage you to mark your own opinion: do you agree with the claims? Why or why not?</p>
<p>Feel free to submit your comments by using the text box below each page, especially if your own views are not represented here. Your comments will be displayed publicly at the end of the walkthrough.</p>
<p>On the final page, you’ll see a visual summary of your opinions, and how they compare to other visitors. If you don’t want your responses recorded, you can still navigate throughout the site by using the top navigation bar.</p>
<p>We hope this will help you explore some of the ongoing discussion on the development and deployment of highly capable AI systems, and encourage you to get in touch with us with any feedback you have. To read more about the interviews (40-60 minute interviews with Dr. Vael Gates, in which 92 interviewees were randomly selected from submissions to machine learning conferences NeurIPS or ICML 2021, and 5 interviewees were outside recommendations), you can read most of the anonymized transcripts and further analysis in <a href='../interviews'  class='button xsmall '>Interviews</a>.</p>
<em><p>Note:</em> <a href='https://bounded-regret.ghost.io/more-is-different-for-ai/' target='_blank'>Jacob Steinhardt describes two common approaches to thinking about future AI systems</a> : the “Engineering” approach and the “Philosophy” approach. The Engineering approach “tends to be empirically-driven, drawing experience from existing or past ML systems”, while the Philosophy approach “tends to think more about the limit of very advanced systems”. He says that “in my experience, people who strongly subscribe to the Engineering worldview tend to think of Philosophy as fundamentally confused and ungrounded, while those who strongly subscribe to Philosophy think of most Engineering work as misguided and orthogonal (at best) to the long-term safety of ML.” However, he concludes that “Neither of these approaches is satisfying and we actually have no single good approach to thinking about risks from future ML systems,” and suggests ML researchers should lean on both when thinking about the future. In this walkthrough, we encourage readers to engage with both approaches, and we try to supplement thought experiments with real-world examples whenever possible.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
