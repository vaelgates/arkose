---
layout: page
title: Opportunities
og-description: Job, Funding, and other opportunities in the AI safety space.
nav-menu: true
order: 4
---

<!-- Main -->
<div id="main" class="alt">

<section class="bg-gray">
	<div class="inner">
		<h3 id="ai_safety"><a href="aisafety">Learn more context on large-scale risks from advanced AI</a></h3>
	<p>These opportunities focus on AI safety work aimed at preventing loss of human control of very capable AI systems. <b>To maximize your eligibility for these opportunities, we recommend gaining context on the perspectives of this subfield</b>, e.g. by <a href="aisafety">skimming pertinent AI safety papers</a>.</p>
		<a href="aisafety" class="button button-white button-right button-special" style="z-index: 2">AI Safety Papers</a>
    </div>
    </section>

<section>
	<div class="inner">
		<h1>Opportunities</h1> 
		<p><i>Last updated: 6/4/24</i></p>
		<!-- <p style="margin-left:5%"><i>Opportunities relevant to <a href="aisafety">reducing large-scale risks from advanced AI</a>.</i></p> -->


<!-- 		<div id="opportunities_headsup" class="box box-blue special">
		<p>These opportunities are relevant to reducing large-scale risks from advanced AI, with  focus on AI safety work aimed at preventing loss of human control of very capable AI systems. <b>To maximize your eligibility for these opportunities, we recommend gaining familiarity with the context and perspectives of this subfield</b>, either by closely reading the grantmakers' webpages or skimming some AI safety papers.</p>
		<a href="aisafety" class="button button-white">AI Safety Papers</a>
		</div>
 -->

		<h3 id="research_opportunities">Job Opportunities (Research and Engineering)</h3>
		<ul>
<!-- 			<li class="expandable">OpenAI</li>
			<ul>
				<li><a href="https://openai.com/safety/safety-systems">Safety Systems Team</a> (<a href="https://openai.com/careers/search?c=safety-systems">roles</a>)</li>
				<li><a href="https://openai.com/safety/preparedness">Preparedness Team</a> (<a href="https://openai.com/careers/search?c=preparedness">roles</a>)</li>
				<li><a href="https://openai.com/blog/introducing-superalignment">Superalignment Team</a> (<a href="https://openai.com/careers/search?c=alignment">roles</a>)</li>
				<li>Security Team</li>
				<li>Policy Research Team</li>
				<li>Trustworthy AI Team</li>
			</ul> -->
			<li class="expandable">Anthropic (<a href="https://www.anthropic.com/careers#open-roles">roles</a>)</li>
			<ul>
				<li><a href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety">Frontier Red Team</a></li>
				<li><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy Team</a></li>
				<li><a href="https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic">Alignment Stress Testing Team</a></li>
				<li>Interpretability Team</li>
				<li>Dangerous Capability Evaluations Team</li>
				<li>Assurance Team</li>
				<li>Security Team</li>
			</ul>
			<li class="expandable">Google DeepMind (<a href="https://deepmind.google/about/careers/#open-roles">roles</a>)</li>
			<ul>
				<li>AI Safety and Alignment Team (Bay Area)</li>
				<li>Scalable Alignment Team</li>
				<li>Frontier Model and Governance Team</li>
				<li>Mechanistic Interpretability Team</li>
				<li><a href="https://deepmind.google/about/responsibility-safety/#:~:text=To%20empower%20teams%20to%20pioneer,and%20collaborations%20against%20our%20AI">Responsibility & Safety Team</a></li>
			</ul>
			<li><a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute">UK AI Safety Institute (AISI)</a> (<a href="https://www.linkedin.com/jobs/search/?currentJobId=3841859538&f_C=10872416&f_F=othr%2Cit%2Ceng%2Cstra&geoId=92000000&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=R&position=39&pageNum=0">research scientist</a>, <a href="https://www.linkedin.com/jobs/search/?currentJobId=3841581526&f_C=10872416&f_F=othr%2Cit%2Ceng%2Cstra&geoId=92000000&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=R&position=41&pageNum=0">research engineer</a>, <a href="https://www.linkedin.com/jobs/search/?currentJobId=3841139997&f_C=10872416&f_F=othr%2Cit%2Ceng%2Cstra&geoId=92000000&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=R&position=40&pageNum=0">software engineer</a>)</li>
			<li class="expandable">RAND (<a href="https://www.rand.org/jobs/technology-security-policy-fellows.html">roles</a>)</li>
			<ul>
			<li> Note that RAND's Technology and Security Policy Fellowship is not just for policy research; ML engineers, software engineers with either infrastructure or front-end experience, and technical program managers are also encouraged to apply via this Fellowship.
			</li>
			</ul>
			<li><a href="https://far.ai/">FAR AI</a> (<a href="https://far.ai/jobs">roles</a>)</li>
			<li><a href="https://metr.org">Model Evaluations and Threat Research (METR)</a> (<a href="https://hiring.metr.org/">roles</a>)</li>
			<li><a href="https://www.apolloresearch.ai/">Apollo Research</a> (<a href="https://www.apolloresearch.ai/careers">roles</a>)</li>
			<!-- <li class="expandable" data-toggle="closed_orgs"><i>Currently Closed Opportunities</i></li>
			<ul>
				<li><a href="https://www.safe.ai/">Center for AI Safety (CAIS)</a> (<a href="https://safe.ai/careers">roles</a>)</li>
				<li><a href="https://www.redwoodresearch.org/">Redwood Research</a> (<a href="https://www.redwoodresearch.org/careers">roles</a>)</li>
				<li><a href="https://palisaderesearch.org/">Palisade Research</a> (<a href="https://palisaderesearch.org/work">roles</a>)
			</ul> -->

		</ul>

		<h3 id="funding">Funding Opportunities</h3>

		<ul>
			<li class="expandable">Open Philanthropy</li>
			<ul>
				<li><a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/"><b>Request for proposals: benchmarking LLM agents on consequential real-world tasks</b></a></li>
				<!--<li><a href="https://www.openphilanthropy.org/rfp-llm-impacts/">Request for proposals: studying and forecasting the real-world impacts of systems built from LLMs</a></li>-->
				<li><a href="https://www.openphilanthropy.org/career-development-and-transition-funding/">Career development and transition funding</a></li>
				<li><a href="https://www.openphilanthropy.org/open-philanthropy-course-development-grants/">Course development grants</a></li>
				<li><a href="https://www.openphilanthropy.org/rfp-for-projects-to-grow-our-capacity-for-reducing-global-catastrophic-risks/"> Request for proposals for projects to grow our capacity for reducing global catastrophic risks</a></li>
				<!-- <li><a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">How to Apply for Funding</a></li> -->
			</ul>	
			<li class="expandable">Survival and Flourishing Fund (SFF): <a href="https://survivalandflourishing.fund/sff-2024-applications">Grant Round</a> with additional <a href="https://survivalandflourishing.fund/sff-freedom-and-fairness-tracks">Freedom and Fairness tracks</a> (<i>deadline: 7/1</i>) </li>
			<!-- and <a href="https://survivalandflourishing.fund/speculation-grants">Speculation Grants</a> -->
			<ul>
					<li> Note: SFF gives <a href="https://survivalandflourishing.fund/faq#does-sff-have-an-indirect--overhead-rate-limit-for-grants-to-universities">grants to universities</a>. Alternatively, SFF requires that you have a 501c3 charity (i.e. your nonprofit has 501c3 status or you have a fiscal sponsor that has 501c3 status).</li>
					<!-- ALSO: https://jaan.info/xrisk/ -->

			</ul>
			<li><a href="https://www.cooperativeai.com/grants/cooperative-ai">Cooperative AI Research Grants</a> (<i>deadlines: 7/30 & 10/6</i>)</li>
			<li><a href="https://www.mlsafety.org/safebench">SafeBench Competition</a> (<i>deadline: 2/25/2025; $250k in prizes</i>)</li>
			<li><a href="https://taskdev.metr.org/bounty/">METR Evaluation Task Bounty</a></i> (<i>related: <a href="https://metr.github.io/autonomy-evals-guide/">METR's Autonomy Evaluation Resources</a></i>)</li>
			<li><a href="https://cset.georgetown.edu/wp-content/uploads/FRG-Call-for-Research-Ideas-Expanding-the-Toolkit-for-Frontier-Model-Releases.pdf">Call for Research Ideas: Expanding the Toolkit for Frontier Model Releases</a> from CSET (<i>deadline: 7/3</i>)</li>
			<li><a href="https://aisfund.org/grant-process/">AI Safety Fund</a> <a href="https://www.frontiermodelforum.org/updates/ai-safety-fund-initiates-first-round-of-research-grants/">via the Frontier Model Forum</a></li>
			<li><a href="https://new.nsf.gov/funding/opportunities/secure-trustworthy-cyberspace-satc">NSF Secure and Trustworthy Cyberspace Grants</a></li>
			<li><a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a></li>
			<!-- https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_role_type%5D%5B0%5D=Funding -->


			<li class="expandable" data-toggle="closed_funding"><i>Currently Closed Funding Opportunities</i></li>
			<ul>
				<li><a href="https://www.aria.org.uk/programme-safeguarded-ai/">ARIA's Safeguarded AI Program</a> (aimed at quantitative safety guarantees, accepting proposals)</li>	
				<li>OpenAI: <a href="https://openai.smapply.org/prog/agentic-ai-research-grants/">Research into Agentic AI Systems</a>, <a href="https://openai.com/blog/superalignment-fast-grants">Superalignment Fast Grants</a>, <a href="https://openai.com/blog/openai-cybersecurity-grant-program">OpenAI Cybersecurity Grants (<i>assumed closed</i>)</a></li>
				<li>NSF: <a href="https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems">Safe Learning-Enabled Systems</a> and <a href="https://new.nsf.gov/funding/opportunities/responsible-design-development-deployment">Responsible Design, Development, and Deployment of Technologies</a></li>
				<li>Center for Security and Emerging Technology (CSET): <a href="https://cset.georgetown.edu/foundational-research-grants/">Foundational Research Grants</a></li>
				<li><a href="https://futureoflife.org/our-work/grantmaking-work/">Future of Life Institute</a>: <a href="https://futureoflife.org/grant-program/phd-fellowships/">PhD Fellowships</a> and <a href="https://futureoflife.org/grant-program/postdoctoral-fellowships/">Postdoctoral Fellowships</a></li>
		</ul>
		
		<br>
		<h3 class="expandable" id="compute">Compute Opportunities</h3>
		<ul>
			<li><a href="https://www.safe.ai/compute-cluster">Center for AI Safety Compute Cluster</a> (<i>deadline: 9/30/24</i>)</li>
			<li><a href="https://ndif.us/start.html">National Deep Inference Fabric (NDIF)</a>, research computing project for interpretability research, can request early access</li>
			<li><a href="https://txt.cohere.com/c4ai-research-grants/">Cohere for AI</a>, subsidized access to APIs</li>
		</ul>

		<br>
		<h3 class="expandable" id="visitor-programs">AI Safety Programs / Fellowships / Residencies</h3>
		<ul> 
			<li class="expandable" data-toggle="constellation_visiting_fellows_description">Visiting Fellows at Constellation (<i>applications open for fall and winter cohorts, for researchers & engineers</i>)</li>
			<ul>
				<li><a href="https://www.constellation.org">Constellation</a> is offering 3–6 month extended visits (unpaid) at their office (Berkeley, CA) for researchers, engineers, entrepreneurs, and other professionals working on their <a href="https://www.constellation.org/focus-areas">focus areas</a>. <a href="https://airtable.com/appEr4IN5Kkzu9GLq/shrx1EusilwHfAOGs">Apply here</a>. See here for <a href="https://www.constellation.org/programs/visiting-fellows">more details</a>.</li>
			</ul>
			<li class="expandable" data-toggle="constellation_residency_description">Residencies at Constellation (<i>deadline closed for July/Sept start dates for 1-year residencies, <a href="https://airtable.com/appEr4IN5Kkzu9GLq/pagPUC76WrUmrzUpj/form">short visits</a> are likely still available; for researchers & engineers</i>)</li>
			<ul>
				<li><a href="https://www.constellation.org">Constellation</a> is offering year-long salaried positions ($100K-$180K) at their office (Berkeley, CA) for experienced researchers, engineers, entrepreneurs, and other professionals to pursue self-directed work on one of Constellation's <a href="https://www.constellation.org/focus-areas">focus areas</a>. <a href="https://airtable.com/appEr4IN5Kkzu9GLq/shr3LgseSOaRxA2mQ">Apply here</a>. See here for <a href="https://www.constellation.org/programs/residency">more details</a>.</li>
			</ul>
			<li class="expandable" data-toggle="mats_description">MATS Winter Program (<i>deadline: 8/1/24, for graduate students</i>)</li>
			<ul>
				<li>The <a href="https://www.matsprogram.org/">ML Alignment & Theory Scholars (MATS)</a> Program is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment and safety. We also connect them with the Berkeley alignment research community. Our Winter Program will run from early Jan, 2025. <a href="https://airtable.com/appPxJ0QMqR7TElYU/pagRPwHQtcN8L0vIE/form">Apply here</a>.</li>
			</ul>
			<li id="spar"><a href="https://supervisedprogramforalignment.org/">Supervised Program for Alignment Research (SPAR) Fall Program</a></li>
<!-- 	<li class="expandable" data-toggle="closed_programs"><i>Currently Closed Programs</i></li> -->
<!-- 				<li class="expandable" data-toggle="astra_description"><a href="https://www.constellation.org/programs/astra-fellowship">Astra Fellowship</a> at Constellation (<i>for researchers</i>)</li>
				<ul>
					<li>The Astra Fellowship pairs fellows with experienced advisors to collaborate on a two or three month AI safety research project. Fellows will be part of a cohort of talented researchers working out of the Constellation offices in Berkeley, CA, allowing them to connect and exchange ideas with leading AI safety researchers.</li>
				</ul> -->
			
<!--  			<li class="expandable" data-toggle="lasr_description">LASR (London AI Safety Research) Labs (<i>deadline: 4/24, for graduate students</i>)</li> -->
<!-- 			<li class="https://supervisedprogramforalignment.org/">SPAR</li>
 -->
		</ul>

		<br>
		<h3 class="expandable" id="workshops">Workshops and Community</h3>
		<ul>
			<li class="expandable">Upcoming Workshops</li>
			<ul>
				<!-- <li class="expandable" data-toggle="ICLR">ICLR 2024 Workshops</li>
				<ul>
					<li><a href="https://www.mlsafety.org/events/iclr-social">ML Safety Social</a> hosted by the Center for AI Safety</li>
					<li><a href="https://set-llm.github.io/">Secure and Trustworthy Large Language Models</a></li>
					<li><a href="https://agiworkshop.github.io">How Far Are We From AGI?</a></li>
					<li><a href="https://iclr-r2fm.github.io/">Reliable and Responsible Foundation Models</a></li>
					<li><a href="https://sites.google.com/view/me-fomo2024">ME-FoMo: Mathematical and Empirical Understanding of Foundation Models</a></li>
					<!--https://pml-workshop.github.io/iclr24/-->
				<!--</ul> -->
				<li><a href="https://airtable.com/appK578d2GvKbkbDD/pagkxO35Dx2fPrTlu/form">Future events interest form</a> for the Alignment Workshop Series (previous: <a href="https://www.alignment-workshop.com/nola-2023">Dec 2023</a>, <a href="https://www.alignment-workshop.com/sf-2023">Feb 2023</a>)</li>
				<li><a href="https://airtable.com/appEr4IN5Kkzu9GLq/shrRJhQiMx0I6QsSb">Future events interest form</a> for Constellation Workshops. Constellation expects to offer 1–2 day intensive workshops for people working in or transitioning into their <a href="https://www.constellation.org/focus-areas">focus areas</a>.</li>
			</ul>
			<li class="expandable" data-toggle="past_workshops"><i>Past Workshops</i></li>
			<ul>
				<li class="expandable" data-toggle="ICLR">ICLR 2024 Workshops</li>
				<ul>
					<li><a href="https://www.mlsafety.org/events/iclr-social">ML Safety Social</a> hosted by the Center for AI Safety</li>
					<li><a href="https://set-llm.github.io/">Secure and Trustworthy Large Language Models</a></li>
					<li><a href="https://agiworkshop.github.io">How Far Are We From AGI?</a></li>
					<li><a href="https://iclr-r2fm.github.io/">Reliable and Responsible Foundation Models</a></li>
					<li><a href="https://sites.google.com/view/me-fomo2024">ME-FoMo: Mathematical and Empirical Understanding of Foundation Models</a></li>
					<!--https://pml-workshop.github.io/iclr24/-->
				</ul
				<li><b><a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop (Dec 2023)</a></b>, <i>recordings available</i></li>
				<li><a href="https://www.alignment-workshop.com/sf-2023">San Francisco Alignment Workshop 2023 (Feb 2023)</a>, <i>recordings available</i></li>
				<li><a href="https://sites.google.com/mila.quebec/scaling-laws-workshop/">Neural Scaling & Alignment: Towards Maximally Beneficial AGI Workshop Series (2021-2023)</a></li>
				<li><a href="https://sites.google.com/mila.quebec/hlai-2023-boston/home">Human-Level AI: Possibilities, Challenges, and Societal Implications (June 2023)</a></li>
				<li><a href="https://futuretech.mit.edu/workshop-on-ai-scaling-and-its-implications#:~:text=The%20FutureTech%20workshop%20on%20AI,a%20range%20of%20key%20tasks%3F">Workshop on AI Scaling and its Implications (Oct 2023)</a></li>
			</ul>
			<br>
			<li class="expandable" data-toggle="academia">Reseachers working in AI safety</li>
			<ul>
				<li><a href="https://futureoflife.org/about-us/our-people/ai-existential-safety-community/">AI Existential Safety Community</a> from Future of Life Institute</li>
				<li>See speakers from the Alignment Workshop series (<a href="https://www.alignment-workshop.com/sf-2023">SF 2023</a>, <a href="https://www.alignment-workshop.com/nola-2023">NOLA 2023</a>)</li>
			</ul>
			<li class="expandable" data-toggle="china">Interested in working in China?</li>
			<ul>
				<li>Contact <a href="https://concordia-ai.com/">Concordia AI 安远AI</a></li>
				<li><a href="https://idais.ai/">International Dialogues on AI Safety</a></li>
				<li><a href="https://alignmentsurvey.com/">AI Alignment: A Comprehensive Survey</a></li>
				<li>Newsletters: <a href="https://aisafetychina.substack.com/">AI Safety in China</a>, <a href="https://chinai.substack.com/about">ChinAI Newsletter</a></li>
			</ul>

		</ul>

		<br>
		<h3 class="expandable" id="jobs">Job Board</h3>
		<ul class="width-100">
		<div class="iframe-container">
			<iframe id="job_board" onload="iframeLoaded('iframe_loading_spinner_job_board')"  src="https://airtable.com/embed/appQwyPiYo4egdPR5/shr6n4WqjnmmoTpMf?backgroundColor=grayLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
			<div id="iframe_loading_spinner_job_board" class="iframe-loading">
				{% include loading_spinner.html %}
			</div>
			<p style="text-align: right;"><i>Filtered from the <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Bcompany_data%5D%5B0%5D=Highlighted%20organisations&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Data&refinementList%5Btags_skill%5D%5B1%5D=Engineering&refinementList%5Btags_skill%5D%5B2%5D=Research&refinementList%5Btags_skill%5D%5B3%5D=Software%20engineering">80,000 Hours Job Board</a></i></p>
		</div>

	</ul>
	<h4 class="expandable" id="alternative_opportunities">Alternative Technical Opportunities</h4>
	<ul>
		<li class="expandable" data-toggle="theoretical_research"><b>Theoretical Research</b></li>
		<ul>
			<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
			<!-- <li><a href="https://intelligence.org/careers/">Machine Intelligence Research Institute (MIRI)</a> </li> -->
		</ul>
		<li class="expandable" data-toggle="information_security"><b>Information Security</b></li>
		<ul>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Information%20security">Information Security roles</a></li>
			<li><a href="https://80000hours.org/career-reviews/information-security/">Overview from a security engineer at Google</a></li>
			<li><a href="https://www.linkedin.com/in/jason-clinton-475671159/">Jason Clinton</a>'s recommended <a href="https://www.google.com/books/edition/Building_Secure_and_Reliable_Systems/Kn7UxwEACAAJ?hl=en&kptab=getbook">upskilling book</a></li> <!--<a href="https://forum.effectivealtruism.org/posts/zxrBi4tzKwq2eNYKm/ea-infosec-skill-up-in-or-make-a-transition-to-infosec-via">EA Infosec: skill up in or make a transition to infosec via this book club</a>-->
		</ul>
		<li><a href="https://jobs.80000hours.org/?query=Forecasting&refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy"><b>Forecasting</b></a> (see especially <a href="https://epochai.org/careers">Epoch</a>)</li>
		<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering"><b>Software Engineering</b></a></li>
		<li class="expandable" id="technical_governance"><b>Technical AI Governance</b> (from <a href="https://web.archive.org/web/20231012161714/https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/#4-ai-governance-and-policy-aigp"> Section 4.3</a>)</li>
		<ul>
			<li>Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>
			<li class="expandable" data-toggle="technical_ai_governance_learn_more"><i>Learn more:</i></li>
			<ul>
				<li><a href="https://docs.google.com/document/d/1KNkIrp1nr6ETXAFpu5k8wYz51sJlDyDDnTXtWwDcKBw/edit?ref=blog.heim.xyz">Blog post (Anonymous): "AI Governance Needs Technical Work"</a></li>
				<li><a href="https://blog.heim.xyz/technical-ai-governance/">Blog post by Lennart Heim: "Technical AI Governance"</a> (focuses on compute governance), <a href="https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/">Podcast: "Lennart Heim on the compute governance era and what has to come after"</a></li>
				<li><a href="https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/">Blog post by Luke Muehlhauser, "12 Tentative Ideas for US AI Policy"</a></li>
				<li><a href="https://80000hours.org/career-reviews/become-an-expert-in-ai-hardware/">Perspectives on options given AI hardware expertise</a> (80,000 Hours)</li>
				<li><i>Arkose is looking for further resources about technical governance, as this is a narrow set; please send recommendations to team@arkose.org!</i></li>
			</ul>
			<li>See also the AI Governance and Policy section below.</li>
			<!-- Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>-->
		</ul>
		<li class="expandable" id="governance"><b>AI Governance and Policy</b></li>
		<ul>
			<p>AI governance is focused on developing global norms, policies, and institutions to increase the chances that advanced AI is beneficial for humanity.</p>
			<li><a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a> by BlueDot Impact</li>
			<li><a href="https://emergingtechpolicy.org/areas/ai-policy/">AI Policy Resources</a> by Emerging Technology Policy Careers</li>
			<li class="expandable expanded">Several organizations working in the space:</li>
			<ul>
				<li><a href="https://www.longtermresilience.org/">Center for Long-Term Resilience (CLTR)</a></li>
				<li><a href="https://www.rand.org/topics/science-technology-and-innovation-policy.html">RAND's Technology and Security Policy work</a></li>
				<li><a href="https://www.horizonpublicservice.org/">Horizon Institute for Public Service</a></li>
				<li><a href="https://www.iaps.ai/">Institute for AI Policy and Strategy</a></li>
				<li><a href="https://cset.georgetown.edu/">Center for Security and Emerging Technology (CSET)</a></li>
				<li>Frontier AI Task Force</li>
				<li><a href="https://www.governance.ai/">Center for the Governance of AI (GovAI)</a></li>
				<li>Industry AI Governance teams</li>
				<li><a href="https://www.aipolicy.us/about">Center for AI Policy</a></li>
				<!-- Center for AI Safety -->
			</ul>
		</ul>
	</ul>


<!--
		<h3>Example of expandable lists</h3>
			<li class="expandable" data-toggle="theoretical_research">Theoretical research</li>
			<li class="expandable expanded" data-toggle="theoretical_research">Theoretical</li>
-->

	</div>
</section>

<script>
	function iframeLoaded(spinnerID) {
		document.getElementById(spinnerID).style.display = 'none';
	}
</script>


{% if false %}
<!-- All this commented-out section is inside an if-statement so it doesn't get sent to users.-->


<!--
{% if false %}
<section id="two" class="bg-gray">
	<div class="inner">

<div id="book_a_call"><h2>Book a call</h2>
<p>If you're interested in working in AI alignment and advanced AI safety, please book a call with <a href="https://vaelgates.com">Vael Gates</a>, who leads this project and conducted the <a href=interviews>interviews</a> as part of their postdoctoral work with Stanford University.</p>

<form id="book_call_form" method="post" action="#">
	<div class="row uniform">
		<div class="6u 12u$(xsmall)">
			<input type="text" name="name" id="name" value="" placeholder="Name" />
		</div>
		<div class="6u$ 12u$(xsmall)">
			<input type="email" name="email" id="email" value="" placeholder="Email" />
		</div>
		<div class="12u$">
			<div class="select-wrapper">
				<select name="interest" id="interest">
					<option value=""> Interested in talking about... </option>
					<option value="AI Alignment Research or Engineering">AI Alignment Research or Engineering</option>
					<option value="AI Alignment Governance">AI Alignment Governance</option>
					<option value="Other">Other (please specify below)</option>
				</select>
			</div>
		</div>
		<div class="12u$">
			<textarea name="message" id="message" placeholder="Enter your message" rows="6"></textarea>
		</div>
		<div class="12u$">
			<ul class="actions">
				<li>
          <button class="button" id="send_form_button">
            <div class="button-progress-bar"></div>
            <div class="button-text">Send Message</div>
          </button>
        </li>
			</ul>
		</div>
	</div>
</form>
</div>

</div>
</section>

{% endif %}

</div>

{% if false %}
<script src="{{ "assets/js/book_call.js" | absolute_url }}" type="module"></script>
<script>
  window.contactEmail = "{{site.email}}"
</script>
{% endif %}
-->

{% endif %}
