---
layout: page
title: Resources
image: assets/images/pic01.jpg
nav-menu: true
order: 3
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Resources</h1>
		</header>

<!-- Content -->
<!-- <h2 id="content">Readings</h2> -->

<p><i> Sorted by <a href="#MLreadings">machine learning researchers</a>, <a href="#publicreadings">general audience</a>, and <a href="what_can_i_do.html">more involved.</a> </i></p>

<h3 id="MLreadings">For machine learning researchers</h3>
<div class="row">
	<div class="6u 12u$(medium)">
		<div class = "box">
		<h4>Risks from advanced AI</h4>
		<ul>
		    <li><a href="https://wp.nyu.edu/arg/why-ai-safety/" class="button special small">read</a> <a href="https://wp.nyu.edu/arg/why-ai-safety/"> Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022) <!-- , 15m <i>(stop at the section "The new lab")</i> --> </li>
		    <li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0" class="button special small">watch</a> <a href="https://www.youtube.com/watch?v=yl2nlejBcg0"> Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)<!-- , 48m <i>(skip the Q&A)</i> --> </li>
		</ul>
	</div>
	</div>
	<div class="6u$ 12u$(medium)">
		<div class = "box">
		<h4>Orienting</h4>
		<ul>
		    <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/" class="button special small">read</a> <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/"> More is Different for AI</a> by Jacob Steinhardt (2022)<!-- , 30m <i>(intro and first three posts only)</i> --></li>
		    <li><a href="https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit?usp=sharing">AI Timelines/Risk Projections as of Sep. 2022</a><!-- <i>(first 3 pages only)</i>, 5m --></li>
		    <li><a href="https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment">​​Frequent Arguments About Alignment</a> by John Schulman (2021)<!-- , 15m --></li>
		</ul>
		</div>
	</div>
</div>
<div class = "box">
<h4>Research directions</h4>
<h5> Reviews </h5>
<ul>
	<li><a href="https://arxiv.org/pdf/2209.00626.pdf" class="button special small">read</a> <a href="https://arxiv.org/pdf/2209.00626.pdf"> The Alignment Problem from a Deep Learning Perspective</a> by Richard Ngo et al. (2022)<!-- , 65m --></li>
	<li><a href="https://arxiv.org/pdf/2109.13916.pdf"> Unsolved Problems in ML Safety</a> by Hendrycks et al. (2022)</li>
    <li><a href="https://www.youtube.com/watch?v=-vsYtevJ2bc"> Current Work in AI Alignment</a> by Paul Christiano (2019)<!-- , 30m (<a href="https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment">transcript</a>) --></li>
</ul>
<h5> Primary </h5>
<ul>
    <li><a href="https://arxiv.org/pdf/2210.01790.pdf" class="button special small">read</a> <a href="https://arxiv.org/pdf/2210.01790.pdf"> Goal Misgeneralization: Why correct Specifications Aren't Enough For Correct Goals</a> (Shah et al., 2022)</li>
    <li><a href="https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4">Specification gaming: the flip side of AI ingenuity </a> (Krakovna et al., 2020)</li>
    <li>Mechanistic interpretability <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">(Olsson et al., 2022)</a>, <a href="https://arxiv.org/abs/2202.05262">(Meng et al. 2022)</a></li> 
    <li><a href="https://proceedings.neurips.cc/paper/2021/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf">Optimal Policies Tend to Seek Power</a> (Turner et al., 2021)</li>
    <!-- <li>Eliciting latent knowledge (<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit">ELK</a> </li>-->
</ul>
</div>
 

<h3 id="publicreadings">For a general audience</h3>
<ul>
	<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment" class="button special small">read</a><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)<!-- , 30m --></li>
    <li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)<!-- , book --></li>
    <li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)<!-- , 2.5h --></li>
	<li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcast, summaries, various articles)</li>
	<li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
</ul>

</div>
</section>

</div>
