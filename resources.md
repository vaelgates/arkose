---
layout: page
title: Resources
og-description: "Resources for machine learning researchers, for those interested in AI governance and policy, and for a general audience."
nav-menu: true
order: 3
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Resources</h1>
		</header>

<!-- Content -->
<!-- <h2 id="content">Readings</h2> -->

<!-- <p><i> Sorted by <a href="#MLreadings">machine learning researchers</a>, <a href="#publicreadings">general audience</a>, and <a href="what_can_i_do.html">more involved.</a> </i></p> -->

<h3 id="MLreadings">For machine learning researchers</h3>
<div class = "box">
<h4>Risks from Advanced AI</h4>
<h5> Reviews </h5>
<ul>
	<li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/" class="button xsmall">∗</a> <a href="https://wp.nyu.edu/arg/why-ai-safety/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
	<li><a href="https://arxiv.org/pdf/2209.00626.pdf" class="button xsmall">∗</a> <a href="https://arxiv.org/pdf/2209.00626.pdf"> The Alignment Problem from a Deep Learning Perspective</a> (Ngo et al., 2022)></li>
	<li><a href="https://wp.nyu.edu/arg/why-ai-safety/" class="button xsmall">∗</a> <a href="https://wp.nyu.edu/arg/why-ai-safety/"> Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022), 15m <i>(stop at the section "The new lab")</i> </li>
	<li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0" class="button xsmall">∗</a> <a href="https://www.youtube.com/watch?v=yl2nlejBcg0"> Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022), 48m <i>(skip the Q&A)</i> </li>
	<li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/" class="button xsmall">∗</a> <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/"> More is Different for AI</a> by Jacob Steinhardt (2022)</li>
</ul>
</div>

<!--<div class="row">
	<div class="6u 12u$(medium)">
		<div class = "box">
		<h4>Risks from advanced AI</h4>
		<ul>
			<li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/" class="button xsmall">∗</a> <a href="https://wp.nyu.edu/arg/why-ai-safety/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
		    <li><a href="https://wp.nyu.edu/arg/why-ai-safety/" class="button xsmall">∗</a> <a href="https://wp.nyu.edu/arg/why-ai-safety/"> Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022), 15m <i>(stop at the section "The new lab")</i> </li>
		    <li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0" class="button xsmall">∗</a> <a href="https://www.youtube.com/watch?v=yl2nlejBcg0"> Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022), 48m <i>(skip the Q&A)</i> </li>
		</ul>
		</div>
	</div>
	<div class="6u$ 12u$(medium)">
		<div class = "box">
		<h4>Orienting</h4>
		<ul>
		    <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/" class="button xsmall">∗</a> <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/"> More is Different for AI</a> by Jacob Steinhardt (2022)</li>
		    <li><a href="https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit?usp=sharing">AI Timelines/Risk Projections as of Sep. 2022</a></li>
		    <li><a href="https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment">​​Frequent Arguments About Alignment</a> by John Schulman (2021)</li>
		</ul>
		</div>
	</div>
</div>-->
<div class = "box">
<h4>Technical Research</h4>
<h5> Reviews </h5>
<ul>
	<li><a href="https://arxiv.org/pdf/2209.00626.pdf" class="button xsmall">∗</a> <a href="https://arxiv.org/pdf/2209.00626.pdf"> The Alignment Problem from a Deep Learning Perspective</a> (Ngo et al., 2022)<!-- , 65m --></li>
	<li> Agendas: <a href="https://arxiv.org/pdf/2109.13916.pdf"> Unsolved Problems in ML Safety</a> (Hendrycks et al., 2022),  <a href="https://arxiv.org/pdf/1606.06565.pdf">Concrete Problems in AI safety</a> (Amodei et al., 2016)</li>
	<li><a href="https://vkrakovna.wordpress.com/ai-safety-resources/">AI Safety Resources</a> and <a href="https://www.alignmentforum.org/posts/JC7aJZjt2WvxxffGz/paradigms-of-ai-alignment-components-and-enablers">Overview</a> by Victoria Krakovna (DeepMind) </li>
</ul>
<h5> Primary </h5>
<ul>
    <li><a href="https://arxiv.org/pdf/2210.01790.pdf" class="button xsmall">∗</a> <a href="https://arxiv.org/pdf/2210.01790.pdf"> Goal Misgeneralization: Why correct specifications aren't enough for correct goals</a> (Shah et al., 2022)</li>
    <li><a href="https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4">Specification gaming: the flip side of AI ingenuity </a> (Krakovna et al., 2020)</li>
    <li>Mechanistic interpretability: <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">In-context Learning and Induction Heads</a> (Olsson et al., 2022), <a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a> (Meng et al., 2022)</li>
    <li><a href="https://arxiv.org/pdf/2212.03827.pdf">Discovering Latent Knowledge in Language Models Without Supervision</a> (Burns et al., 2022)</li>
    <!--<li><a href="https://proceedings.neurips.cc/paper/2021/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf">Optimal Policies Tend to Seek Power</a> (Turner et al., 2021)</li>-->
</ul>

<h5> Other </h5>

<ul>
	<li><a href="https://www.agisafetyfundamentals.com/ai-alignment-curriculum" class="button xsmall">∗</a> <a href="https://www.agisafetyfundamentals.com/ai-alignment-curriculum">AGI Safety Fundamentals Curriculum</a> (<b>best in-depth resource</b>) </li>
	<li><a href="https://rohinshah.com/alignment-newsletter/">Alignment Newsletter</a> and <a href="https://newsletter.mlsafety.org/">ML Safety Newsletter</a></li>
</ul>

<h5> Interested in doing AI alignment research?</h5>
<ul>
<li> Learn about the organizations and researchers in the space, funding and job opportunities, and guides to get involved at&nbsp; <a href="what_can_i_do#technical" class="button xsmall">What can I do?</a></li>
</ul>

</div>

<!-- <h4> Further resources </h4> -->

<!-- <h5> Reviews </h5> -->
<!-- <ul> -->
<!-- 	<li><a href="https://www.youtube.com/watch?v=-vsYtevJ2bc"> Current Work in AI Alignment</a> by Paul Christiano (2019), 30m (<a href="https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment">transcript</a>)</li> -->
<!-- 	<li><a href="https://www.alignmentforum.org/posts/3DFBbPFZyscrAiTKS/my-overview-of-the-ai-alignment-landscape-threat-models">My Overview of the AI Alignment Landscape: Threat Models</a> by Neel Nanda</li> -->
<!-- 	<li>A provisionary list of alignment / safety organizations and examples of their work, as of Fall 2022: <a href="https://docs.google.com/document/d/1gimXyGj4nTU9TFJ6svlpmMtEWGbTrMoNYfzZMi8siAA/edit?usp=sharing">Shortform</a>, <a href="https://docs.google.com/document/d/1SXhls4pCFdJ6PbRnlmNiF3GhTSx3qq2SkDRsKGKb1O4/edit?usp=sharing">Longform</a></li> -->
<!-- </ul> -->

<!-- <h5> Primary </h5> -->
<!-- <ul> -->
<!--     <li><a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit">Eliciting Latent Knowledge</a> (Alignment Research Center, 2022)</li> -->
<!-- 	<li><a href="https://arxiv.org/pdf/1611.08219.pdf">The Off-Switch Game</a> (Menell et al., 2016)</li>
	<li><a href="https://intelligence.org/files/Corrigibility.pdf">Corrigibility</a> (Soares et al., 2015)</li> -->
<!-- </ul> -->

<!-- <h5> Additional resources</h5> -->
<!-- <ul> -->
<!-- 	<li><a href="https://www.alignmentforum.org/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as">AI Safety and Neighboring Communities: A Quick-Start Guide, as of Summer 2022</a> by Sam Bowman (NYU)</li> -->
<!-- 	<li><a href="https://docs.google.com/document/d/1zGxvxccxNap4KL70iqHiOBw8voB38Bhp-Z5WpvPQm-w/edit">Critiques of AI safety arguments</a>, see also "Disagree" arguments in <a href="arguments">Perspectives</a></li> -->
<!-- </ul> -->

<!-- </div> -->

<br>
</div>
</section>



<section id="two" class="bg-gray">
	<div class="inner">





<h3 id="publicreadings">For a general audience</h3>
<ul>
	<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment" class="button xsmall">∗</a><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)<!-- , 30m --></li>
    <li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)<!-- , book --></li>
    <li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
    <li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
    <li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)<!-- , 2.5h --></li>
	<li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcast, summaries, various articles)</li>
	<li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
</ul>



<br>
<h3 id="governance readings">For interest in AI governance and policy</h3>
<ul>
	<li><a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum" class="button xsmall">∗</a> <a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a></li>
</ul>




</div>
</section>

</div>
